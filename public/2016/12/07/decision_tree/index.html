<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="决策树,spark mlilib源码," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="决策树算法源码学习，其中模型的训练部分以随机森林的训练过程进行说明，决策树相当于树的数量为1的随机森林">
<meta name="keywords" content="决策树,spark mlilib源码">
<meta property="og:type" content="article">
<meta property="og:title" content="spark mllib 决策树算法源码学习">
<meta property="og:url" content="http://learning.github.com/2016/12/07/decision_tree/index.html">
<meta property="og:site_name" content="个人学习博客">
<meta property="og:description" content="决策树算法源码学习，其中模型的训练部分以随机森林的训练过程进行说明，决策树相当于树的数量为1的随机森林">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://learning.github.com/2016/12/07/decision_tree/decision_tree_example.png">
<meta property="og:updated_time" content="2018-02-11T09:43:37.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="spark mllib 决策树算法源码学习">
<meta name="twitter:description" content="决策树算法源码学习，其中模型的训练部分以随机森林的训练过程进行说明，决策树相当于树的数量为1的随机森林">
<meta name="twitter:image" content="http://learning.github.com/2016/12/07/decision_tree/decision_tree_example.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> spark mllib 决策树算法源码学习 | 个人学习博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">个人学习博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                spark mllib 决策树算法源码学习
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-12-07T00:00:00+08:00" content="2016-12-07">
              2016-12-07
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/模型与算法/" itemprop="url" rel="index">
                    <span itemprop="name">模型与算法</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/2016/12/07/decision_tree/" class="leancloud_visitors" data-flag-title="spark mllib 决策树算法源码学习">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js">
</script>

<p>该文章来自于2016年后半年整理的算法源码笔记，由于之前没有写博客的习惯，都直接以笔记的形式存在电脑上，分享起来非常不便，因此抽出时间，将其整理成博客的形式，和大家一起学习交流。</p>
<h1 id="决策树算法简要介绍"><a href="#决策树算法简要介绍" class="headerlink" title="决策树算法简要介绍"></a>决策树算法简要介绍</h1><p>决策树算法是一种常见的分类算法，也可以用于回归问题。相对于其他分类算法，决策树的优点在于简单,可解释性强；对特征尺度不敏感，不需要做太多的特征预处理工作;能够自动挖掘特征之间的关联关系。缺点是比较容易过拟合（通过随机森林可以避免过拟合）</p>
<p>决策树是一个树形结构，其中叶子节点表示分类（或回归）结果，非叶子节点是属性判断判断节点，每个属性判断节点都选择样本的一个特征，并根据该特征的取值决定选择哪一个分支路径。在对样本进行预测时，从根节点开始直到叶子节点，对于路径上的每个分支节点，都根据其对应的属性取值选择下一个分支节点，直到叶子节点。整个完整的路径，表示对样本的预测过程。如图1所示，表示一个女孩在决定是否决定去相亲的一个过程，最终选择去或者不去，对应分类的结果，中间的各种条件对应相关的属性。</p>
<center><br><img src="/2016/12/07/decision_tree/decision_tree_example.png" alt="“决策树样例”"><br></center><br><center>图1：决策树样例：对女孩决定是否参加相亲的问题进行决策树建模</center>


<h2 id="决策树的训练"><a href="#决策树的训练" class="headerlink" title="决策树的训练"></a>决策树的训练</h2><p>从根节点开始，根据信息增益或其他条件，不断选择分裂的属性，直到生成叶子节点的过程。具体过程如下所示：</p>
<ul>
<li>对不同的属性，计算其信息增益，选择增益最大的特征对应根节点的最佳分裂。</li>
<li>从根节点开始，对于不同的分支节点，分别选择信息增益最大的特征作为分支节点的最佳分裂。</li>
<li>如果达到停止分裂的条件，则将该节点作为叶子节点：当前节点对应的样本都是一类样本，分类结果为对应的样本的类别；总样本数量小于一定值，或者树的高度达到最大值，或者信息增益小于一定值，或者已经用完所有的属性，选择占比最大的样本分类作为节点对应的分类结果。否则，根据步骤2进一步构造分裂节点。</li>
</ul>
<h2 id="属性度量"><a href="#属性度量" class="headerlink" title="属性度量"></a>属性度量</h2><p>决策树构建的关键，在于不断地选择最佳分裂属性。属性的收益度量方法，常见的有信息增益（ID3算法）、信息增益率（C4.5算法），基尼系数(CART算法)等。</p>
<p><strong>ID3算法:</strong></p>
<p>熵：信息论中，用于描述信息的不确定性，定义如式1，其中$D$表示对样本的一个划分，$m$表示划分的类别数量，$p_i$表示第i个类别的样本数量比例。</p>
<p>$info(D)=-\sum_{i=1}^m p_ilog_2(p_i)\;\;\;（式1）$</p>
<p>假设按照属性A对样本D进行划分，$v$为属性$A$的划分数量。则$A$对$D$划分的期望熵如式2：</p>
<p>$info_A(D)=\sum_{j=1}^v\frac{|D_j|}{|D|}info(D_j)\;\;\;（式2）$</p>
<p>信心增益为上述原始熵和属性A对D划分后期望熵的差值，可以看做是加入信息A后，不确定性的减少程度。信息增益的定义如式3所示：</p>
<p>$gain(A)=info(D)-info_A(D)\;\;\;（式3）$</p>
<p>ID3算法即在每次选择最佳分裂的属性时，根据信息增益进行选择。</p>
<p><strong>C4.5算法:</strong><br>ID3算法容易使得选取值较多的属性。一种极端的情况是，对于ID类特征有很多的无意义的值的划分，ID3会选择该属性其作为最佳划分。C4.5算法通过采用信息增益率作为衡量特征有效性的指标，可以克服这个问题。</p>
<p>首先定义分裂信息：<br>$splitInfo_A(D)=-\sum_{j=1}^v\frac{|D_j|}{|D|}log_2(\frac{|D_j|}{|D|})\;\;\;（式4）$</p>
<p>信息增益率：<br>$gainRatio(A)=\frac{gain(A)}{splitInfo_A(D)}\;\;\;（式5）$</p>
<p><strong>CART算法:</strong></p>
<p>使用基尼系数作为不纯度的度量。<br>基尼系数:表示在样本集合中一个随机选中的样本被分错的概率，Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。当所有样本属于一个类别时，基尼系数最小为0。所有类别以等概率出现时，基尼系数最大。<br>$GINI(P)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^K p_k^2\;\;\;（式6）$</p>
<p>由于cart建立的树是个二叉树，所以K的取值为2。对于特征取值超过2的情况，以每个取值作为划分点，计算该划分下对应的基尼系数的期望。期望值最小的划分点，作为最佳分裂使用的特征划分。</p>
<h1 id="spark-决策树源码分析"><a href="#spark-决策树源码分析" class="headerlink" title="spark 决策树源码分析"></a>spark 决策树源码分析</h1><p>为加深对ALS算法的理解，该部分主要分析spark mllib中决策树源码的实现。主要包括模型训练、模型预测2个部分</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="决策树伴生类"><a href="#决策树伴生类" class="headerlink" title="决策树伴生类"></a>决策树伴生类</h3><p>DecisionTree伴随类，外部调用决策树模型进行训练的入口。通过外部传入数据和配置参数，调用DecisionTree中的run方法进行模型训练， 最终返回DecisionTreeModel类型对象。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DecisionTree</span> <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">train</span></span>(</span><br><span class="line">      input: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>], <span class="comment">//训练数据，包括label和特征向量</span></span><br><span class="line">      algo: <span class="type">Algo</span>,<span class="comment">//决策树类型，分类树or回归树</span></span><br><span class="line">      impurity: <span class="type">Impurity</span>,<span class="comment">//衡量特征信息增益的标准，如信息增益、基尼、方差</span></span><br><span class="line">      maxDepth: <span class="type">Int</span>,<span class="comment">//树的深度</span></span><br><span class="line">      numClasses: <span class="type">Int</span>,<span class="comment">//待分类类别的数量</span></span><br><span class="line">      maxBins: <span class="type">Int</span>,<span class="comment">//用于特征分裂的bin的最大数量</span></span><br><span class="line">      quantileCalculationStrategy: <span class="type">QuantileStrategy</span>,<span class="comment">//计算分位数的算法</span></span><br><span class="line">      <span class="comment">//离散特征存储，如n-&gt;k表示第n个特征有k个取值（0，1，..., k-1）</span></span><br><span class="line">      categoricalFeaturesInfo: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>]): <span class="type">DecisionTreeModel</span> = &#123; </span><br><span class="line">    <span class="comment">//根据参数信息，生成决策树配置</span></span><br><span class="line">    <span class="keyword">val</span> strategy = <span class="keyword">new</span> <span class="type">Strategy</span>(algo, impurity, maxDepth, numClasses, maxBins,</span><br><span class="line">      quantileCalculationStrategy, categoricalFeaturesInfo)</span><br><span class="line">    <span class="comment">//调用DecisionTree对象的run方法，训练决策树模型</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">DecisionTree</span>(strategy).run(input)</span><br><span class="line">  &#125;</span><br><span class="line">   <span class="comment">//训练分类决策树</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">trainClassifier</span></span>(</span><br><span class="line">      input: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>],</span><br><span class="line">      numClasses: <span class="type">Int</span>,</span><br><span class="line">      categoricalFeaturesInfo: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>],</span><br><span class="line">      impurity: <span class="type">String</span>,</span><br><span class="line">      maxDepth: <span class="type">Int</span>,</span><br><span class="line">      maxBins: <span class="type">Int</span>): <span class="type">DecisionTreeModel</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> impurityType = <span class="type">Impurities</span>.fromString(impurity)</span><br><span class="line">    train(input, <span class="type">Classification</span>, impurityType, maxDepth, numClasses, maxBins, <span class="type">Sort</span>,categoricalFeaturesInfo)</span><br><span class="line">  &#125;</span><br><span class="line">    <span class="comment">//训练回归决策树</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trainRegressor</span></span>(</span><br><span class="line">      input: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>],</span><br><span class="line">      categoricalFeaturesInfo: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>],</span><br><span class="line">      impurity: <span class="type">String</span>,</span><br><span class="line">      maxDepth: <span class="type">Int</span>,</span><br><span class="line">      maxBins: <span class="type">Int</span>): <span class="type">DecisionTreeModel</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> impurityType = <span class="type">Impurities</span>.fromString(impurity) <span class="comment">//基尼、熵、方差三种衡量标准</span></span><br><span class="line">    train(input, <span class="type">Regression</span>, impurityType, maxDepth, <span class="number">0</span>, maxBins, <span class="type">Sort</span>, categoricalFeaturesInfo)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="决策树类"><a href="#决策树类" class="headerlink" title="决策树类"></a>决策树类</h3><p>接受strategy参数初始化，并通过对run方法调用随机森林的run方法，通过设置特征集合为全集、树的个数为1，将随机森林训练后结果集中的第一棵树作为结果返回。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class DecisionTree private[spark] (private val strategy: Strategy, private val seed: Int)</span><br><span class="line">  extends Serializable with Logging &#123;</span><br><span class="line">  def run(input: RDD[LabeledPoint]): DecisionTreeModel = &#123;</span><br><span class="line">    val rf = new RandomForest(strategy, numTrees = 1, featureSubsetStrategy = &quot;all&quot;, seed = seed)</span><br><span class="line">    val rfModel = rf.run(input)</span><br><span class="line">    rfModel.trees(0)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="RandomForest私有类run方法-通过run方法完成模型的训练"><a href="#RandomForest私有类run方法-通过run方法完成模型的训练" class="headerlink" title="RandomForest私有类run方法,通过run方法完成模型的训练"></a>RandomForest私有类run方法,通过run方法完成模型的训练</h3><p><strong>分布式训练思想：</strong></p>
<ul>
<li>分布式存储样本</li>
<li>对于每次迭代，算法都会对一个node集合进行分裂。对于每个node，相关worker计算的的所有相关统计特征全部传递到某个worker进行汇总，并选择最好的特征分裂</li>
<li>findSplitsBins方法可用于将连续特征离散化，在初始化阶段完成</li>
<li>迭代算法<br>每次都作用于树的边缘节点，如果是随机森林，则选择所有的树的边缘节点。具体迭代步骤如下：<ol>
<li>Master 节点: 从node queue中选取节点，如果训练的是随机森林,且featureSubsetStrategy取值不是all，则对于每个节点选择随机特征子集。selectNodesToSplit用于选择待分裂的节点。</li>
<li>Worer节点: findBestSplits函数，对每个(tree, node, feature, split)，遍历所有本地所有样本计算相关特征，计算结果通过reduceByKey传递给某个节点，由该节点汇总数据，得到(feature, split)或者判断是否停止分裂</li>
<li>Master节点: 收集所有节点分裂信息，更新model, 并将新的model传递给各个worker节点 </li>
</ol>
</li>
</ul>
<p>####<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line">def run(</span><br><span class="line">      input: RDD[LabeledPoint],</span><br><span class="line">      strategy: OldStrategy,</span><br><span class="line">      numTrees: Int,</span><br><span class="line">      featureSubsetStrategy: String,</span><br><span class="line">      seed: Long,</span><br><span class="line">      instr: Option[Instrumentation[_]],</span><br><span class="line">      parentUID: Option[String] = None): Array[DecisionTreeModel] = &#123;</span><br><span class="line">    val timer = new TimeTracker()</span><br><span class="line">    timer.start(&quot;total&quot;)</span><br><span class="line">    timer.start(&quot;init&quot;)</span><br><span class="line">    </span><br><span class="line">    val retaggedInput = input.retag(classOf[LabeledPoint])</span><br><span class="line">    //构建元数据</span><br><span class="line">    val metadata =</span><br><span class="line">      DecisionTreeMetadata.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)</span><br><span class="line">    instr match &#123;</span><br><span class="line">      case Some(instrumentation) =&gt;</span><br><span class="line">        instrumentation.logNumFeatures(metadata.numFeatures)</span><br><span class="line">        instrumentation.logNumClasses(metadata.numClasses)</span><br><span class="line">      case None =&gt;</span><br><span class="line">        logInfo(&quot;numFeatures: &quot; + metadata.numFeatures)</span><br><span class="line">        logInfo(&quot;numClasses: &quot; + metadata.numClasses)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //每个特征对应的splits和bins</span><br><span class="line">    timer.start(&quot;findSplits&quot;)</span><br><span class="line">    val splits = findSplits(retaggedInput, metadata, seed)</span><br><span class="line">    timer.stop(&quot;findSplits&quot;)</span><br><span class="line">    logDebug(&quot;numBins: feature: number of bins&quot;)</span><br><span class="line">    logDebug(Range(0, metadata.numFeatures).map &#123; featureIndex =&gt;</span><br><span class="line">      s&quot;\t$featureIndex\t$&#123;metadata.numBins(featureIndex)&#125;&quot;</span><br><span class="line">    &#125;.mkString(&quot;\n&quot;))</span><br><span class="line"></span><br><span class="line">    // Bin feature values (TreePoint representation).</span><br><span class="line">    // Cache input RDD for speedup during multiple passes.</span><br><span class="line">    //输入</span><br><span class="line">    val treeInput = TreePoint.convertToTreeRDD(retaggedInput, splits, metadata)</span><br><span class="line"></span><br><span class="line">    val withReplacement = numTrees &gt; 1</span><br><span class="line"></span><br><span class="line">    val baggedInput = BaggedPoint</span><br><span class="line">      .convertToBaggedRDD(treeInput, strategy.subsamplingRate, numTrees, withReplacement, seed)</span><br><span class="line">      .persist(StorageLevel.MEMORY_AND_DISK)</span><br><span class="line"></span><br><span class="line">    // depth of the decision tree</span><br><span class="line">    val maxDepth = strategy.maxDepth</span><br><span class="line">    require(maxDepth &lt;= 30,</span><br><span class="line">      s&quot;DecisionTree currently only supports maxDepth &lt;= 30, but was given maxDepth = $maxDepth.&quot;)</span><br><span class="line"></span><br><span class="line">    // Max memory usage for aggregates</span><br><span class="line">    // TODO: Calculate memory usage more precisely.</span><br><span class="line">    val maxMemoryUsage: Long = strategy.maxMemoryInMB * 1024L * 1024L</span><br><span class="line">    logDebug(&quot;max memory usage for aggregates = &quot; + maxMemoryUsage + &quot; bytes.&quot;)</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * The main idea here is to perform group-wise training of the decision tree nodes thus</span><br><span class="line">     * reducing the passes over the data from (# nodes) to (# nodes / maxNumberOfNodesPerGroup).</span><br><span class="line">     * Each data sample is handled by a particular node (or it reaches a leaf and is not used</span><br><span class="line">     * in lower levels).</span><br><span class="line">     */</span><br><span class="line"></span><br><span class="line">    // Create an RDD of node Id cache.</span><br><span class="line">    // At first, all the rows belong to the root nodes (node Id == 1).</span><br><span class="line">    val nodeIdCache = if (strategy.useNodeIdCache) &#123;</span><br><span class="line">      Some(NodeIdCache.init(</span><br><span class="line">        data = baggedInput,</span><br><span class="line">        numTrees = numTrees,</span><br><span class="line">        checkpointInterval = strategy.checkpointInterval,</span><br><span class="line">        initVal = 1))</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      None</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">      Stack of nodes to train: (treeIndex, node)</span><br><span class="line">      The reason this is a stack is that we train many trees at once, but we want to focus on</span><br><span class="line">      completing trees, rather than training all simultaneously.  If we are splitting nodes from</span><br><span class="line">      1 tree, then the new nodes to split will be put at the top of this stack, so we will continue</span><br><span class="line">      training the same tree in the next iteration.  This focus allows us to send fewer trees to</span><br><span class="line">      workers on each iteration; see topNodesForGroup below.</span><br><span class="line">     */</span><br><span class="line">    val nodeStack = new mutable.Stack[(Int, LearningNode)]</span><br><span class="line"></span><br><span class="line">    val rng = new Random()</span><br><span class="line">    rng.setSeed(seed)</span><br><span class="line"></span><br><span class="line">    // Allocate and queue root nodes.</span><br><span class="line">    val topNodes = Array.fill[LearningNode](numTrees)(LearningNode.emptyNode(nodeIndex = 1))</span><br><span class="line">    Range(0, numTrees).foreach(treeIndex =&gt; nodeStack.push((treeIndex, topNodes(treeIndex))))</span><br><span class="line"></span><br><span class="line">    timer.stop(&quot;init&quot;)</span><br><span class="line"></span><br><span class="line">    while (nodeStack.nonEmpty) &#123;</span><br><span class="line">      // Collect some nodes to split, and choose features for each node (if subsampling).</span><br><span class="line">      // Each group of nodes may come from one or multiple trees, and at multiple levels.</span><br><span class="line">      val (nodesForGroup, treeToNodeToIndexInfo) =</span><br><span class="line">        RandomForest.selectNodesToSplit(nodeStack, maxMemoryUsage, metadata, rng)</span><br><span class="line">      // Sanity check (should never occur):</span><br><span class="line">      assert(nodesForGroup.nonEmpty,</span><br><span class="line">        s&quot;RandomForest selected empty nodesForGroup.  Error for unknown reason.&quot;)</span><br><span class="line"></span><br><span class="line">      // Only send trees to worker if they contain nodes being split this iteration.</span><br><span class="line">      val topNodesForGroup: Map[Int, LearningNode] =</span><br><span class="line">        nodesForGroup.keys.map(treeIdx =&gt; treeIdx -&gt; topNodes(treeIdx)).toMap</span><br><span class="line"></span><br><span class="line">      // Choose node splits, and enqueue new nodes as needed.</span><br><span class="line">      timer.start(&quot;findBestSplits&quot;)</span><br><span class="line">      RandomForest.findBestSplits(baggedInput, metadata, topNodesForGroup, nodesForGroup,</span><br><span class="line">        treeToNodeToIndexInfo, splits, nodeStack, timer, nodeIdCache)</span><br><span class="line">      timer.stop(&quot;findBestSplits&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    baggedInput.unpersist()</span><br><span class="line"></span><br><span class="line">    timer.stop(&quot;total&quot;)</span><br><span class="line"></span><br><span class="line">    logInfo(&quot;Internal timing for DecisionTree:&quot;)</span><br><span class="line">    logInfo(s&quot;$timer&quot;)</span><br><span class="line"></span><br><span class="line">    // Delete any remaining checkpoints used for node Id cache.</span><br><span class="line">    if (nodeIdCache.nonEmpty) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        nodeIdCache.get.deleteAllCheckpoints()</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        case e: IOException =&gt;</span><br><span class="line">          logWarning(s&quot;delete all checkpoints failed. Error reason: $&#123;e.getMessage&#125;&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val numFeatures = metadata.numFeatures</span><br><span class="line"></span><br><span class="line">    parentUID match &#123;</span><br><span class="line">      case Some(uid) =&gt;</span><br><span class="line">        if (strategy.algo == OldAlgo.Classification) &#123;</span><br><span class="line">          topNodes.map &#123; rootNode =&gt;</span><br><span class="line">            new DecisionTreeClassificationModel(uid, rootNode.toNode, numFeatures,</span><br><span class="line">              strategy.getNumClasses)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          topNodes.map &#123; rootNode =&gt;</span><br><span class="line">            new DecisionTreeRegressionModel(uid, rootNode.toNode, numFeatures)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      case None =&gt;</span><br><span class="line">        if (strategy.algo == OldAlgo.Classification) &#123;</span><br><span class="line">          topNodes.map &#123; rootNode =&gt;</span><br><span class="line">            new DecisionTreeClassificationModel(rootNode.toNode, numFeatures,</span><br><span class="line">              strategy.getNumClasses)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          topNodes.map(rootNode =&gt; new DecisionTreeRegressionModel(rootNode.toNode, numFeatures))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="buildMetadata"><a href="#buildMetadata" class="headerlink" title="buildMetadata"></a>buildMetadata</h4><p>决策树训练的元数据构造。主要用于计算每个特征的bin数量，以及无序类特征集合, 每个节点使用的特征数量等。其中决策树一般使用所有特征、随机森林分类采用$sqrt(n)$个特征，随机森林回归采用$\frac{n}{3}$个特征</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">def buildMetadata(</span><br><span class="line">      input: RDD[LabeledPoint],</span><br><span class="line">      strategy: Strategy,</span><br><span class="line">      numTrees: Int,</span><br><span class="line">      featureSubsetStrategy: String): DecisionTreeMetadata = &#123;</span><br><span class="line">    //特征数量</span><br><span class="line">    val numFeatures = input.map(_.features.size).take(1).headOption.getOrElse &#123;</span><br><span class="line">      throw new IllegalArgumentException(s&quot;DecisionTree requires size of input RDD &gt; 0, &quot; +</span><br><span class="line">        s&quot;but was given by empty one.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    val numExamples = input.count() //样本数量</span><br><span class="line">    val numClasses = strategy.algo match &#123;</span><br><span class="line">      case Classification =&gt; strategy.numClasses</span><br><span class="line">      case Regression =&gt; 0</span><br><span class="line">    &#125;</span><br><span class="line">    //最大划分数量 </span><br><span class="line">    val maxPossibleBins = math.min(strategy.maxBins, numExamples).toInt</span><br><span class="line">    if (maxPossibleBins &lt; strategy.maxBins) &#123;</span><br><span class="line">      logWarning(s&quot;DecisionTree reducing maxBins from $&#123;strategy.maxBins&#125; to $maxPossibleBins&quot; +</span><br><span class="line">        s&quot; (= number of training instances)&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    //maxPossibleBins可能被numExamples修改过，导致小于刚开始设置的strategy.maxBins。</span><br><span class="line">    //需要进一步确保离散值的特征取值数量小于maxPossibleBins，</span><br><span class="line">    if (strategy.categoricalFeaturesInfo.nonEmpty) &#123;</span><br><span class="line">      val maxCategoriesPerFeature = strategy.categoricalFeaturesInfo.values.max</span><br><span class="line">      val maxCategory =</span><br><span class="line">        strategy.categoricalFeaturesInfo.find(_._2 == maxCategoriesPerFeature).get._1</span><br><span class="line">      require(maxCategoriesPerFeature &lt;= maxPossibleBins,</span><br><span class="line">        s&quot;DecisionTree requires maxBins (= $maxPossibleBins) to be at least as large as the &quot; +</span><br><span class="line">        s&quot;number of values in each categorical feature, but categorical feature $maxCategory &quot; +</span><br><span class="line">        s&quot;has $maxCategoriesPerFeature values. Considering remove this and other categorical &quot; +</span><br><span class="line">        &quot;features with a large number of values, or add more training examples.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    //存储每个无序特征的索引</span><br><span class="line">    val unorderedFeatures = new mutable.HashSet[Int]()</span><br><span class="line">    //存储每个无序特征的bin数量</span><br><span class="line">    val numBins = Array.fill[Int](numFeatures)(maxPossibleBins)</span><br><span class="line">    if (numClasses &gt; 2) &#123; //多分类问题</span><br><span class="line">      //根据maxPossibleBins，计算每个无序特征对应的最大类别数量</span><br><span class="line">      val maxCategoriesForUnorderedFeature =</span><br><span class="line">        ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt</span><br><span class="line">      strategy.categoricalFeaturesInfo.foreach &#123; case (featureIndex, numCategories) =&gt;</span><br><span class="line">        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤</span><br><span class="line">          if (numCategories &gt; 1) &#123;</span><br><span class="line">          //判断离散特征是否可当做无序特征，需要保证</span><br><span class="line">          //bins的数量需要小于2 * ((1 &lt;&lt; numCategories - 1) - 1)）</span><br><span class="line">          if (numCategories &lt;= maxCategoriesForUnorderedFeature) &#123;</span><br><span class="line">            unorderedFeatures.add(featureIndex)</span><br><span class="line">            //有numCategories个取值的的特征，对应bins数量为(1 &lt;&lt; numCategories - 1) - 1</span><br><span class="line">            //此处刚开始有点疑惑，感觉应该是2 *（(1 &lt;&lt; numCategories - 1) - 1）</span><br><span class="line">            //通过DecisionTreeMetadata中numSplits函数发现，此处的bin数量和split数量有一定对应关系，(featureIndex)</span><br><span class="line">           //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1</span><br><span class="line">            numBins(featureIndex) = numUnorderedBins(numCategories)</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            //对于其他离散特征，numBins数量为特征可能的取值数量</span><br><span class="line">            numBins(featureIndex) = numCategories</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123; //对于二值分类或回归问题</span><br><span class="line">      strategy.categoricalFeaturesInfo.foreach &#123; case (featureIndex, numCategories) =&gt;</span><br><span class="line">        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤</span><br><span class="line">        if (numCategories &gt; 1) &#123;</span><br><span class="line">          //numBins数量为特征可能的取值数量</span><br><span class="line">          numBins(featureIndex) = numCategories </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //设置每个分支节点对应的特征数量</span><br><span class="line">    val _featureSubsetStrategy = featureSubsetStrategy match &#123;</span><br><span class="line">      case &quot;auto&quot; =&gt;</span><br><span class="line">        if (numTrees == 1) &#123; //如果是树，使用所有特征n</span><br><span class="line">          &quot;all&quot;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          if (strategy.algo == Classification) &#123; //如果是用于分类的随机森林，使用sqrt(n)个特征</span><br><span class="line">            &quot;sqrt&quot;</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            &quot;onethird&quot;  //如果是用于回归的随机森林，使用n/3个特征</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      case _ =&gt; featureSubsetStrategy</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val numFeaturesPerNode: Int = _featureSubsetStrategy match &#123;</span><br><span class="line">      case &quot;all&quot; =&gt; numFeatures</span><br><span class="line">      case &quot;sqrt&quot; =&gt; math.sqrt(numFeatures).ceil.toInt</span><br><span class="line">      case &quot;log2&quot; =&gt; math.max(1, (math.log(numFeatures) / math.log(2)).ceil.toInt)</span><br><span class="line">      case &quot;onethird&quot; =&gt; (numFeatures / 3.0).ceil.toInt</span><br><span class="line">      case _ =&gt;</span><br><span class="line">        Try(_featureSubsetStrategy.toInt).filter(_ &gt; 0).toOption match &#123;</span><br><span class="line">          case Some(value) =&gt; math.min(value, numFeatures)</span><br><span class="line">          case None =&gt;</span><br><span class="line">            Try(_featureSubsetStrategy.toDouble).filter(_ &gt; 0).filter(_ &lt;= 1.0).toOption match &#123;</span><br><span class="line">              case Some(value) =&gt; math.ceil(value * numFeatures).toInt</span><br><span class="line">              case _ =&gt; throw new IllegalArgumentException(s&quot;Supported values:&quot; +</span><br><span class="line">                s&quot; $&#123;RandomForestParams.supportedFeatureSubsetStrategies.mkString(&quot;, &quot;)&#125;,&quot; +</span><br><span class="line">                s&quot; (0.0-1.0], [1-n].&quot;)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    new DecisionTreeMetadata(numFeatures, numExamples, numClasses, numBins.max,</span><br><span class="line">      strategy.categoricalFeaturesInfo, unorderedFeatures.toSet, numBins,</span><br><span class="line">      strategy.impurity, strategy.quantileCalculationStrategy, strategy.maxDepth,</span><br><span class="line">      strategy.minInstancesPerNode, strategy.minInfoGain, numTrees, numFeaturesPerNode)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="DecisionTreeMetadata类"><a href="#DecisionTreeMetadata类" class="headerlink" title="DecisionTreeMetadata类"></a>DecisionTreeMetadata类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">private[spark] class DecisionTreeMetadata(</span><br><span class="line">    val numFeatures: Int,</span><br><span class="line">    val numExamples: Long,</span><br><span class="line">    val numClasses: Int,</span><br><span class="line">    val maxBins: Int,</span><br><span class="line">    val featureArity: Map[Int, Int],</span><br><span class="line">    val unorderedFeatures: Set[Int],</span><br><span class="line">    val numBins: Array[Int],</span><br><span class="line">    val impurity: Impurity,</span><br><span class="line">    val quantileStrategy: QuantileStrategy,</span><br><span class="line">    val maxDepth: Int,</span><br><span class="line">    val minInstancesPerNode: Int,</span><br><span class="line">    val minInfoGain: Double,</span><br><span class="line">    val numTrees: Int,</span><br><span class="line">    val numFeaturesPerNode: Int) extends Serializable &#123;</span><br><span class="line">  //判断是否为无序特征</span><br><span class="line">  def isUnordered(featureIndex: Int): Boolean = unorderedFeatures.contains(featureIndex)</span><br><span class="line">  //判断是否用于分类的决策树（随机森林）</span><br><span class="line">  def isClassification: Boolean = numClasses &gt;= 2</span><br><span class="line">  //判断是否用于多分类的决策树（随机森林）</span><br><span class="line">  def isMulticlass: Boolean = numClasses &gt; 2</span><br><span class="line">  //判断是否拥有离散特征的多分类决策树（随机森林）</span><br><span class="line">  def isMulticlassWithCategoricalFeatures: Boolean = isMulticlass &amp;&amp; (featureArity.size &gt; 0)</span><br><span class="line">  //判断是否离散特征</span><br><span class="line">  def isCategorical(featureIndex: Int): Boolean = featureArity.contains(featureIndex)</span><br><span class="line"> //判断是否连续特征</span><br><span class="line">  def isContinuous(featureIndex: Int): Boolean = !featureArity.contains(featureIndex)</span><br><span class="line">  //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1</span><br><span class="line">  def numSplits(featureIndex: Int): Int = if (isUnordered(featureIndex)) &#123;</span><br><span class="line">    numBins(featureIndex)</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    numBins(featureIndex) - 1</span><br><span class="line">  &#125;</span><br><span class="line">  //对于连续特征，根据划分数量设置bin数量为划分数量加1</span><br><span class="line">  def setNumSplits(featureIndex: Int, numSplits: Int) &#123;</span><br><span class="line">    require(isContinuous(featureIndex),</span><br><span class="line">      s&quot;Only number of bin for a continuous feature can be set.&quot;)</span><br><span class="line">    numBins(featureIndex) = numSplits + 1</span><br><span class="line">  &#125;</span><br><span class="line">  //判断是否需要对特征进行采样</span><br><span class="line">  def subsamplingFeatures: Boolean = numFeatures != numFeaturesPerNode</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="findSplits"><a href="#findSplits" class="headerlink" title="findSplits"></a>findSplits</h4><p>通过使用采样的样本，寻找样本的划分splits和划分后的bins。</p>
<p><strong>划分的思想：</strong>对连续特征和离散特征，分别采用不同处理方式。对于每个连续特征，numBins - 1个splits, 代表每个树的节点的所有可能的二值化分；对于每个离散特征，无序离散特征（用于多分类的维度较大的feature）基于特征的子集进行划分。有序类特征（用于回归、二分类、多分类的维度较小的feature)的每个取值对应一个bin.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">protected[tree] def findSplits(</span><br><span class="line">      input: RDD[LabeledPoint],</span><br><span class="line">      metadata: DecisionTreeMetadata,</span><br><span class="line">      seed: Long): Array[Array[Split]] = &#123;</span><br><span class="line">    logDebug(&quot;isMulticlass = &quot; + metadata.isMulticlass)</span><br><span class="line">    val numFeatures = metadata.numFeatures //特征的数量</span><br><span class="line">    // 得到所有连续特征索引</span><br><span class="line">    val continuousFeatures = Range(0, numFeatures).filter(metadata.isContinuous)</span><br><span class="line">    //当有连续特征的时候需要采样样本   </span><br><span class="line">    val sampledInput = if (continuousFeatures.nonEmpty) &#123;</span><br><span class="line">      // 计算近似分位数计算需要的样本数</span><br><span class="line">      val requiredSamples = math.max(metadata.maxBins * metadata.maxBins, 10000)</span><br><span class="line">      // 计算需要的样本占总样本比例</span><br><span class="line">      val fraction = if (requiredSamples &lt; metadata.numExamples) &#123;</span><br><span class="line">        requiredSamples.toDouble / metadata.numExamples</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        1.0</span><br><span class="line">      &#125;</span><br><span class="line">      logDebug(&quot;fraction of data used for calculating quantiles = &quot; + fraction)</span><br><span class="line">      input.sample(withReplacement = false, fraction, new XORShiftRandom(seed).nextInt())</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      input.sparkContext.emptyRDD[LabeledPoint]</span><br><span class="line">    &#125;</span><br><span class="line">    //对每个连续特征和非有序类离散特征，通过排序的方式，寻找最佳的splits点</span><br><span class="line">    findSplitsBySorting(sampledInput, metadata, continuousFeatures)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">//对每个特征，通过排序的方式，寻找最佳的splits点</span><br><span class="line">private def findSplitsBySorting(</span><br><span class="line">     input: RDD[LabeledPoint],</span><br><span class="line">     metadata: DecisionTreeMetadata,</span><br><span class="line">     continuousFeatures: IndexedSeq[Int]): Array[Array[Split]] = &#123;</span><br><span class="line">  </span><br><span class="line">   //寻找连续特征的划分阈值</span><br><span class="line">   val continuousSplits: scala.collection.Map[Int, Array[Split]] = &#123;</span><br><span class="line">     //设置分区数量，如果连续特征的数量小于原始分区数，则进一步减少分区，防止无效的启动的task任务。</span><br><span class="line">     val numPartitions = math.min(continuousFeatures.length, input.partitions.length)</span><br><span class="line"></span><br><span class="line">     input</span><br><span class="line">       .flatMap(point =&gt; continuousFeatures.map(idx =&gt; (idx, point.features(idx))))</span><br><span class="line">       .groupByKey(numPartitions)</span><br><span class="line">       .map &#123; case (idx, samples) =&gt;</span><br><span class="line">         val thresholds = findSplitsForContinuousFeature(samples, metadata, idx)</span><br><span class="line">         val splits: Array[Split] = thresholds.map(thresh =&gt; new ContinuousSplit(idx, thresh))</span><br><span class="line">         logDebug(s&quot;featureIndex = $idx, numSplits = $&#123;splits.length&#125;&quot;)</span><br><span class="line">         (idx, splits)</span><br><span class="line">       &#125;.collectAsMap()</span><br><span class="line">   &#125;</span><br><span class="line">   //特征数量</span><br><span class="line">   val numFeatures = metadata.numFeatures</span><br><span class="line">   //汇总所有特征的split(不包括无序离散特征)</span><br><span class="line">   val splits: Array[Array[Split]] = Array.tabulate(numFeatures) &#123;</span><br><span class="line">     //如果是连续特征，返回该连续特征的split</span><br><span class="line">     case i if metadata.isContinuous(i) =&gt;</span><br><span class="line">       val split = continuousSplits(i)</span><br><span class="line">       metadata.setNumSplits(i, split.length)</span><br><span class="line">       split</span><br><span class="line">     //如果是无序离散特征，则提取该特征的split， 具体是对于每个离散特征，其第k个split为其k对应二进制的所有位置为1的数值。</span><br><span class="line">     case i if metadata.isCategorical(i) &amp;&amp; metadata.isUnordered(i) =&gt;</span><br><span class="line">       // Unordered features</span><br><span class="line">       // 2^(maxFeatureValue - 1) - 1 combinations</span><br><span class="line">       //特征的取值数量</span><br><span class="line">       val featureArity = metadata.featureArity(i)</span><br><span class="line">       Array.tabulate[Split](metadata.numSplits(i)) &#123; splitIndex =&gt;</span><br><span class="line">         val categories = extractMultiClassCategories(splitIndex + 1, featureArity)</span><br><span class="line">         new CategoricalSplit(i, categories.toArray, featureArity)</span><br><span class="line">       &#125;</span><br><span class="line">     //对于有序离散特征，暂时不求解split, 在训练阶段求解</span><br><span class="line">     case i if metadata.isCategorical(i) =&gt;</span><br><span class="line">       // Ordered features</span><br><span class="line">       //   Splits are constructed as needed during training.</span><br><span class="line">       Array.empty[Split]</span><br><span class="line">   &#125;</span><br><span class="line">   splits</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//将input这个数对应的二进制位置为1的位置加入到当前划分</span><br><span class="line">private[tree] def extractMultiClassCategories(</span><br><span class="line">      input: Int,</span><br><span class="line">      maxFeatureValue: Int): List[Double] = &#123;</span><br><span class="line">    var categories = List[Double]()</span><br><span class="line">    var j = 0</span><br><span class="line">    var bitShiftedInput = input</span><br><span class="line">    while (j &lt; maxFeatureValue) &#123;</span><br><span class="line">      if (bitShiftedInput % 2 != 0) &#123;</span><br><span class="line">        // updating the list of categories.</span><br><span class="line">        categories = j.toDouble :: categories</span><br><span class="line">      &#125;</span><br><span class="line">      // Right shift by one</span><br><span class="line">      bitShiftedInput = bitShiftedInput &gt;&gt; 1</span><br><span class="line">      j += 1</span><br><span class="line">    &#125;</span><br><span class="line">    categories</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">//对于连续特征，找到其对应的splits分割点</span><br><span class="line">private[tree] def findSplitsForContinuousFeature(</span><br><span class="line">      featureSamples: Iterable[Double], </span><br><span class="line">      metadata: DecisionTreeMetadata, </span><br><span class="line">      featureIndex: Int): Array[Double] = &#123;</span><br><span class="line">    //确保有连续特征</span><br><span class="line">    require(metadata.isContinuous(featureIndex),</span><br><span class="line">      &quot;findSplitsForContinuousFeature can only be used to find splits for a continuous feature.&quot;)</span><br><span class="line">    //寻找splits分割点</span><br><span class="line">    val splits = if (featureSamples.isEmpty) &#123;</span><br><span class="line">      Array.empty[Double]  //如果样本数为0， 返回空数组</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      //得到metadata里的split数量</span><br><span class="line">      val numSplits = metadata.numSplits(featureIndex) </span><br><span class="line"></span><br><span class="line">      //在采样得到的样本中，计算每个特征取值的计数、以及总样本数量</span><br><span class="line">      val (valueCountMap, numSamples) = featureSamples.foldLeft((Map.empty[Double, Int], 0)) &#123;</span><br><span class="line">        case ((m, cnt), x) =&gt;</span><br><span class="line">          (m + ((x, m.getOrElse(x, 0) + 1)), cnt + 1)</span><br><span class="line">      &#125;</span><br><span class="line">      // 对于每个特征取值进行排序</span><br><span class="line">      val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray</span><br><span class="line">      //如果得到的possible splits数量小于metadata中该特征的的split数量，则直接以当前每个特征取值作为分割的阈值</span><br><span class="line">      val possibleSplits = valueCounts.length - 1</span><br><span class="line">      if (possibleSplits &lt;= numSplits) &#123; </span><br><span class="line">        valueCounts.map(_._1).init</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        //否则，根据总样本数量，计算平均每个区间对应的特征取值数量，假设为n。然后，对于n, 2*n, 3*n ...的位置分别设置标记。设置2个游标分别指向valueCounts内部连续的两个特征取值，从前向后遍历，当后面游标到标记的距离大于前面的游标时，将前面游标的位置对应的特征取值设置为一个split点。</span><br><span class="line">        //计算平均每个区间对应的特征取值数量</span><br><span class="line">        val stride: Double = numSamples.toDouble / (numSplits + 1)</span><br><span class="line">        logDebug(&quot;stride = &quot; + stride)</span><br><span class="line">        //splitsBuilder用于存储每个分割阈值</span><br><span class="line">        val splitsBuilder = mutable.ArrayBuilder.make[Double]</span><br><span class="line">        //特征取值从小到大的位置索引</span><br><span class="line">        var index = 1</span><br><span class="line">        //当前访问的所有特征取值数量之和</span><br><span class="line">        var currentCount = valueCounts(0)._2</span><br><span class="line">        //下一次的标记位置      </span><br><span class="line">        var targetCount = stride</span><br><span class="line">        while (index &lt; valueCounts.length) &#123;</span><br><span class="line">          val previousCount = currentCount</span><br><span class="line">          currentCount += valueCounts(index)._2</span><br><span class="line">          val previousGap = math.abs(previousCount - targetCount)</span><br><span class="line">          val currentGap = math.abs(currentCount - targetCount)</span><br><span class="line">          //使前面游标和后面游标的距离更小，且较小游标距离标记位置的距离最近</span><br><span class="line">          if (previousGap &lt; currentGap) &#123;</span><br><span class="line">            splitsBuilder += valueCounts(index - 1)._1</span><br><span class="line">            targetCount += stride</span><br><span class="line">          &#125;</span><br><span class="line">          index += 1</span><br><span class="line">        &#125;</span><br><span class="line">        splitsBuilder.result()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    splits</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="TreePoint-convertToTreeRDD"><a href="#TreePoint-convertToTreeRDD" class="headerlink" title="TreePoint.convertToTreeRDD"></a>TreePoint.convertToTreeRDD</h4><p>调用TreePoint类的convertToTreeRDD方法，RDD[LabeledPoint]转化为RDD[TreePoint]。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def convertToTreeRDD(</span><br><span class="line">     input: RDD[LabeledPoint],</span><br><span class="line">     splits: Array[Array[Split]],</span><br><span class="line">     metadata: DecisionTreeMetadata): RDD[TreePoint] = &#123;</span><br><span class="line">   // 构建数组featureArity，存储每个特征对应的离散值个数，连续值对应的value为0</span><br><span class="line">   val featureArity: Array[Int] = new Array[Int](metadata.numFeatures)</span><br><span class="line">   var featureIndex = 0</span><br><span class="line">   while (featureIndex &lt; metadata.numFeatures) &#123;</span><br><span class="line">     featureArity(featureIndex) = metadata.featureArity.getOrElse(featureIndex, 0)</span><br><span class="line">     featureIndex += 1</span><br><span class="line">   &#125;</span><br><span class="line">   //获得所有连续特征的分裂阈值，如果是离散特征，则数组对应空</span><br><span class="line">   val thresholds: Array[Array[Double]] = featureArity.zipWithIndex.map &#123; case (arity, idx) =&gt;</span><br><span class="line">     if (arity == 0) &#123;</span><br><span class="line">       splits(idx).map(_.asInstanceOf[ContinuousSplit].threshold)</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       Array.empty[Double]</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   //将样本的每个原始特征，转化为对应的bin特征值，用于训练</span><br><span class="line">   input.map &#123; x =&gt;</span><br><span class="line">     TreePoint.labeledPointToTreePoint(x, thresholds, featureArity)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//将单个样本的原始特征，转化为对应的bin特征值，用于训练</span><br><span class="line">private def labeledPointToTreePoint(</span><br><span class="line">    labeledPoint: LabeledPoint,</span><br><span class="line">    thresholds: Array[Array[Double]],</span><br><span class="line">    featureArity: Array[Int]): TreePoint = &#123;</span><br><span class="line">  //特征数量</span><br><span class="line">  val numFeatures = labeledPoint.features.size</span><br><span class="line">  //为每个特征找到对应的bin特征值，存储在arr数组</span><br><span class="line">  val arr = new Array[Int](numFeatures)</span><br><span class="line">  var featureIndex = 0</span><br><span class="line">  while (featureIndex &lt; numFeatures) &#123;</span><br><span class="line">    //寻找数据点labeledPoint、当前特征featureIndex对应的bin特征值</span><br><span class="line">    arr(featureIndex) =</span><br><span class="line">      findBin(featureIndex, labeledPoint, featureArity(featureIndex), thresholds(featureIndex))</span><br><span class="line">    featureIndex += 1</span><br><span class="line">  &#125;</span><br><span class="line">  new TreePoint(labeledPoint.label, arr)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">private def findBin(</span><br><span class="line">      featureIndex: Int,</span><br><span class="line">      labeledPoint: LabeledPoint,</span><br><span class="line">      featureArity: Int,</span><br><span class="line">      thresholds: Array[Double]): Int = &#123;</span><br><span class="line">    //获取当前labeledPoint的第featureIndex个原始特征值</span><br><span class="line">    val featureValue = labeledPoint.features(featureIndex)</span><br><span class="line">    </span><br><span class="line">    if (featureArity == 0) &#123; </span><br><span class="line">      //如果是连续特征，利用二分法得到当前特征值对应的离散区间下标</span><br><span class="line">      val idx = java.util.Arrays.binarySearch(thresholds, featureValue)</span><br><span class="line">      if (idx &gt;= 0) &#123;</span><br><span class="line">        idx</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        -idx - 1</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      //如果是离散值，则直接返回当前的特征值</span><br><span class="line">      if (featureValue &lt; 0 || featureValue &gt;= featureArity) &#123;</span><br><span class="line">        throw new IllegalArgumentException(</span><br><span class="line">          s&quot;DecisionTree given invalid data:&quot; +</span><br><span class="line">            s&quot; Feature $featureIndex is categorical with values in &#123;0,...,$&#123;featureArity - 1&#125;,&quot; +</span><br><span class="line">            s&quot; but a data point gives it value $featureValue.\n&quot; +</span><br><span class="line">            &quot;  Bad data point: &quot; + labeledPoint.toString)</span><br><span class="line">      &#125;</span><br><span class="line">      featureValue.toInt</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//LabeledPoint类</span><br><span class="line">case class LabeledPoint(@Since(&quot;2.0.0&quot;) label: Double, @Since(&quot;2.0.0&quot;) features: Vector) &#123;</span><br><span class="line">  override def toString: String = &#123;</span><br><span class="line">    s&quot;($label,$features)&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//TreePoint类</span><br><span class="line">private[spark] class TreePoint(val label: Double, val binnedFeatures: Array[Int])</span><br><span class="line">  extends Serializable &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="BaggedPoint-convertToBaggedRDD"><a href="#BaggedPoint-convertToBaggedRDD" class="headerlink" title="BaggedPoint.convertToBaggedRDD"></a>BaggedPoint.convertToBaggedRDD</h4><p>RDD[Datum]数据集转换成RDD[BaggedPoint[Datum]的表示类型，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def convertToBaggedRDD[Datum] (</span><br><span class="line">    input: RDD[Datum], //输入数据集</span><br><span class="line">    subsamplingRate: Double, //采样率</span><br><span class="line">    numSubsamples: Int, //采样次数</span><br><span class="line">    withReplacement: Boolean, //是否有放回</span><br><span class="line">    //随机数种子</span><br><span class="line">    seed: Long = Utils.random.nextLong()): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class="line">  if (withReplacement) &#123;//有放回采样，生成BaggedPoint结构表示</span><br><span class="line">    convertToBaggedRDDSamplingWithReplacement(input, subsamplingRate, numSubsamples, seed)</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    //当采样比为1，并且采样次数为1时，不采样，只生成BaggedPoint结构表示</span><br><span class="line">    if (numSubsamples == 1 &amp;&amp; subsamplingRate == 1.0) &#123;</span><br><span class="line">      convertToBaggedRDDWithoutSampling(input)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      //无放回采样，生成BaggedPoint结构表示</span><br><span class="line">      convertToBaggedRDDSamplingWithoutReplacement(input, subsamplingRate, numSubsamples, seed)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">//有放回采样，数据转换为RDD[BaggedPoint[Datum]]</span><br><span class="line">private def convertToBaggedRDDSamplingWithReplacement[Datum] (</span><br><span class="line">    input: RDD[Datum],//输入数据集</span><br><span class="line">    subsample: Double,//采样率</span><br><span class="line">    numSubsamples: Int,//采样次数</span><br><span class="line">    //随机数种子</span><br><span class="line">    seed: Long): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class="line">  input.mapPartitionsWithIndex &#123; (partitionIndex, instances) =&gt;</span><br><span class="line">    //每个分区生成一个泊松采样器，通过采样率、随机种子、分区索引等初始化</span><br><span class="line">    val poisson = new PoissonDistribution(subsample)</span><br><span class="line">    poisson.reseedRandomGenerator(seed + partitionIndex + 1)</span><br><span class="line">    //将每个实例变换成BaggedPoint结构表示</span><br><span class="line">    instances.map &#123; instance =&gt;</span><br><span class="line">      val subsampleWeights = new Array[Double](numSubsamples)</span><br><span class="line">      var subsampleIndex = 0</span><br><span class="line">      //依次对每次采样，生成权重（即该实例在每次无放回采样出现的次数）</span><br><span class="line">      while (subsampleIndex &lt; numSubsamples) &#123;</span><br><span class="line">        subsampleWeights(subsampleIndex) = poisson.sample()</span><br><span class="line">        subsampleIndex += 1</span><br><span class="line">      &#125;</span><br><span class="line">      //生成BaggedPoint结构表示</span><br><span class="line">      new BaggedPoint(instance, subsampleWeights) </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//BaggedPoint类，datum表示数据实例，subsampleWeights表示当前实例在每个采样中的权重。</span><br><span class="line">如(datum, [1, 0, 4])表示有3次采样，数据实例在3次采样中出现的次数分别为1，0，4</span><br><span class="line">private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])</span><br><span class="line">  extends Serializable</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//原始数据（不采样）直接转换为BaggedPoint结构表示</span><br><span class="line">private def convertToBaggedRDDWithoutSampling[Datum] (</span><br><span class="line">    input: RDD[Datum]): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class="line">  input.map(datum =&gt; new BaggedPoint(datum, Array(1.0)))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">//无放回采样，数据转换为RDD[BaggedPoint[Datum]]</span><br><span class="line">private def convertToBaggedRDDSamplingWithoutReplacement[Datum] (</span><br><span class="line">    input: RDD[Datum],</span><br><span class="line">    subsamplingRate: Double,</span><br><span class="line">    numSubsamples: Int,</span><br><span class="line">    seed: Long): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class="line">  input.mapPartitionsWithIndex &#123; (partitionIndex, instances) =&gt;</span><br><span class="line">    //使用随机数种子，分区索引，构建随机数生成器</span><br><span class="line">    val rng = new XORShiftRandom</span><br><span class="line">    rng.setSeed(seed + partitionIndex + 1)</span><br><span class="line">    //将每个实例变换成BaggedPoint结构表示</span><br><span class="line">    instances.map &#123; instance =&gt;</span><br><span class="line">      val subsampleWeights = new Array[Double](numSubsamples)</span><br><span class="line">      var subsampleIndex = 0</span><br><span class="line">      //对于每次采样，生成0-1之间的随机数，如果小于采样比，则对应权重为1，否则为0</span><br><span class="line">      while (subsampleIndex &lt; numSubsamples) &#123;</span><br><span class="line">        val x = rng.nextDouble()</span><br><span class="line">        subsampleWeights(subsampleIndex) = &#123;</span><br><span class="line">          if (x &lt; subsamplingRate) 1.0 else 0.0</span><br><span class="line">        &#125;</span><br><span class="line">        subsampleIndex += 1</span><br><span class="line">      &#125;</span><br><span class="line">      //转换为BaggedPoint结构数据</span><br><span class="line">      new BaggedPoint(instance, subsampleWeights)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="RandomForest-selectNodesToSplit"><a href="#RandomForest-selectNodesToSplit" class="headerlink" title="RandomForest.selectNodesToSplit"></a>RandomForest.selectNodesToSplit</h4><p>选择当前迭代待分裂的节点，以及确定每个节点使用的特征。每次选择都根据内存限制、每个节点占用的内存（如果每个节点使用的是采样后的特征），自适应地确定节点个数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">private[tree] def selectNodesToSplit(</span><br><span class="line">      nodeStack: mutable.Stack[(Int, LearningNode)], //存储节点的栈结构</span><br><span class="line">      maxMemoryUsage: Long, //最大占用内存限制</span><br><span class="line">      metadata: DecisionTreeMetadata, //元数据</span><br><span class="line">      //随机数</span><br><span class="line">      rng: Random): </span><br><span class="line">      //返回值包括：（1）每个树对应的待分裂节点数组， </span><br><span class="line">      //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）</span><br><span class="line">      (Map[Int, Array[LearningNode]], Map[Int, Map[Int, NodeIndexInfo]]) = &#123;</span><br><span class="line">      //nodesForGroup(treeIndex) 存储第treeIndex个树对应的待分裂节点数组</span><br><span class="line">      val mutableNodesForGroup = new mutable.HashMap[Int, mutable.ArrayBuffer[LearningNode]]()</span><br><span class="line">      //每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）</span><br><span class="line">      val mutableTreeToNodeToIndexInfo =</span><br><span class="line">      new mutable.HashMap[Int, mutable.HashMap[Int, NodeIndexInfo]]()</span><br><span class="line">      var memUsage: Long = 0L  //当前使用内存</span><br><span class="line">      var numNodesInGroup = 0  //当前分组的节点数量</span><br><span class="line">      // If maxMemoryInMB is set very small, we want to still try to split 1 node,</span><br><span class="line">      // so we allow one iteration if memUsage == 0.</span><br><span class="line">      //如果栈不空，并且（1）如果内存上限设置非常小，我们要去报至少能有1个节点用于分裂</span><br><span class="line">      //（2）当前使用内存小于内存上限值，则进一步选择节点用于分裂</span><br><span class="line">      while (nodeStack.nonEmpty &amp;&amp; (memUsage &lt; maxMemoryUsage || memUsage == 0)) &#123;</span><br><span class="line">      val (treeIndex, node) = nodeStack.top //选择栈顶节点</span><br><span class="line">      // Choose subset of features for node (if subsampling).</span><br><span class="line">     </span><br><span class="line">      val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) &#123;       //如果特征需要采样，则对所有特征进行无放回采样</span><br><span class="line">        Some(SamplingUtils.reservoirSampleAndCount(Range(0,</span><br><span class="line">          metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong())._1)</span><br><span class="line">      &#125; else &#123;//如果特征不需要采样，则返回None</span><br><span class="line">        None</span><br><span class="line">      &#125;</span><br><span class="line">      //通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的内存</span><br><span class="line">      val nodeMemUsage = RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L</span><br><span class="line">      ////检查增加当前节点后，内存容量是是否超过限制</span><br><span class="line">      if (memUsage + nodeMemUsage &lt;= maxMemoryUsage || memUsage == 0) &#123;</span><br><span class="line">        //如果加入该节点后内存没有超过限制</span><br><span class="line">        nodeStack.pop() //当前节点出栈</span><br><span class="line">        //更新mutableNodesForGroup，将当前节点加入对应treeIndex的节点数组</span><br><span class="line">        mutableNodesForGroup.getOrElseUpdate(treeIndex, new mutable.ArrayBuffer[LearningNode]()) +=</span><br><span class="line">          node</span><br><span class="line">        //更新mutableTreeToNodeToIndexInfo，将当前节点的具体信息，加入对应treeindex的节点map</span><br><span class="line">        mutableTreeToNodeToIndexInfo</span><br><span class="line">          .getOrElseUpdate(treeIndex, new mutable.HashMap[Int, NodeIndexInfo]())(node.id)</span><br><span class="line">          = new NodeIndexInfo(numNodesInGroup, featureSubset)</span><br><span class="line">      &#125;</span><br><span class="line">      numNodesInGroup += 1 //当前分组的节点数量加一</span><br><span class="line">      memUsage += nodeMemUsage //当前使用内存数量加一</span><br><span class="line">    &#125;</span><br><span class="line">    if (memUsage &gt; maxMemoryUsage) &#123;</span><br><span class="line">      // If maxMemoryUsage is 0, we should still allow splitting 1 node.</span><br><span class="line">      logWarning(s&quot;Tree learning is using approximately $memUsage bytes per iteration, which&quot; +</span><br><span class="line">        s&quot; exceeds requested limit maxMemoryUsage=$maxMemoryUsage. This allows splitting&quot; +</span><br><span class="line">        s&quot; $numNodesInGroup nodes in this iteration.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    //转换可变map为不可变map类型</span><br><span class="line">    val nodesForGroup: Map[Int, Array[LearningNode]] =</span><br><span class="line">      mutableNodesForGroup.mapValues(_.toArray).toMap</span><br><span class="line">    val treeToNodeToIndexInfo = mutableTreeToNodeToIndexInfo.mapValues(_.toMap).toMap</span><br><span class="line">    //返回（1）每个树对应的待分裂节点数组， </span><br><span class="line">    //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）</span><br><span class="line">    (nodesForGroup, treeToNodeToIndexInfo)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">//无放回采样</span><br><span class="line">def reservoirSampleAndCount[T: ClassTag](</span><br><span class="line">      input: Iterator[T], //input输入的迭代器</span><br><span class="line">      k: Int, //采样的样本数</span><br><span class="line">      seed: Long = Random.nextLong()) //随机数种子</span><br><span class="line">    : (Array[T], Long) = &#123;</span><br><span class="line">    val reservoir = new Array[T](k) //存储采样结果的数组</span><br><span class="line">    // 放置迭代器的前k个元素到结果数组</span><br><span class="line">    var i = 0</span><br><span class="line">    while (i &lt; k &amp;&amp; input.hasNext) &#123;</span><br><span class="line">      val item = input.next()</span><br><span class="line">      reservoir(i) = item</span><br><span class="line">      i += 1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //如果输入元素个数小于k, 则这k个特征作为返回的结果</span><br><span class="line">    if (i &lt; k) &#123;</span><br><span class="line">      // If input size &lt; k, trim the array to return only an array of input size.</span><br><span class="line">      val trimReservoir = new Array[T](i)</span><br><span class="line">      System.arraycopy(reservoir, 0, trimReservoir, 0, i)</span><br><span class="line">      (trimReservoir, i) //返回结果数组，以及原始数组的元素个数</span><br><span class="line">    &#125; else &#123; </span><br><span class="line">      //如果输入元素个数大于k, 继续采样过程，将后面元素以一定概率随机替换前面的某个元素</span><br><span class="line">      var l = i.toLong</span><br><span class="line">      val rand = new XORShiftRandom(seed)</span><br><span class="line">      while (input.hasNext) &#123;</span><br><span class="line">        val item = input.next()</span><br><span class="line">        l += 1</span><br><span class="line">        //当前结果数组有k个元素，l为当前元素的序号。k/l为当前元素替换结果数组中某个元素的概率。</span><br><span class="line">        //在进行替换时，对结果数组的每个元素以相等概率发生替换</span><br><span class="line">        //具体方式是产生一个0到l-1之间的随机整数replacementIndex，</span><br><span class="line">        //如果小于k则对第replacementIndex这个元素进行替换</span><br><span class="line">        val replacementIndex = (rand.nextDouble() * l).toLong</span><br><span class="line">        if (replacementIndex &lt; k) &#123;</span><br><span class="line">          reservoir(replacementIndex.toInt) = item</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      (reservoir, l) //返回结果数组，以及原始数组的元素个数</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">//通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的字节数</span><br><span class="line">private def aggregateSizeForNode(</span><br><span class="line">    metadata: DecisionTreeMetadata,</span><br><span class="line">    featureSubset: Option[Array[Int]]): Long = &#123;</span><br><span class="line">  //得到所有使用的特征的bin的数量之后</span><br><span class="line">  val totalBins = if (featureSubset.nonEmpty) &#123;</span><br><span class="line">    //如果使用采样特征，得到采样后的所有特征bin数量之和</span><br><span class="line">    featureSubset.get.map(featureIndex =&gt; metadata.numBins(featureIndex).toLong).sum</span><br><span class="line">  &#125; else &#123;//否则使用所有的特征的bin数量之和</span><br><span class="line">    metadata.numBins.map(_.toLong).sum</span><br><span class="line">  &#125;</span><br><span class="line">  if (metadata.isClassification) &#123;</span><br><span class="line">    //如果是分类问题，则返回bin数量之和*类别个数</span><br><span class="line">    metadata.numClasses * totalBins </span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    //否则返回bin数量之和*3</span><br><span class="line">    3 * totalBins</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="RandomForest-findBestSplits"><a href="#RandomForest-findBestSplits" class="headerlink" title="RandomForest.findBestSplits"></a>RandomForest.findBestSplits</h4><p>给定selectNodesToSplit方法选择的一组节点，找到每个节点对应的最佳分类特征的分裂位置。<strong>求解的主要思想如下：</strong></p>
<p><strong>基于节点的分组进行并行训练：</strong>对一组的节点同时进行每个bin的统计和计算，减少不必要的数据传输成本。这样每次迭代需要更多的计算和存储成本，但是可以大大减少迭代的次数</p>
<p><strong>基于bin的最佳分割点计算：</strong>基于bin的计算来寻找最佳分割点，计算的思想不是依次对每个样本计算其对每个孩子节点的增益贡献，而是先将所有样本的每个特征映射到对应的bin，通过聚合每个bin的数据，进一步计算对应每个特征每个分割的增益。</p>
<p><strong>对每个partition进行聚合：</strong>由于提取知道了每个特征对应的split个数，因此可以用一个数组存储所有的bin的聚合信息，通过使用RDD的聚合方法，大大减少通讯开销。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line">private[tree] def findBestSplits(</span><br><span class="line">     input: RDD[BaggedPoint[TreePoint]], //训练数据</span><br><span class="line">     metadata: DecisionTreeMetadata, //随机森林元数据信息</span><br><span class="line">     topNodesForGroup: Map[Int, LearningNode], //存储当前节点分组对应的每个树的根节点</span><br><span class="line">     nodesForGroup: Map[Int, Array[LearningNode]],//存储当前节点分组对应的每个树的节点数组</span><br><span class="line">     treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]],//存储当前节点分组对应的每个树索引、节点索引、及详细信息</span><br><span class="line">     splits: Array[Array[Split]], //存储每个特征的所有split信息</span><br><span class="line">     //存储节点的栈结构，初始化时为各个树的根节点</span><br><span class="line">     nodeStack: mutable.Stack[(Int, LearningNode)],</span><br><span class="line">     timer: TimeTracker = new TimeTracker,       </span><br><span class="line">     nodeIdCache: Option[NodeIdCache] = None): Unit = &#123;</span><br><span class="line"></span><br><span class="line">   //存储当前分组的节点数量</span><br><span class="line">   val numNodes = nodesForGroup.values.map(_.length).sum</span><br><span class="line">   logDebug(&quot;numNodes = &quot; + numNodes)</span><br><span class="line">   logDebug(&quot;numFeatures = &quot; + metadata.numFeatures)</span><br><span class="line">   logDebug(&quot;numClasses = &quot; + metadata.numClasses)</span><br><span class="line">   logDebug(&quot;isMulticlass = &quot; + metadata.isMulticlass)</span><br><span class="line">   logDebug(&quot;isMulticlassWithCategoricalFeatures = &quot; +</span><br><span class="line">     metadata.isMulticlassWithCategoricalFeatures)</span><br><span class="line">   logDebug(&quot;using nodeIdCache = &quot; + nodeIdCache.nonEmpty.toString)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">   //对于一个特定的树的特定节点，通过baggedPoint数据点，更新DTStatsAggregator聚合信息（更新相关的特征及bin的聚合类信息）</span><br><span class="line">   def nodeBinSeqOp(</span><br><span class="line">       treeIndex: Int, //树的索引</span><br><span class="line">       nodeInfo: NodeIndexInfo, //节点信息</span><br><span class="line">       agg: Array[DTStatsAggregator], //聚合信息，(node, feature, bin)</span><br><span class="line">       baggedPoint: BaggedPoint[TreePoint]): Unit = &#123;//数据点</span><br><span class="line">     if (nodeInfo != null) &#123;//如果节点信息不为空，表示该节点在当前计算的节点集合中</span><br><span class="line">       val aggNodeIndex = nodeInfo.nodeIndexInGroup //该节点在当前分组的编号</span><br><span class="line">       val featuresForNode = nodeInfo.featureSubset //该节点对应的特征集合</span><br><span class="line">       //该样本在该树上的采样次数，如果为n表示5个同样的数据点同时用于更新对应的聚合信息</span><br><span class="line">       val instanceWeight = baggedPoint.subsampleWeights(treeIndex) </span><br><span class="line">       if (metadata.unorderedFeatures.isEmpty) &#123;</span><br><span class="line">         //如果不存在无序特征，根据有序特征进行更新</span><br><span class="line">         orderedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, instanceWeight, featuresForNode)</span><br><span class="line">       &#125; else &#123; //都是有序特征</span><br><span class="line">         mixedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, splits,</span><br><span class="line">           metadata.unorderedFeatures, instanceWeight, featuresForNode)</span><br><span class="line">       &#125;</span><br><span class="line">       agg(aggNodeIndex).updateParent(baggedPoint.datum.label, instanceWeight)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   //计算当前数据被划分到的树的节点，并更新在对应节点的聚合信息。对于每个特征的相关bin,更新其聚合信息。</span><br><span class="line">   def binSeqOp(</span><br><span class="line">       agg: Array[DTStatsAggregator],//agg数组存储聚合信息，数据结构为（node, feature, bin）</span><br><span class="line">       baggedPoint: BaggedPoint[TreePoint]): Array[DTStatsAggregator] = &#123;</span><br><span class="line">     treeToNodeToIndexInfo.foreach &#123; case (treeIndex, nodeIndexToInfo) =&gt;</span><br><span class="line">       //得到要更新的节点编号</span><br><span class="line">       val nodeIndex = </span><br><span class="line">         topNodesForGroup(treeIndex).predictImpl(baggedPoint.datum.binnedFeatures, splits)</span><br><span class="line">       //对上步得到的节点，根据样本点更新其对应的bin的聚合信息</span><br><span class="line">       nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)</span><br><span class="line">     &#125;</span><br><span class="line">     agg</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   /**</span><br><span class="line">    * Do the same thing as binSeqOp, but with nodeIdCache.</span><br><span class="line">    */</span><br><span class="line">   def binSeqOpWithNodeIdCache(</span><br><span class="line">       agg: Array[DTStatsAggregator],</span><br><span class="line">       dataPoint: (BaggedPoint[TreePoint], Array[Int])): Array[DTStatsAggregator] = &#123;</span><br><span class="line">     treeToNodeToIndexInfo.foreach &#123; case (treeIndex, nodeIndexToInfo) =&gt;</span><br><span class="line">       val baggedPoint = dataPoint._1</span><br><span class="line">       val nodeIdCache = dataPoint._2</span><br><span class="line">       val nodeIndex = nodeIdCache(treeIndex)</span><br><span class="line">       nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     agg</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">   //从treeToNodeToIndexInfo中获取每个节点对应的特征集合。key为节点在本组节点的编号，value为对应特征集合</span><br><span class="line">   def getNodeToFeatures(</span><br><span class="line">       treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]]): Option[Map[Int, Array[Int]]] = &#123;</span><br><span class="line">     if (!metadata.subsamplingFeatures) &#123; //如果定义为不进行特征采样</span><br><span class="line">       None</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       //定义为特征采样，从treeToNodeToIndexInfo中获取对应的节点编号和特征集合。</span><br><span class="line">       val mutableNodeToFeatures = new mutable.HashMap[Int, Array[Int]]()</span><br><span class="line">       treeToNodeToIndexInfo.values.foreach &#123; nodeIdToNodeInfo =&gt;</span><br><span class="line">         nodeIdToNodeInfo.values.foreach &#123; nodeIndexInfo =&gt;</span><br><span class="line">           assert(nodeIndexInfo.featureSubset.isDefined)</span><br><span class="line">           mutableNodeToFeatures(nodeIndexInfo.nodeIndexInGroup) = nodeIndexInfo.featureSubset.get</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       Some(mutableNodeToFeatures.toMap)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">   //用于训练的节点数组</span><br><span class="line">   val nodes = new Array[LearningNode](numNodes)</span><br><span class="line">   //根据nodesForGroup，在nodes中存储本轮迭代的节点，存储到nodes中</span><br><span class="line">   nodesForGroup.foreach &#123; case (treeIndex, nodesForTree) =&gt;</span><br><span class="line">     nodesForTree.foreach &#123; node =&gt;</span><br><span class="line">       nodes(treeToNodeToIndexInfo(treeIndex)(node.id).nodeIndexInGroup) = node</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   //对于所有的节点，计算最佳特征及分割点</span><br><span class="line">   timer.start(&quot;chooseSplits&quot;)</span><br><span class="line">   //对于每个分区，迭代所有的样本，计算每个节点的聚合信息，</span><br><span class="line">   //产出(nodeIndex, nodeAggregateStats)数据结构，</span><br><span class="line">   //通过reduceByKey操作，一个节点的所有信息会被shuffle到同一个分区，通过合并信息，</span><br><span class="line">   //计算每个节点的最佳分割，最后只有最佳的分割用于进一步构建决策树。</span><br><span class="line">   val nodeToFeatures = getNodeToFeatures(treeToNodeToIndexInfo)//</span><br><span class="line">   val nodeToFeaturesBc = input.sparkContext.broadcast(nodeToFeatures)</span><br><span class="line"></span><br><span class="line">   val partitionAggregates: RDD[(Int, DTStatsAggregator)] = if (nodeIdCache.nonEmpty) &#123;</span><br><span class="line">     input.zip(nodeIdCache.get.nodeIdsForInstances).mapPartitions &#123; points =&gt;</span><br><span class="line">       // Construct a nodeStatsAggregators array to hold node aggregate stats,</span><br><span class="line">       // each node will have a nodeStatsAggregator</span><br><span class="line">       val nodeStatsAggregators = Array.tabulate(numNodes) &#123; nodeIndex =&gt;</span><br><span class="line">         val featuresForNode = nodeToFeaturesBc.value.map &#123; nodeToFeatures =&gt;</span><br><span class="line">           nodeToFeatures(nodeIndex)</span><br><span class="line">         &#125;</span><br><span class="line">         new DTStatsAggregator(metadata, featuresForNode)</span><br><span class="line">       &#125;</span><br><span class="line">       // iterator all instances in current partition and update aggregate stats</span><br><span class="line">       points.foreach(binSeqOpWithNodeIdCache(nodeStatsAggregators, _))</span><br><span class="line">       // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,</span><br><span class="line">       // which can be combined with other partition using `reduceByKey`</span><br><span class="line">       nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator</span><br><span class="line">     &#125;</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     input.mapPartitions &#123; points =&gt;</span><br><span class="line">       // 在每个分区内，构建一个nodeStatsAggregators数组，其中每个元素对应一个node的DTStatsAggregator，该DTStatsAggregator包括了决策树元数据信息、以及该node对应的特征集合</span><br><span class="line">       val nodeStatsAggregators = Array.tabulate(numNodes) &#123; nodeIndex =&gt;</span><br><span class="line">         val featuresForNode = nodeToFeaturesBc.value.flatMap &#123; nodeToFeatures =&gt;</span><br><span class="line">           Some(nodeToFeatures(nodeIndex))</span><br><span class="line">         &#125;</span><br><span class="line">         new DTStatsAggregator(metadata, featuresForNode)</span><br><span class="line">       &#125;</span><br><span class="line">       //对当前分区，迭代所有样本，更新nodeStatsAggregators，即每个node对应的DTStatsAggregator</span><br><span class="line">       points.foreach(binSeqOp(nodeStatsAggregators, _))</span><br><span class="line">       //转化成(nodeIndex, nodeAggregateStats)格式，用于后续通过reduceByKey对多个分区的结果进行聚合。</span><br><span class="line">       nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   //reduceByKey聚合多个partition的统计特征</span><br><span class="line">   val nodeToBestSplits = partitionAggregates.reduceByKey((a, b) =&gt; a.merge(b)).map &#123;</span><br><span class="line">     case (nodeIndex, aggStats) =&gt;</span><br><span class="line">       //得到节点对应的特征集合</span><br><span class="line">       val featuresForNode = nodeToFeaturesBc.value.flatMap &#123; nodeToFeatures =&gt;</span><br><span class="line">         Some(nodeToFeatures(nodeIndex))</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       // 找到最佳分裂特征和分裂位置，并返回度量的统计特征</span><br><span class="line">       val (split: Split, stats: ImpurityStats) =</span><br><span class="line">         binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))</span><br><span class="line">       (nodeIndex, (split, stats))</span><br><span class="line">   &#125;.collectAsMap()</span><br><span class="line"></span><br><span class="line">   timer.stop(&quot;chooseSplits&quot;)</span><br><span class="line"></span><br><span class="line">   val nodeIdUpdaters = if (nodeIdCache.nonEmpty) &#123;</span><br><span class="line">     Array.fill[mutable.Map[Int, NodeIndexUpdater]](</span><br><span class="line">       metadata.numTrees)(mutable.Map[Int, NodeIndexUpdater]())</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     null</span><br><span class="line">   &#125;</span><br><span class="line">   // Iterate over all nodes in this group.</span><br><span class="line">   //对于本组所有节点，更新节点本身信息，如果孩子节点是课分裂的叶子节点，则将其加入栈中</span><br><span class="line">   nodesForGroup.foreach &#123; case (treeIndex, nodesForTree) =&gt;</span><br><span class="line">     nodesForTree.foreach &#123; node =&gt;</span><br><span class="line">       val nodeIndex = node.id //节点id</span><br><span class="line">       val nodeInfo = treeToNodeToIndexInfo(treeIndex)(nodeIndex) //节点信息，包括节点在当前分组编号，节点特征等</span><br><span class="line">       val aggNodeIndex = nodeInfo.nodeIndexInGroup //节点在当前分组编号</span><br><span class="line">       //节点对应的最佳分裂，及最佳分裂对应的不纯度度量相关统计信息</span><br><span class="line">       val (split: Split, stats: ImpurityStats) =</span><br><span class="line">         nodeToBestSplits(aggNodeIndex) </span><br><span class="line">       logDebug(&quot;best split = &quot; + split)</span><br><span class="line"></span><br><span class="line">       //如果信息增益小于0，或者层次达到上限，则将当前节点设置为叶子节点</span><br><span class="line">       val isLeaf =</span><br><span class="line">         (stats.gain &lt;= 0) || (LearningNode.indexToLevel(nodeIndex) == metadata.maxDepth)</span><br><span class="line">       node.isLeaf = isLeaf</span><br><span class="line">       node.stats = stats</span><br><span class="line">       logDebug(&quot;Node = &quot; + node)</span><br><span class="line">       </span><br><span class="line">       //当前节点非叶子节点，创建子节点</span><br><span class="line">       if (!isLeaf) &#123;</span><br><span class="line">         node.split = Some(split) //设置节点split参数</span><br><span class="line">         //子节点层数是否达到最大值</span><br><span class="line">         val childIsLeaf = (LearningNode.indexToLevel(nodeIndex) + 1) == metadata.maxDepth</span><br><span class="line">         //左孩子节点层数达到最大值，或者不纯度度量等于0，则左孩子节点为叶子节点</span><br><span class="line">         val leftChildIsLeaf = childIsLeaf || (stats.leftImpurity == 0.0)</span><br><span class="line">         //右孩子节点层数达到最大值，或者不纯度度量等于0，则右孩子节点为叶子节点          </span><br><span class="line">         val rightChildIsLeaf = childIsLeaf || (stats.rightImpurity == 0.0)</span><br><span class="line">         //创建左孩子节点，getEmptyImpurityStats(stats.leftImpurityCalculator)为左孩子的不纯度度量，只有impurity、impurityCalculator两个属性</span><br><span class="line">         node.leftChild = Some(LearningNode(LearningNode.leftChildIndex(nodeIndex),</span><br><span class="line">           leftChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.leftImpurityCalculator)))</span><br><span class="line">         //创建右孩子节点</span><br><span class="line">         node.rightChild = Some(LearningNode(LearningNode.rightChildIndex(nodeIndex),</span><br><span class="line">           rightChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.rightImpurityCalculator)))</span><br><span class="line"></span><br><span class="line">         if (nodeIdCache.nonEmpty) &#123;</span><br><span class="line">           val nodeIndexUpdater = NodeIndexUpdater(</span><br><span class="line">             split = split,</span><br><span class="line">             nodeIndex = nodeIndex)</span><br><span class="line">           nodeIdUpdaters(treeIndex).put(nodeIndex, nodeIndexUpdater)</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         // enqueue left child and right child if they are not leaves</span><br><span class="line">         //如果左孩子节点不是叶子节点，则将左孩子节点入栈</span><br><span class="line">         if (!leftChildIsLeaf) &#123;</span><br><span class="line">           nodeStack.push((treeIndex, node.leftChild.get))</span><br><span class="line">         &#125;</span><br><span class="line">         if (!rightChildIsLeaf) &#123;</span><br><span class="line">           //如果右孩子节点不是叶子节点，则将右孩子节点入栈</span><br><span class="line">           nodeStack.push((treeIndex, node.rightChild.get))</span><br><span class="line">         &#125;</span><br><span class="line">         logDebug(&quot;leftChildIndex = &quot; + node.leftChild.get.id +</span><br><span class="line">           &quot;, impurity = &quot; + stats.leftImpurity)</span><br><span class="line">         logDebug(&quot;rightChildIndex = &quot; + node.rightChild.get.id +</span><br><span class="line">           &quot;, impurity = &quot; + stats.rightImpurity)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   if (nodeIdCache.nonEmpty) &#123;</span><br><span class="line">     // Update the cache if needed.</span><br><span class="line">     nodeIdCache.get.updateNodeIndices(input, nodeIdUpdaters, splits)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">//得到当前数据点对应的node index输出,模仿对数据的预测过程，从根节点开始向下传播，</span><br><span class="line">//直到一个叶子节点或者未进行分裂的节点终止，返回终止节点对应的索引。</span><br><span class="line">def predictImpl(binnedFeatures: Array[Int], splits: Array[Array[Split]]): Int = &#123;</span><br><span class="line">  if (this.isLeaf || this.split.isEmpty) &#123;</span><br><span class="line">    this.id //如果当前节点是叶子节点或者未分裂的节点，返回当前节点索引</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    val split = this.split.get //当前节点的split</span><br><span class="line">    val featureIndex = split.featureIndex //当前节点split对应的特征索引</span><br><span class="line">    //根据数据点在featureIndex特征上的取值，以及featureIndex特征对应的分裂，判断当前数据点是否应该向左传递。</span><br><span class="line">    val splitLeft = split.shouldGoLeft(binnedFeatures(featureIndex), splits(featureIndex)) </span><br><span class="line">    if (this.leftChild.isEmpty) &#123; //如果左孩子为空</span><br><span class="line">      // Not yet split. Return next layer of nodes to train</span><br><span class="line">      if (splitLeft) &#123; //当前节点应该向左传递，得到左孩子节点索引值</span><br><span class="line">        LearningNode.leftChildIndex(this.id)</span><br><span class="line">      &#125; else &#123; //当前节点应该向右传递，得到右孩子节点索引值</span><br><span class="line">        LearningNode.rightChildIndex(this.id)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123; //如果左孩子不为空，</span><br><span class="line">      if (splitLeft) &#123; //当前节点应该向左传递，从左节点开始，递归计算最终节点的索引</span><br><span class="line">        this.leftChild.get.predictImpl(binnedFeatures, splits)</span><br><span class="line">      &#125; else &#123; //当前节点应该向右传递，从右节点开始，递归计算最终节点的索引</span><br><span class="line">        this.rightChild.get.predictImpl(binnedFeatures, splits)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">//对于排序类特征，根据数据点、权重，更新每个特征的每个bin信息        </span><br><span class="line">private def orderedBinSeqOp(</span><br><span class="line">      agg: DTStatsAggregator, //聚合信息，(feature, bin)</span><br><span class="line">      treePoint: TreePoint,</span><br><span class="line">      instanceWeight: Double,</span><br><span class="line">      featuresForNode: Option[Array[Int]]): Unit = &#123;</span><br><span class="line">    val label = treePoint.label</span><br><span class="line"></span><br><span class="line">    // 如果是采样特征</span><br><span class="line">    if (featuresForNode.nonEmpty) &#123;</span><br><span class="line">      // 使用采样的特征，对于每个特征的每个bin，进行更新</span><br><span class="line">      var featureIndexIdx = 0</span><br><span class="line">      while (featureIndexIdx &lt; featuresForNode.get.length) &#123;</span><br><span class="line">        val binIndex = treePoint.binnedFeatures(featuresForNode.get.apply(featureIndexIdx))</span><br><span class="line">        agg.update(featureIndexIdx, binIndex, label, instanceWeight)</span><br><span class="line">        featureIndexIdx += 1</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // 如果是非采样特征，使用所有特征，对每个特征的每个bin，进行更新</span><br><span class="line">      val numFeatures = agg.metadata.numFeatures</span><br><span class="line">      var featureIndex = 0</span><br><span class="line">      while (featureIndex &lt; numFeatures) &#123;</span><br><span class="line">        val binIndex = treePoint.binnedFeatures(featureIndex)</span><br><span class="line">        agg.update(featureIndex, binIndex, label, instanceWeight)</span><br><span class="line">        featureIndex += 1</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">//相对于orderedBinSeqOp函数，mixedBinSeqOp函数在同时包括排序和非排序特征情况下，更新聚合信息.</span><br><span class="line">//对于有序特征，对每个特征更新一个bin</span><br><span class="line">//对于无序特征，类别的子集对应的bin需要消息，每个子集的靠左bin或者靠右bin需要更新</span><br><span class="line">private def mixedBinSeqOp(</span><br><span class="line">      agg: DTStatsAggregator, //聚合信息，(feature, bin)</span><br><span class="line">      treePoint: TreePoint,</span><br><span class="line">      splits: Array[Array[Split]],</span><br><span class="line">      unorderedFeatures: Set[Int],</span><br><span class="line">      instanceWeight: Double,</span><br><span class="line">      featuresForNode: Option[Array[Int]]): Unit = &#123;</span><br><span class="line">    val numFeaturesPerNode = if (featuresForNode.nonEmpty) &#123;</span><br><span class="line">      // 如果特征需要采样，使用采样特征</span><br><span class="line">      featuresForNode.get.length</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // 否则使用所有特征</span><br><span class="line">      agg.metadata.numFeatures</span><br><span class="line">    &#125;</span><br><span class="line">    // 迭代每个特征，更新该节点对应的bin聚合信息.</span><br><span class="line">    var featureIndexIdx = 0</span><br><span class="line">    while (featureIndexIdx &lt; numFeaturesPerNode) &#123;</span><br><span class="line">      //得到特征对应的原始索引值</span><br><span class="line">      val featureIndex = if (featuresForNode.nonEmpty) &#123;</span><br><span class="line">        featuresForNode.get.apply(featureIndexIdx)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        featureIndexIdx</span><br><span class="line">      &#125;</span><br><span class="line">      if (unorderedFeatures.contains(featureIndex)) &#123;</span><br><span class="line">        //如果当前特征是无序特征</span><br><span class="line">        val featureValue = treePoint.binnedFeatures(featureIndex) //得到bin features</span><br><span class="line">        //得到当前特征偏移量</span><br><span class="line">        val leftNodeFeatureOffset = agg.getFeatureOffset(featureIndexIdx)</span><br><span class="line">        // Update the left or right bin for each split.</span><br><span class="line">        //得到当前特征的split数量</span><br><span class="line">        val numSplits = agg.metadata.numSplits(featureIndex)</span><br><span class="line">        //得到当前特征分裂信息</span><br><span class="line">        val featureSplits = splits(featureIndex)</span><br><span class="line">        var splitIndex = 0</span><br><span class="line">        while (splitIndex &lt; numSplits) &#123;</span><br><span class="line">          //根据当前特征值，判断是否应该向左传递，如果向左传递，则将节点对当前特征的当前区间聚合信息进行更新</span><br><span class="line">          if (featureSplits(splitIndex).shouldGoLeft(featureValue, featureSplits)) &#123;</span><br><span class="line">            agg.featureUpdate(leftNodeFeatureOffset, splitIndex, treePoint.label, instanceWeight)</span><br><span class="line">          &#125;</span><br><span class="line">          splitIndex += 1</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        // 如果是有序特征，则直接更新对应特征的对应bin信息</span><br><span class="line">        val binIndex = treePoint.binnedFeatures(featureIndex)</span><br><span class="line">        agg.update(featureIndexIdx, binIndex, treePoint.label, instanceWeight)</span><br><span class="line">      &#125;</span><br><span class="line">      featureIndexIdx += 1</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line">//寻找最佳分裂特征和分裂位置</span><br><span class="line">private[tree] def binsToBestSplit(</span><br><span class="line">      binAggregates: DTStatsAggregator, //所有feature的bin的统计信息</span><br><span class="line">      splits: Array[Array[Split]],//所有feature的所有split</span><br><span class="line">      featuresForNode: Option[Array[Int]],//node对应的feature子集</span><br><span class="line">      //当前node</span><br><span class="line">      node: LearningNode): (Split, ImpurityStats) = &#123; //返回值为最佳分裂，及对应的不纯度相关度量</span><br><span class="line"></span><br><span class="line">    // Calculate InformationGain and ImpurityStats if current node is top node</span><br><span class="line">    // 当前节点对应的树的层次</span><br><span class="line">    val level = LearningNode.indexToLevel(node.id)</span><br><span class="line">    // 如果是根节点，不纯度度量为0</span><br><span class="line">    var gainAndImpurityStats: ImpurityStats = if (level == 0) &#123;</span><br><span class="line">      null</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      //否则为当前节点对应的相关度量stats</span><br><span class="line">      node.stats</span><br><span class="line">    &#125;</span><br><span class="line">    //获得合法的特征分裂</span><br><span class="line">    val validFeatureSplits =</span><br><span class="line">      Range(0, binAggregates.metadata.numFeaturesPerNode).view.map &#123; </span><br><span class="line">      //得到原始特征对应的feature index</span><br><span class="line">      featureIndexIdx =&gt;</span><br><span class="line">        featuresForNode.map(features =&gt; (featureIndexIdx, features(featureIndexIdx)))</span><br><span class="line">          .getOrElse((featureIndexIdx, featureIndexIdx))</span><br><span class="line">      &#125;.withFilter &#123; case (_, featureIndex) =&gt; //过滤对应split数量为0的特征</span><br><span class="line">        binAggregates.metadata.numSplits(featureIndex) != 0</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    //对每个(feature,split), 计算增益，并选择增益最大的(feature,split)</span><br><span class="line">    val (bestSplit, bestSplitStats) =</span><br><span class="line">      validFeatureSplits.map &#123; case (featureIndexIdx, featureIndex) =&gt;</span><br><span class="line">        //得到索引为featureIndex的特征对应的split数量</span><br><span class="line">        val numSplits = binAggregates.metadata.numSplits(featureIndex)</span><br><span class="line">        if (binAggregates.metadata.isContinuous(featureIndex)) &#123;</span><br><span class="line">          //如果是连续特征</span><br><span class="line">          //计算每个bin的累积统计信息（包括第一个bin到当前bin之间的所有bin对应的统计信息）</span><br><span class="line">          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class="line">          var splitIndex = 0</span><br><span class="line">          while (splitIndex &lt; numSplits) &#123;</span><br><span class="line">            binAggregates.mergeForFeature(nodeFeatureOffset, splitIndex + 1, splitIndex)</span><br><span class="line">            splitIndex += 1</span><br><span class="line">          &#125;</span><br><span class="line">          //找到最好的split</span><br><span class="line">          val (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class="line">            Range(0, numSplits).map &#123; case splitIdx =&gt;</span><br><span class="line">              //得到当前split左孩子对应的统计信息</span><br><span class="line">              val leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, splitIdx)</span><br><span class="line">              //得到当前split右孩子对应的统计信息， 为得到右孩子对应的统计信息，需要所有的统计信息减去左孩子的统计信息</span><br><span class="line">              val rightChildStats =</span><br><span class="line">                binAggregates.getImpurityCalculator(nodeFeatureOffset, numSplits)</span><br><span class="line">              //所有的统计信息减去左孩子的统计信息</span><br><span class="line">              rightChildStats.subtract(leftChildStats)</span><br><span class="line">              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,</span><br><span class="line">                leftChildStats, rightChildStats, binAggregates.metadata)</span><br><span class="line">              (splitIdx, gainAndImpurityStats)//分裂索引，不纯度度量信息</span><br><span class="line">            &#125;.maxBy(_._2.gain)//取信息增益最大的分裂</span><br><span class="line">          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)</span><br><span class="line">        &#125; else if (binAggregates.metadata.isUnordered(featureIndex)) &#123;</span><br><span class="line">          //无序离散特征</span><br><span class="line">          val leftChildOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class="line">          val (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class="line">            Range(0, numSplits).map &#123; splitIndex =&gt;</span><br><span class="line">              //得到左孩子聚合信息</span><br><span class="line">              val leftChildStats = binAggregates.getImpurityCalculator(leftChildOffset, splitIndex)</span><br><span class="line">              //得到右孩子聚合信息</span><br><span class="line">              val rightChildStats = binAggregates.getParentImpurityCalculator()</span><br><span class="line">                .subtract(leftChildStats)</span><br><span class="line">              //计算不纯度度量相关统计信息</span><br><span class="line">              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,</span><br><span class="line">                leftChildStats, rightChildStats, binAggregates.metadata)</span><br><span class="line">              (splitIndex, gainAndImpurityStats) //分裂索引，不纯度度量信息</span><br><span class="line">            &#125;.maxBy(_._2.gain)//取信息增益最大的分裂</span><br><span class="line">          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          // 对于排序离散特征</span><br><span class="line">          //得到聚合信息的其实地址</span><br><span class="line">          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class="line">          //得到类别数量</span><br><span class="line">          val numCategories = binAggregates.metadata.numBins(featureIndex)</span><br><span class="line"></span><br><span class="line">          //每个bin是一个特征值，根据质心对这些特征值排序，共K个特征值，对应生成K-1个划分</span><br><span class="line">          val centroidForCategories = Range(0, numCategories).map &#123; case featureValue =&gt;</span><br><span class="line">            //得到不纯度度量的统计信息</span><br><span class="line">            val categoryStats =</span><br><span class="line">              binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)</span><br><span class="line">            val centroid = if (categoryStats.count != 0) &#123;//如果对应样本数量不为0，</span><br><span class="line">              if (binAggregates.metadata.isMulticlass) &#123;</span><br><span class="line">                //如果是多分类决策树，则将对应多标签的不纯度度量作为质心</span><br><span class="line">                categoryStats.calculate()</span><br><span class="line">              &#125; else if (binAggregates.metadata.isClassification) &#123;</span><br><span class="line">                //如果是二分类问题，则将对应的正样本数量作为质心</span><br><span class="line">                categoryStats.stats(1)</span><br><span class="line">              &#125; else &#123;</span><br><span class="line">                //如果是回归问题，则将对应的预测值作为质心</span><br><span class="line">                categoryStats.predict</span><br><span class="line">              &#125;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">              Double.MaxValue //如果对应样本数量为0，则质心为Double.MaxValue</span><br><span class="line">            &#125;</span><br><span class="line">            (featureValue, centroid) //返回每个特征值对应的样本质心</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          logDebug(&quot;Centroids for categorical variable: &quot; + centroidForCategories.mkString(&quot;,&quot;))</span><br><span class="line"></span><br><span class="line">          // 根据质心，将特征对应的bin排序（即对应的离散特征值排序）</span><br><span class="line">          val categoriesSortedByCentroid = centroidForCategories.toList.sortBy(_._2)</span><br><span class="line"></span><br><span class="line">          logDebug(&quot;Sorted centroids for categorical variable = &quot; +</span><br><span class="line">            categoriesSortedByCentroid.mkString(&quot;,&quot;))</span><br><span class="line"></span><br><span class="line">          // 从左到右，依次计算每个category对应的从第一个category到当前categofy的统计信息聚合结果</span><br><span class="line">          var splitIndex = 0</span><br><span class="line">          while (splitIndex &lt; numSplits) &#123;</span><br><span class="line">            val currentCategory = categoriesSortedByCentroid(splitIndex)._1</span><br><span class="line">            val nextCategory = categoriesSortedByCentroid(splitIndex + 1)._1</span><br><span class="line">            binAggregates.mergeForFeature(nodeFeatureOffset, nextCategory, currentCategory)</span><br><span class="line">            splitIndex += 1</span><br><span class="line">          &#125;</span><br><span class="line">          </span><br><span class="line">          //所有特征值的聚合结果对应的category索引</span><br><span class="line">          val lastCategory = categoriesSortedByCentroid.last._1</span><br><span class="line">          //找到最佳的分裂</span><br><span class="line">          val (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class="line">            Range(0, numSplits).map &#123; splitIndex =&gt;</span><br><span class="line">              //得到当前索引的特征值</span><br><span class="line">              val featureValue = categoriesSortedByCentroid(splitIndex)._1</span><br><span class="line">              //得到左孩子对应的聚合信息</span><br><span class="line">              val leftChildStats =</span><br><span class="line">                binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)</span><br><span class="line">              //得到右孩子对应的聚合信息</span><br><span class="line">              val rightChildStats =</span><br><span class="line">                binAggregates.getImpurityCalculator(nodeFeatureOffset, lastCategory)</span><br><span class="line">              rightChildStats.subtract(leftChildStats)</span><br><span class="line">              //得到不纯度度量的相关统计信息</span><br><span class="line">              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,</span><br><span class="line">                leftChildStats, rightChildStats, binAggregates.metadata)</span><br><span class="line">              (splitIndex, gainAndImpurityStats)</span><br><span class="line">            &#125;.maxBy(_._2.gain)//根据信息增益进行排序，得到信息增益最大的split索引及增益</span><br><span class="line">          </span><br><span class="line">          //得到最佳分裂边界</span><br><span class="line">          val categoriesForSplit =</span><br><span class="line">            categoriesSortedByCentroid.map(_._1.toDouble).slice(0, bestFeatureSplitIndex + 1)</span><br><span class="line">          //得到最佳分裂，包括特征索引、划分边界、类别数量等</span><br><span class="line">          val bestFeatureSplit =</span><br><span class="line">            new CategoricalSplit(featureIndex, categoriesForSplit.toArray, numCategories)</span><br><span class="line">           //返回最佳分裂，及对应的增益统计信息</span><br><span class="line">          (bestFeatureSplit, bestFeatureGainStats)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.maxBy(_._2.gain)//针对所有特征，按照信息增益进行排序，取增益最大的特征</span><br><span class="line"></span><br><span class="line">    (bestSplit, bestSplitStats)//返回最佳分裂，及对应的增益统计信息</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">根据分裂对应的左孩子聚合信息，右孩子聚合信息，计算当前节点不纯度度量的相关统计信息</span><br><span class="line">private def calculateImpurityStats(</span><br><span class="line">      stats: ImpurityStats,</span><br><span class="line">      leftImpurityCalculator: ImpurityCalculator,</span><br><span class="line">      rightImpurityCalculator: ImpurityCalculator,</span><br><span class="line">      metadata: DecisionTreeMetadata): ImpurityStats = &#123;</span><br><span class="line">    //得到父节点的聚合信息</span><br><span class="line">    val parentImpurityCalculator: ImpurityCalculator = if (stats == null) &#123;</span><br><span class="line">      leftImpurityCalculator.copy.add(rightImpurityCalculator)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      stats.impurityCalculator</span><br><span class="line">    &#125;</span><br><span class="line">    //得到父节点不纯度度量</span><br><span class="line">    val impurity: Double = if (stats == null) &#123;</span><br><span class="line">      parentImpurityCalculator.calculate()</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      stats.impurity</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line">    val leftCount = leftImpurityCalculator.count //根据当前分裂得到的左孩子对应样本数量</span><br><span class="line">    val rightCount = rightImpurityCalculator.count //根据当前分裂得到的右孩子对应样本数量</span><br><span class="line"></span><br><span class="line">    val totalCount = leftCount + rightCount  //当前分裂对应的总样本数量</span><br><span class="line"></span><br><span class="line">    // If left child or right child doesn&apos;t satisfy minimum instances per node,</span><br><span class="line">    // then this split is invalid, return invalid information gain stats.</span><br><span class="line">    //如果左孩子或者右孩子样本数量小于下限值，返回不合法的不纯度度量信息</span><br><span class="line">    if ((leftCount &lt; metadata.minInstancesPerNode) ||</span><br><span class="line">      (rightCount &lt; metadata.minInstancesPerNode)) &#123;</span><br><span class="line">      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)</span><br><span class="line">    &#125;</span><br><span class="line">    //左孩子对应的不纯度度量</span><br><span class="line">    val leftImpurity = leftImpurityCalculator.calculate() // Note: This equals 0 if count = 0</span><br><span class="line">    //右孩子对应的不纯度度量</span><br><span class="line">    val rightImpurity = rightImpurityCalculator.calculate()</span><br><span class="line">    //左孩子权重</span><br><span class="line">    val leftWeight = leftCount / totalCount.toDouble</span><br><span class="line">    //右孩子权重</span><br><span class="line">    val rightWeight = rightCount / totalCount.toDouble</span><br><span class="line">    //信息增益</span><br><span class="line">    val gain = impurity - leftWeight * leftImpurity - rightWeight * rightImpurity</span><br><span class="line">    //信息增益小于下限值，则返回不合法的不纯度度量信息</span><br><span class="line">      if (gain &lt; metadata.minInfoGain) &#123;</span><br><span class="line">      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)</span><br><span class="line">    &#125;</span><br><span class="line">    //返回不纯度度量信息</span><br><span class="line">    new ImpurityStats(gain, impurity, parentImpurityCalculator,</span><br><span class="line">      leftImpurityCalculator, rightImpurityCalculator)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h2><p>通过模型训练生成决策树（随机森林）模型RandomForestModel，随机森林模型继承了树的组合模型TreeEnsembleModel，进一步通过predictBySumming函数，对传进的样本点进行预测。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//对样本点features进行预测</span><br><span class="line">private def predictBySumming(features: Vector): Double = &#123;</span><br><span class="line">  //对每棵决策树进行预测，然后自后结果为每个决策树结果的加权求和</span><br><span class="line">  val treePredictions = trees.map(_.predict(features))</span><br><span class="line">  blas.ddot(numTrees, treePredictions, 1, treeWeights, 1)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//DecisionTreeModel.predict方法</span><br><span class="line">def predict(features: Vector): Double = &#123;</span><br><span class="line">  //根据头部节点预测lable</span><br><span class="line">  topNode.predict(features)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//Node. predict方法</span><br><span class="line">def predict(features: Vector): Double = &#123;</span><br><span class="line">  if (isLeaf) &#123;</span><br><span class="line">    predict.predict //如果是叶子节点，直接输出</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    if (split.get.featureType == Continuous) &#123; </span><br><span class="line">      //如果是连续特征，根据分裂阈值，决定走左孩子节点还是右孩子节点</span><br><span class="line">      if (features(split.get.feature) &lt;= split.get.threshold) &#123;</span><br><span class="line">        leftNode.get.predict(features)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        rightNode.get.predict(features)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      //如果是离散特征，根据特征是否被当前节点对应的特征集合包含，决定走左孩子节点还是右孩子节点</span><br><span class="line">      if (split.get.categories.contains(features(split.get.feature))) &#123;</span><br><span class="line">        leftNode.get.predict(features)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        rightNode.get.predict(features)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>【1】<a href="http://spark.apache.org/mllib/" target="_blank" rel="noopener">http://spark.apache.org/mllib/</a><br>【2】<a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="noopener">http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html</a></p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/决策树/" rel="tag">#决策树</a>
          
            <a href="/tags/spark-mlilib源码/" rel="tag">#spark mlilib源码</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/02/22/als/" rel="prev" title="ALS推荐算法学习与实践">
                ALS推荐算法学习与实践 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="lantian" />
          <p class="site-author-name" itemprop="name">lantian</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">10</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树算法简要介绍"><span class="nav-number">1.</span> <span class="nav-text">决策树算法简要介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树的训练"><span class="nav-number">1.1.</span> <span class="nav-text">决策树的训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#属性度量"><span class="nav-number">1.2.</span> <span class="nav-text">属性度量</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark-决策树源码分析"><span class="nav-number">2.</span> <span class="nav-text">spark 决策树源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#模型训练"><span class="nav-number">2.1.</span> <span class="nav-text">模型训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树伴生类"><span class="nav-number">2.1.1.</span> <span class="nav-text">决策树伴生类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树类"><span class="nav-number">2.1.2.</span> <span class="nav-text">决策树类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RandomForest私有类run方法-通过run方法完成模型的训练"><span class="nav-number">2.1.3.</span> <span class="nav-text">RandomForest私有类run方法,通过run方法完成模型的训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#buildMetadata"><span class="nav-number">2.1.3.1.</span> <span class="nav-text">buildMetadata</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DecisionTreeMetadata类"><span class="nav-number">2.1.3.2.</span> <span class="nav-text">DecisionTreeMetadata类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#findSplits"><span class="nav-number">2.1.3.3.</span> <span class="nav-text">findSplits</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TreePoint-convertToTreeRDD"><span class="nav-number">2.1.3.4.</span> <span class="nav-text">TreePoint.convertToTreeRDD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BaggedPoint-convertToBaggedRDD"><span class="nav-number">2.1.3.5.</span> <span class="nav-text">BaggedPoint.convertToBaggedRDD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RandomForest-selectNodesToSplit"><span class="nav-number">2.1.3.6.</span> <span class="nav-text">RandomForest.selectNodesToSplit</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RandomForest-findBestSplits"><span class="nav-number">2.1.3.7.</span> <span class="nav-text">RandomForest.findBestSplits</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型预测"><span class="nav-number">2.2.</span> <span class="nav-text">模型预测</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考资料"><span class="nav-number">3.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lantian</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  



  
  
  

  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("L0fIeSgn4Q8LXNCF4L3zEpU7-gzGzoHsz", "MKiQArcDI3O3uBaLP8MaDXw3");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
