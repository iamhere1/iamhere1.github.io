{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/LICENSE","path":"vendors/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/bower.json","path":"vendors/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/README.md","path":"vendors/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/HELP-US-OUT.txt","path":"vendors/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/bower.json","path":"vendors/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/README.md","path":"vendors/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/CONTRIBUTING.md","path":"vendors/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/bower.json","path":"vendors/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.lazyload.js","path":"vendors/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.scrollstop.js","path":"vendors/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/bower.json","path":"vendors/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.ui.min.js","path":"vendors/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.min.js","path":"vendors/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.ui.js","path":"vendors/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery/index.js","path":"vendors/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/blank.gif","path":"vendors/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading.gif","path":"vendors/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading@2x.gif","path":"vendors/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_overlay.png","path":"vendors/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite.png","path":"vendors/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite@2x.png","path":"vendors/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.css","path":"vendors/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.js","path":"vendors/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.pack.js","path":"vendors/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.js","path":"vendors/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.min.js","path":"vendors/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css","path":"vendors/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css.map","path":"vendors/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.min.css","path":"vendors/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.min.js","path":"vendors/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.pack.js","path":"vendors/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/FontAwesome.otf","path":"vendors/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","path":"vendors/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.js","path":"vendors/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/fancybox_buttons.png","path":"vendors/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","path":"vendors/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","path":"vendors/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next/.gitignore","hash":"efec790f5b7a0256763e1cc08f12c4f0aff509f6","modified":1518336888000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1518336888000},{"_id":"themes/next/.javascript_ignore","hash":"d619ee13031908cd72666e4ff652d2ea3483b1c3","modified":1518336888000},{"_id":"themes/next/.bowerrc","hash":"80e096fdc1cf912ee85dd9f7e6e77fd40cf60f10","modified":1518336888000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1518336888000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1518336888000},{"_id":"themes/next/README.en.md","hash":"565ba52b3825b85a9f05b41183caca7f18b741d4","modified":1518336888000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1518336888000},{"_id":"themes/next/_config.yml","hash":"c384ec72205eb681465151d97030039a9f508a53","modified":1518336888000},{"_id":"themes/next/gulpfile.coffee","hash":"26e5b1b945704c8bc78b928feede895c4c111c95","modified":1518336888000},{"_id":"themes/next/bower.json","hash":"f89c6700a11d81e067cc97273ca6bf96cb88c8f9","modified":1518336888000},{"_id":"themes/next/package.json","hash":"63e9c0f1dd9e5d7f51b4ae383981ef939a2ed45d","modified":1518336888000},{"_id":"source/_posts/clapack.md","hash":"d626d424aeac68f8459d2e7a5e26a79f567b216c","modified":1518337983000},{"_id":"source/_posts/als.md","hash":"8a8bd3587be78369c662225e632828c77497d1fa","modified":1518337983000},{"_id":"source/_posts/gender_based_rs.md","hash":"916b8f87f17d357652bd1f9db17aba743e25d1fe","modified":1518337983000},{"_id":"source/_posts/dnn_for_youtube_recommandation.md","hash":"98528e84b068b78d643a8ba2543c279b1c8cb749","modified":1518339472000},{"_id":"source/_posts/lbfgs.md","hash":"63c299c6bb824a85325361b9019ba87e80bc3e9c","modified":1518337983000},{"_id":"source/_posts/mf.md","hash":"5d9b85b495b92382e659a45ba2f8712a0e51bbc1","modified":1518337983000},{"_id":"source/_posts/qualified_model.md","hash":"c4f559e13b7e3bb24d5a8ee86f0a220919295e01","modified":1518337983000},{"_id":"source/about/index.md","hash":"3654ee5ec0b3e169b8cec5b13d3e6d18fbbde196","modified":1518336888000},{"_id":"source/_posts/user_profile.md","hash":"36c9b68c294f8023ca11cfee94f85ab494154f99","modified":1518337983000},{"_id":"source/tags/index.md","hash":"f68dfa0d2136291b29121af7b0611267f05859dd","modified":1518336888000},{"_id":"source/categories/index.md","hash":"b254c89f36caabf61967e4234d9975a959b5da36","modified":1518336888000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ab257af816986cd0e53f9527a92d5934ac70ae9","modified":1518336888000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"c2024ded82143807c28a299c5fe6b927ef3525ff","modified":1518336888000},{"_id":"themes/next/languages/de.yml","hash":"786afba25cfc98845a20d9901823ebeebcd1cbbf","modified":1518336888000},{"_id":"themes/next/languages/en.yml","hash":"f03799cbdb5a33064ead080bcac4baca1f6bc5f9","modified":1518336888000},{"_id":"themes/next/languages/default.yml","hash":"9db835c0543ade5a89bc80ec5a898203227cf3d8","modified":1518336888000},{"_id":"themes/next/languages/fr-FR.yml","hash":"1a084623c39de74301f3e92f9388a3a815a542ca","modified":1518336888000},{"_id":"themes/next/languages/ja.yml","hash":"a2c7b6301b5474aab798946fb700289df237c3cf","modified":1518336888000},{"_id":"themes/next/languages/ru.yml","hash":"cc7b964a46587aea0e57b0a5269d8fd25570858e","modified":1518336888000},{"_id":"themes/next/languages/id.yml","hash":"147c01e41b931085ad14250fa900c2249dcbbdd7","modified":1518336888000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"bea452bc49aed171a210d09bd6cddc4e846ea8ab","modified":1518336888000},{"_id":"themes/next/languages/zh-hk.yml","hash":"519ab3d817ec3bc5bfc91159c494b6b3c170bea7","modified":1518336888000},{"_id":"themes/next/languages/zh-tw.yml","hash":"6b1f345aaefc13e6723dc8a6741b59ac05c20dfd","modified":1518336888000},{"_id":"themes/next/languages/pt.yml","hash":"ca239b39bf65c9462e59d51b12f0fe566d453197","modified":1518336888000},{"_id":"themes/next/layout/_layout.swig","hash":"74157f6cfd679ea11febec632542793f37c5e5d4","modified":1518336888000},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1518336888000},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1518336888000},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1518336888000},{"_id":"themes/next/layout/page.swig","hash":"8019d02232a6dd1a665b6a4d2daef8e5dd2f0049","modified":1518336888000},{"_id":"themes/next/scripts/merge-configs.js","hash":"0c56be2e85c694247cfa327ea6d627b99ca265e8","modified":1518336888000},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1518336888000},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1518336888000},{"_id":"themes/next/test/.jshintrc","hash":"096ed6df627373edd820f24d46b8baf528dee61d","modified":1518336888000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1518336888000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1518336888000},{"_id":"source/_posts/decision_tree.md","hash":"78c02942aa7fdfee514b4b7dc36a88a72e8c84a8","modified":1518337983000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1518336888000},{"_id":"source/_posts/dnn_for_youtube_recommandation/evaluation_of_diffrent_configure.png","hash":"c5952d8425d643b83681001e8d4621b95ce81e01","modified":1518339428000},{"_id":"source/_posts/dnn_for_youtube_recommandation/model_performance_with_feature_depth.png","hash":"62df28e661723caf2a24b3ab934ad1610064d5a9","modified":1518339428000},{"_id":"source/_posts/dnn_for_youtube_recommandation/example_age_efficacy.png","hash":"a3b30b70d05240d3064fb536b61fdbaf43c914ea","modified":1518339428000},{"_id":"source/_posts/qualified_model/probability.png","hash":"da42fdb54fbec386e942415f4ee09302bce9f0e7","modified":1518337983000},{"_id":"source/_posts/mf/data_model.png","hash":"ce7a86fc2982befc2f1ffa642b537e6b4179f59e","modified":1518337983000},{"_id":"source/_posts/dnn_for_youtube_recommandation/recommand_system.png","hash":"560d6d348f9d2b9426b8a7348a7f7b1e22f7d397","modified":1518339428000},{"_id":"source/_posts/user_profile/life_recycle.png","hash":"7b27c6d396c9976d828f2ec80673492cfb5728f2","modified":1518337983000},{"_id":"source/_posts/user_profile/life_recycle_conversion.png","hash":"454db2488358cecfe2151c78e98d3826dce3387f","modified":1518337983000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"43c3433155ccd9abcbe7dce2e6bfa1f3a66af18b","modified":1518336888000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1518336888000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"b883289054ee54a374caad5d4883591beb94bd8b","modified":1518336888000},{"_id":"themes/next/layout/_macro/post.swig","hash":"1ca03011bed92614832b1343b65be92183957dc5","modified":1518336888000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"85327c2174d09c6d69c9033592e6c8f7eb7ac3ba","modified":1518336888000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"0ce71d8322ea7dea82d9371fa2fe13949aa870e3","modified":1518336888000},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1518336888000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"82a9bc2ba60ce68419128ff60624bd74b15dfb78","modified":1518336888000},{"_id":"themes/next/layout/_partials/head.swig","hash":"f83b1c55bedd2c1a3eb734c72c6997795a4e5f99","modified":1518336888000},{"_id":"themes/next/layout/_partials/header.swig","hash":"963a765dc00e6ac43cfc53ffaf5725eb854cf95e","modified":1518336888000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1518336888000},{"_id":"themes/next/layout/_partials/search.swig","hash":"011b9d6c9f0a2f4654908ea20b9391f9b7981271","modified":1518336888000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1518336888000},{"_id":"themes/next/layout/_scripts/baidu-push.swig","hash":"82d060fe055d6e423bbc9199f82dfe5c68e74779","modified":1518336888000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1518336888000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"0b91cadecead8e0b5211cc42b085998d94af503a","modified":1518336888000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1518336888000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"3acce36db0feb11a982c6c799aa6b6b47df2827c","modified":1518336888000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1518336888000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1518336888000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1518336888000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1518336888000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1518336888000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1518336888000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1518336888000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1518336888000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1518336888000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1518336888000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1518336888000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1518336888000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1518336888000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1518336888000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1518336888000},{"_id":"source/_posts/decision_tree/decision_tree_example.png","hash":"470e012a966a7aad69f5acdcd462c0847224ed55","modified":1518337983000},{"_id":"source/_posts/dnn_for_youtube_recommandation/recommand_matching.png","hash":"3c527cebfcc1ad21ee432552a49a8ae661cce6c4","modified":1518339428000},{"_id":"source/_posts/dnn_for_youtube_recommandation/recommand_ranking.png","hash":"7614e6c934e0639c70e4086942f7fa4aa6449d89","modified":1518339428000},{"_id":"source/_posts/dnn_for_youtube_recommandation/trainning_sequence_behavior_and_lable.png","hash":"caf27b28796e6f4953ef1dc8a04d8af7c300f79e","modified":1518339428000},{"_id":"source/_posts/user_profile/function_of_user_profile.png","hash":"d66aa85a8f694de7abf66a8f21e99ea17c127b3d","modified":1518337983000},{"_id":"source/_posts/user_profile/user_interest_profile.png","hash":"9f81ede538b6b663b0f6f6571458da3f6c7030d4","modified":1518337983000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1518336888000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1518336888000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1518336888000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1518336888000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1518336888000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1518336888000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1518336888000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1518336888000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1518336888000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"ff5523d5dacaa77a55a24e50e6e6530c3b98bfad","modified":1518336888000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1518336888000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1518336888000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1518336888000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"c07f7b2f264e5215b8ed42d67e8cef2477558364","modified":1518336888000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"7ca5cb4daa58b3504e17f3e02975e794bc634658","modified":1518336888000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1518336888000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/comments.swig","hash":"907b931d775d32405d02a25b3b0a3ac03bf804d0","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/lean-analytics.swig","hash":"069bb17fb1db3bc7c85c88efa3ed94ab6becbe2c","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/localsearch.swig","hash":"1561bd0c107d725252c6d746e9ac177fc18f93bf","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/mathjax.swig","hash":"5bafc33f57508d1d04a9930165240f6e9efa8d6d","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/analytics.swig","hash":"0a89c04055bade7baa5962f1d5aefe438d83a244","modified":1518336888000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1518336888000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1518336888000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1518336888000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1518336888000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"3ead77befa064d6327dc7afd0a5af7be59a5f196","modified":1518336888000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"17624186f7a1f28daddea258d044f8e03b2f4bea","modified":1518336888000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1518336888000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1518336888000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1518336888000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1518336888000},{"_id":"themes/next/source/js/src/post-details.js","hash":"2038f54e289b6da5def09689e69f623187147be5","modified":1518336888000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1518336888000},{"_id":"themes/next/source/js/src/utils.js","hash":"e5cb720894c4bc28ca8f10b33df127fb394018d9","modified":1518336888000},{"_id":"themes/next/source/vendors/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1518336888000},{"_id":"themes/next/source/vendors/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1518336888000},{"_id":"themes/next/source/vendors/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1518336888000},{"_id":"themes/next/source/vendors/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/.bower.json","hash":"7da985a99674e54f514d4fd9fcd3bcea6e7e41d5","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/HELP-US-OUT.txt","hash":"69a4c537d167b68a0ccf1c6febd138aeffca60d6","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1518336888000},{"_id":"themes/next/source/vendors/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1518336888000},{"_id":"themes/next/source/vendors/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1518336888000},{"_id":"themes/next/source/vendors/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1518336888000},{"_id":"themes/next/source/vendors/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1518336888000},{"_id":"themes/next/source/vendors/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1518336888000},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1518336888000},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1518336888000},{"_id":"themes/next/source/vendors/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1518336888000},{"_id":"themes/next/source/vendors/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1518336888000},{"_id":"themes/next/source/vendors/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1518336888000},{"_id":"themes/next/source/vendors/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1518336888000},{"_id":"themes/next/source/vendors/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1518336888000},{"_id":"themes/next/source/vendors/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"7c43d66da93cde65b473a7d6db2a86f9a42647d6","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/analytics/busuanzi-counter.swig","hash":"4fcbf57c4918528ab51d3d042cff92cf5aefb599","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"44e761721e8ad787ef571a3cc57bbc12d318a2a3","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"334176d838ee528e58468d8bc74ff3a6d3f25b2b","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/comments/disqus.swig","hash":"3491d3cebabc8a28857200db28a1be65aad6adc2","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1518336888000},{"_id":"themes/next/layout/_scripts/third-party/comments/duoshuo.swig","hash":"8c7af79407d223486fba72b8150fe045a553bf70","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"b49efc66bd055a2d0be7deabfcb02ee72a9a28c8","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"10994990d6e0b4d965a728a22cf7f6ee29cae9f6","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1518336888000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1518336888000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"54c90cf7bdbf5c596179d8dae6e671bad1292662","modified":1518336888000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1518336888000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5304f99581da3a31de3ecec959b7adf9002fde83","modified":1518336888000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1518336888000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"4303776991ef28f5742ca51c7dffe6f12f0acf34","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"6ed60cc621bac096c0ed7534fa25b1a52dc571d4","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"c2c6c4f6434b4f94aac2af5861cd769427f0ee10","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Pisces/_full-image.styl","hash":"938d39eedc6e3d33918c1145a5bf1e79991d3fcf","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"be22ad34f546a07f6d56b424338cdd898683eea4","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"d09280e5b79f3b573edb30f30c7a5f03ac640986","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"8d7cecde4933900c7df2db9d0a98f5f82f88dc93","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"d4b7bd610ca03dbb2f5b66631c0e84a79fb4660b","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"1b10ba2d3ad0c063c418dc94a0b7e0db4b342c53","modified":1518336888000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"7506e7490c69a200831393c38d25e91c156bd471","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1518336888000},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1518336888000},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css","hash":"3b87c2560832748cd06f9bfd2fd6ea8edbdae8c7","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.min.css","hash":"05ea25bc9b3ac48993e1fee322d3bc94b49a6e22","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","hash":"574ea2698c03ae9477db2ea3baf460ee32f1a7ea","modified":1518336888000},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1518336888000},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/fonts/FontAwesome.otf","hash":"0112e96f327d413938d37c1693806f468ffdbace","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","hash":"b3c2f08e73320135b69c23a3908b87a12053a2f6","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","hash":"507970402e328b2baeb05bde73bf9ded4e2c3a2d","modified":1518336888000},{"_id":"themes/next/source/vendors/velocity/velocity.js","hash":"e63dc7cea055ca60a95d286f32349d88b10c5a4d","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"c890ce7fe933abad7baf39764a01894924854e92","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"4da051c7f3924fa2db1e73c55b2baf1c2c150255","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"90f8f9706cd7fe829cf06e9959a65fd3f8b994fa","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"3c46efd6601e268093ce6d7b1471d18501878f0d","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d543d1377c1f61b70e3adb6da0eb12797552e5f2","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-more-link.styl","hash":"15063d79b5befc21820baf05d6f20cc1c1787477","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"cbca4842a54950e2934b3b8f3cd940f122111aef","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"4eb18b12fa0ea6c35925d9a64f64e2a7dae8c7fd","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"c44f6a553ec7ea5508f2054a13be33a62a15d3a9","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"2d3abbc85b979a648e0e579e45f16a6eba49d1e7","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"618f73450cf541f88a4fddc3d22898aee49d105d","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"8e66c2635d48e11de616bb29c4b1323698eebc0a","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"b03f891883446f3a5548b7cc90d29c77e62f1053","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"795d94561888d31cb7a6ff4a125596809ea69b7d","modified":1518336888000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"3afc459442c132c480d1d832f1a872f1070bb048","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1518336888000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1518336888000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","hash":"27cf1f2ec59aece6938c7bb2feb0e287ea778ff9","modified":1518336888000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","hash":"2b3c8ba7008cc014d8fb37abc6f9f49aeda83824","modified":1518336888000},{"_id":"public/about/index.html","hash":"6f4b60f6c09c7edebedd130f99e7835c876e6596","modified":1518340059939},{"_id":"public/tags/index.html","hash":"ba430e24f2634a32fec296f60b415d49424c18d3","modified":1518340059939},{"_id":"public/categories/index.html","hash":"22ae0e4bf590adf4a9c80150f06f0a0d059b8ef6","modified":1518340059939},{"_id":"public/archives/2016/index.html","hash":"86373c5cd43a95413717353e61ba054e26e41c3b","modified":1518340059939},{"_id":"public/archives/2016/12/index.html","hash":"bf0460f6a6a4a8bcd34fc34f99b7f03168c43666","modified":1518340059939},{"_id":"public/archives/2017/index.html","hash":"e171a7035d6e22cbf18df26f246134ad158fd898","modified":1518340059939},{"_id":"public/archives/2017/02/index.html","hash":"9f5710975283b7c6ae8511d8f1199a8854ec5a9f","modified":1518340059939},{"_id":"public/archives/2017/03/index.html","hash":"7bf6bc4814c1ef9ff129d6fdf3228b62c9afffdb","modified":1518340059939},{"_id":"public/archives/2017/05/index.html","hash":"30c46f8e36bde6dcd553d8d880c78dd177079f6e","modified":1518340059940},{"_id":"public/archives/2017/08/index.html","hash":"a0045253b324aafbd1c5c5896f7fa97fe978cbad","modified":1518340059940},{"_id":"public/archives/2017/12/index.html","hash":"f940955c42b6261ad5819c0ad7c75d1f73b799ce","modified":1518340059940},{"_id":"public/archives/2018/index.html","hash":"cbffc3f849663730f43208502cb2c9d0d14bda2a","modified":1518340059940},{"_id":"public/archives/2018/01/index.html","hash":"1b2f2cae873b2ec4ea935ead29920f09124cff24","modified":1518340059940},{"_id":"public/archives/2018/02/index.html","hash":"2ad78239dbb2f59610a06213bbe1f8092c087ff5","modified":1518340059940},{"_id":"public/categories/工具学习/index.html","hash":"1b22434acdba7f658ad6e0abe19886eccf6e8f55","modified":1518340059940},{"_id":"public/categories/推荐系统/index.html","hash":"bc0c596c1c5e6d047060b16230ff8ea69a633d40","modified":1518340059940},{"_id":"public/categories/模型与算法/index.html","hash":"8472e090575761dfba0bf5ed6521fc8a7bc1b203","modified":1518340059940},{"_id":"public/categories/用户画像/index.html","hash":"7e6ba7a9dfda63d781356d1f1cd6d5a4fa7a8c7d","modified":1518340059940},{"_id":"public/tags/lapack/index.html","hash":"e6779d1b48fda022584b60fc4532a979e5a7b8fd","modified":1518340059940},{"_id":"public/tags/clapack/index.html","hash":"35b13466c263687fb0298f9ac4079021e0608381","modified":1518340059940},{"_id":"public/tags/线性代数工具包/index.html","hash":"6aaabeb031df9b9ba80bf3120b14519490fed979","modified":1518340059940},{"_id":"public/tags/推荐算法/index.html","hash":"f2dc61caa9ee5b9de94e2144cf917951221027cf","modified":1518340059940},{"_id":"public/tags/基于性别的推荐/index.html","hash":"9afe6e7c99ab0975634862ef40dd90ea79f0343b","modified":1518340059940},{"_id":"public/tags/协同过滤/index.html","hash":"799e4bae7dec68493574adf50a9e18040c083a87","modified":1518340059940},{"_id":"public/tags/矩阵分解/index.html","hash":"d52bc3bb7afe8767e0530d342911947b5d11add0","modified":1518340059940},{"_id":"public/tags/隐语义模型/index.html","hash":"5789656502490c0db7bf10eb3559f633621cf17b","modified":1518340059940},{"_id":"public/tags/个性化推荐/index.html","hash":"a04e7abeba21a85c17f69c20cea816a4426903c1","modified":1518340059940},{"_id":"public/tags/深度学习/index.html","hash":"75b3baeb463acbdc14cc57ebb1eb13edaa1dcf2b","modified":1518340059941},{"_id":"public/tags/DNN/index.html","hash":"4eeacbdd23a6887298a8029fe70744bfef5731cf","modified":1518340059941},{"_id":"public/tags/lbfgs/index.html","hash":"d6f561a197e56ae365f5a6a12544bdef1209da1a","modified":1518340059941},{"_id":"public/tags/拟牛顿算法/index.html","hash":"b67b51b334e491b3db2ce70b75890d3ecdc3c822","modified":1518340059941},{"_id":"public/tags/非线性优化/index.html","hash":"49ccb5f6580f67291c2e683d9bbf5c000c7cd042","modified":1518340059941},{"_id":"public/tags/优质挖掘/index.html","hash":"ceb5bf9abd1e9d23113283e13e808f9d5caad7ff","modified":1518340059941},{"_id":"public/tags/用户画像/index.html","hash":"a5bb32f77994ddb6946dc0e06469d12780566e84","modified":1518340059941},{"_id":"public/tags/兴趣挖掘/index.html","hash":"105e016d53984813dfdd72fd8d3e57df9281c166","modified":1518340059941},{"_id":"public/tags/年龄性别挖掘/index.html","hash":"ae47f5402f1b618806642c1fd7195e114ba6e5c8","modified":1518340059941},{"_id":"public/tags/常住地挖掘/index.html","hash":"6ad23305d73400f1f0c3c83ce10efecccb98c959","modified":1518340059941},{"_id":"public/tags/生命周期画像/index.html","hash":"9729065373441cf94725ceb5349cd9a9ced48e53","modified":1518340059941},{"_id":"public/tags/流失预测模型/index.html","hash":"783c22532727d2b2b8849266fa580842582da12f","modified":1518340059941},{"_id":"public/tags/决策树/index.html","hash":"43ec22ff754ce24917cb6363db730342e41236bf","modified":1518340059941},{"_id":"public/tags/spark-mlilib源码/index.html","hash":"c8d332dd49f7b6d75b0440e8db4ae8ace635bed0","modified":1518340059941},{"_id":"public/2018/02/08/user_profile/index.html","hash":"0560543063679dbbaead823d2353b954a4fbdcc5","modified":1518340059941},{"_id":"public/2018/01/13/lbfgs/index.html","hash":"265b18dedeef87a003acbaaed95a5ced443d8b5c","modified":1518340059941},{"_id":"public/2018/01/03/mf/index.html","hash":"05cf18e2c4ad6608ea65a537dc9edb34a6512d29","modified":1518340059941},{"_id":"public/2017/12/23/clapack/index.html","hash":"38fd946ebb5b78322b44f8ccd5440edc4ff96b80","modified":1518340059942},{"_id":"public/2017/08/20/dnn_for_youtube_recommandation/index.html","hash":"a68adb40b423e083a4d7ca073904020e7c1d640d","modified":1518340059942},{"_id":"public/2017/05/30/qualified_model/index.html","hash":"aae3d92a85471761de0dad478e3e7626af102ffd","modified":1518340059942},{"_id":"public/2017/03/16/gender_based_rs/index.html","hash":"9c4e2916313bb2ddd5f98aaadf7607be8d1a2003","modified":1518340059942},{"_id":"public/2017/02/22/als/index.html","hash":"faf267a40682d56f96e974ef95c89f85b6c0bb77","modified":1518340059942},{"_id":"public/2016/12/07/decision_tree/index.html","hash":"cc877e90207fe456514c2e97cefac546fa324368","modified":1518340059942},{"_id":"public/archives/index.html","hash":"c5dd7afece84d41713c2e762836e3da2a884fd1b","modified":1518340059942},{"_id":"public/index.html","hash":"b37a2d5025cf9e764f72d9e50c706fc3a6a8860b","modified":1518340059942},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1518340059951},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1518340059951},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1518340059952},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1518340059952},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1518340059952},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1518340059952},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1518340059952},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1518340059952},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1518340059952},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1518340059952},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1518340059952},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1518340059952},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1518340059952},{"_id":"public/vendors/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1518340059952},{"_id":"public/vendors/font-awesome/HELP-US-OUT.txt","hash":"69a4c537d167b68a0ccf1c6febd138aeffca60d6","modified":1518340059952},{"_id":"public/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1518340059952},{"_id":"public/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1518340059952},{"_id":"public/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1518340059952},{"_id":"public/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1518340059952},{"_id":"public/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1518340059952},{"_id":"public/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1518340059953},{"_id":"public/vendors/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1518340059953},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.woff2","hash":"574ea2698c03ae9477db2ea3baf460ee32f1a7ea","modified":1518340059953},{"_id":"public/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1518340059953},{"_id":"public/2017/05/30/qualified_model/probability.png","hash":"da42fdb54fbec386e942415f4ee09302bce9f0e7","modified":1518340059953},{"_id":"public/2018/01/03/mf/data_model.png","hash":"ce7a86fc2982befc2f1ffa642b537e6b4179f59e","modified":1518340059953},{"_id":"public/2017/08/20/dnn_for_youtube_recommandation/evaluation_of_diffrent_configure.png","hash":"c5952d8425d643b83681001e8d4621b95ce81e01","modified":1518340059953},{"_id":"public/2017/08/20/dnn_for_youtube_recommandation/example_age_efficacy.png","hash":"a3b30b70d05240d3064fb536b61fdbaf43c914ea","modified":1518340059953},{"_id":"public/2017/08/20/dnn_for_youtube_recommandation/model_performance_with_feature_depth.png","hash":"62df28e661723caf2a24b3ab934ad1610064d5a9","modified":1518340059953},{"_id":"public/2017/08/20/dnn_for_youtube_recommandation/recommand_system.png","hash":"560d6d348f9d2b9426b8a7348a7f7b1e22f7d397","modified":1518340059953},{"_id":"public/2018/02/08/user_profile/life_recycle.png","hash":"7b27c6d396c9976d828f2ec80673492cfb5728f2","modified":1518340059953},{"_id":"public/2018/02/08/user_profile/life_recycle_conversion.png","hash":"454db2488358cecfe2151c78e98d3826dce3387f","modified":1518340059954},{"_id":"public/vendors/font-awesome/fonts/FontAwesome.otf","hash":"0112e96f327d413938d37c1693806f468ffdbace","modified":1518340060417},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.eot","hash":"b3c2f08e73320135b69c23a3908b87a12053a2f6","modified":1518340060421},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.woff","hash":"507970402e328b2baeb05bde73bf9ded4e2c3a2d","modified":1518340060422},{"_id":"public/2017/08/20/dnn_for_youtube_recommandation/recommand_ranking.png","hash":"7614e6c934e0639c70e4086942f7fa4aa6449d89","modified":1518340060422},{"_id":"public/2017/08/20/dnn_for_youtube_recommandation/recommand_matching.png","hash":"3c527cebfcc1ad21ee432552a49a8ae661cce6c4","modified":1518340060422},{"_id":"public/2016/12/07/decision_tree/decision_tree_example.png","hash":"470e012a966a7aad69f5acdcd462c0847224ed55","modified":1518340060422},{"_id":"public/2017/08/20/dnn_for_youtube_recommandation/trainning_sequence_behavior_and_lable.png","hash":"caf27b28796e6f4953ef1dc8a04d8af7c300f79e","modified":1518340060422},{"_id":"public/2018/02/08/user_profile/function_of_user_profile.png","hash":"d66aa85a8f694de7abf66a8f21e99ea17c127b3d","modified":1518340060422},{"_id":"public/2018/02/08/user_profile/user_interest_profile.png","hash":"9f81ede538b6b663b0f6f6571458da3f6c7030d4","modified":1518340060422},{"_id":"public/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1518340060427},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1518340060427},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1518340060427},{"_id":"public/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1518340060427},{"_id":"public/js/src/post-details.js","hash":"2038f54e289b6da5def09689e69f623187147be5","modified":1518340060427},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1518340060427},{"_id":"public/js/src/utils.js","hash":"e5cb720894c4bc28ca8f10b33df127fb394018d9","modified":1518340060427},{"_id":"public/vendors/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1518340060427},{"_id":"public/vendors/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1518340060427},{"_id":"public/vendors/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1518340060427},{"_id":"public/vendors/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1518340060427},{"_id":"public/vendors/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1518340060427},{"_id":"public/vendors/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1518340060427},{"_id":"public/vendors/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1518340060427},{"_id":"public/vendors/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1518340060428},{"_id":"public/vendors/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1518340060428},{"_id":"public/vendors/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1518340060428},{"_id":"public/js/src/schemes/pisces.js","hash":"7506e7490c69a200831393c38d25e91c156bd471","modified":1518340060428},{"_id":"public/vendors/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1518340060428},{"_id":"public/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1518340060428},{"_id":"public/vendors/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1518340060428},{"_id":"public/vendors/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1518340060428},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1518340060428},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1518340060428},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1518340060428},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1518340060428},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1518340060428},{"_id":"public/css/main.css","hash":"9bcd11f7a22729ea4d7281ce9121fca093430c3c","modified":1518340060428},{"_id":"public/vendors/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1518340060428},{"_id":"public/vendors/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1518340060428},{"_id":"public/vendors/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1518340060428},{"_id":"public/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1518340060428},{"_id":"public/vendors/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1518340060428},{"_id":"public/vendors/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1518340060429},{"_id":"public/vendors/font-awesome/css/font-awesome.css","hash":"3b87c2560832748cd06f9bfd2fd6ea8edbdae8c7","modified":1518340060429},{"_id":"public/vendors/font-awesome/css/font-awesome.min.css","hash":"05ea25bc9b3ac48993e1fee322d3bc94b49a6e22","modified":1518340060429},{"_id":"public/vendors/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1518340060429},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.ttf","hash":"27cf1f2ec59aece6938c7bb2feb0e287ea778ff9","modified":1518340060429},{"_id":"public/vendors/font-awesome/fonts/fontawesome-webfont.svg","hash":"2b3c8ba7008cc014d8fb37abc6f9f49aeda83824","modified":1518340060438}],"Category":[{"name":"工具学习","_id":"cjdikgud40002ga01b4m7c44h"},{"name":"推荐系统","_id":"cjdikgudb0007ga01nnetrsc3"},{"name":"模型与算法","_id":"cjdikgudj000iga01cvx2sj2f"},{"name":"用户画像","_id":"cjdikgudl000rga01wlpvb9az"}],"Data":[],"Page":[{"title":"about","date":"2017-02-23T00:04:37.000Z","type":"about","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-02-23 08:04:37\ntype: about\n---\n","updated":"2018-02-11T08:14:48.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjdikguhm001pga01rb82mh69","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2017-02-23T00:05:23.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-02-23 08:05:23\ntype: tags\n---\n","updated":"2018-02-11T08:14:48.000Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjdikguhq001rga01vz37uk0k","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2017-02-23T00:01:45.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-02-23 08:01:45\ntype: categories\n---\n","updated":"2018-02-11T08:14:48.000Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjdikguhv001uga01g4tbn3dj","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"CLAPACK学习","date":"2017-12-22T16:00:00.000Z","toc":true,"description":"一个开源的线性代数工具包，可用于求解线性方程组、线性最小二乘、特征值和奇异值等相关问题","mathjax":true,"_content":"\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n**LAPACK**：全称Linear Algebra PACKage，美国国家科学基金等资助开发的著名公开软。以Fortran语言编写，提供了丰富函数，用于求解线性方程组、线性最小二乘、特征值和奇异值等相关问题。spark mllib, mxnet等都在底层使用了lapack进行相关的线性代数计算。\n\n**CLAPACK**：使用f2c工具将LAPACK的Fortran代码转换成C语言代码的C语言算法包, 可用于在C语言环境中直接调用线性代数的相关函数功能。\n\n本文主要是描述如何在linux环境中安装clapack，以CLAPACK-3.2.1为例进行说明，并使用clapack实现cholesky分解过程。\n\n# clapack安装\n## 准备安装文件\n* 远程获取文件：wget http://www.netlib.org/clapack/clapack.tgz  \n* 将clapack.tgz拷贝到准备安装的目录，运行tar -xvf clapack.tgz 完成解压。\n* cd CLAPACK-3.2.1 进入CLAPACK主目录。\n* cp make.inc.example make.inc \n\n**此时目录下的主要文件目录：**\n\n* BLAS：blas C语言源码，clapack需要调用的该底层函数库。\n* F2CLIBS：f2c相关函数库\n* INCLUDE：clapack, blas, f2c库对应的头文件\n* INSTALL：测试函数，对于不同的平台提前测试make.inc对应的配置\n* Makefile：构建文件\n* make.inc：定义compiler, compile flags and library。\n* SRC：LAPACK c语言代码，当我们要查某个函数的具体参数时，可以到这个目录下根据函数的名字找到对应的.c文件\n* TESTING：用于对clapack函数测试其正确性\n\n## 安装\n* 编译f2c: make f2clib\n* 编译blas: make blaslib, 需要注意的是，这里是使用的该clapck包所引用的blas库，没有针对所有机器做优化。如果想针对自己的机器，使用对应的库使速度达到最优，可以参考BLAS/WRAP目录\n* 运行blas测试程序：\n  cd BLAS/TESTING && make -f Makeblat2\n  cd ..\n  ./xblat2s < sblat2.in\n  ./xblat2d < dblat2.in\n  ./xblat2c < cblat2.in\n  ./xblat2z < zblat2.in\n  cd TESTING && make -f Makeblat3\n  cd ..\n\t./xblat3s < sblat3.in\n\t./xblat3d < dblat3.in\n\t./xblat3c < cblat3.in\n\t./xblat3z < zblat3.in\n  cd ..\n* 修改make.inc:\n  CC        = gcc\n  BLASLIB      = ../../blas$(PLAT).a\n\n* 编译clapack源码及相关测试：\n  cd INSTALL && make && cd ..\n  cd SRC/ && make && cd ..\n  cd TESTING/MATGEN && make && cd .. && make\n  上述步骤都通过后，在主目录下生成blas_LINUX.a， lapack_LINUX.a二个库，其他程序调用时通过引用这两个库，调用clapack完成线性代数相关计算。  \n \n# 测试\n   为测试环境可正常使用，此处使用clapack，利用cholesky分解求解线性方程组。\n   方程组如下：\n   ```\n   A = “4.16   -3.12  0.56  -0.10\n        -3.12  5.03   -0.83 1.18\n        0.56   -0.83  0.76  0.34\n        -0.10  1.18   0.34  1.18”\n                          \n   b = \"8.7\n        -13.35\n        1.89\n        -4.14\"\n   求解 Ax = b方程组\n   ```\n   \n   c++代码：\n   ```c++\n#include <iostream>\n#include <fstream>\n#include \"blaswrap.h\"\n#include \"f2c.h\"\n#include \"clapack.h\"\n\nusing namespace std;\n\nint main(int argc, char** argv){\n\n  long int k = 4;\n  long int nrhs = 1;\n  long int ldb = k;\n  long int info = 0;\n  double a[10]={4.16, -3.12, 5.03, 0.56, -0.83, 0.76, -0.10, 1.18, 0.34, 1.18};\n  double b[4]={8.7, -13.35, 1.89, -4.14};\n  \n  char matrix_type='U'; \n  dppsv_(&matrix_type, &k, &nrhs, a, b, &k, &info);\n  cout << \"solution:\";\n  for (int i=0; i< k; i++){\n      cout << b[i] << \" \";\n  }\n  cout << endl;\n  return 0;\n}\n   ```\n**编译程序：**\n在目录下新建文件夹example, mkdir example\n保存文件main.cc\ng++ -o main main.cc -I ../INCLUDE -L ../ -lblas -llapack   \n(-I和-L选项需要根据自己机器对应的头文件和库文件目录来写)\n\n**运行程序：**\n./main\n\n输入如下： solution:1 -1 2 -3\n \n# 参考资料\n【1】 lapack@cs.utk.edu, http://www.netlib.org/clapack/\n\n\n","source":"_posts/clapack.md","raw":"---\ntitle: CLAPACK学习\ndate: 2017-12-23\ntoc: true\ncategories: 工具学习\ntags: [lapack, clapack, 线性代数工具包]\ndescription: 一个开源的线性代数工具包，可用于求解线性方程组、线性最小二乘、特征值和奇异值等相关问题\nmathjax: true\n---\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n**LAPACK**：全称Linear Algebra PACKage，美国国家科学基金等资助开发的著名公开软。以Fortran语言编写，提供了丰富函数，用于求解线性方程组、线性最小二乘、特征值和奇异值等相关问题。spark mllib, mxnet等都在底层使用了lapack进行相关的线性代数计算。\n\n**CLAPACK**：使用f2c工具将LAPACK的Fortran代码转换成C语言代码的C语言算法包, 可用于在C语言环境中直接调用线性代数的相关函数功能。\n\n本文主要是描述如何在linux环境中安装clapack，以CLAPACK-3.2.1为例进行说明，并使用clapack实现cholesky分解过程。\n\n# clapack安装\n## 准备安装文件\n* 远程获取文件：wget http://www.netlib.org/clapack/clapack.tgz  \n* 将clapack.tgz拷贝到准备安装的目录，运行tar -xvf clapack.tgz 完成解压。\n* cd CLAPACK-3.2.1 进入CLAPACK主目录。\n* cp make.inc.example make.inc \n\n**此时目录下的主要文件目录：**\n\n* BLAS：blas C语言源码，clapack需要调用的该底层函数库。\n* F2CLIBS：f2c相关函数库\n* INCLUDE：clapack, blas, f2c库对应的头文件\n* INSTALL：测试函数，对于不同的平台提前测试make.inc对应的配置\n* Makefile：构建文件\n* make.inc：定义compiler, compile flags and library。\n* SRC：LAPACK c语言代码，当我们要查某个函数的具体参数时，可以到这个目录下根据函数的名字找到对应的.c文件\n* TESTING：用于对clapack函数测试其正确性\n\n## 安装\n* 编译f2c: make f2clib\n* 编译blas: make blaslib, 需要注意的是，这里是使用的该clapck包所引用的blas库，没有针对所有机器做优化。如果想针对自己的机器，使用对应的库使速度达到最优，可以参考BLAS/WRAP目录\n* 运行blas测试程序：\n  cd BLAS/TESTING && make -f Makeblat2\n  cd ..\n  ./xblat2s < sblat2.in\n  ./xblat2d < dblat2.in\n  ./xblat2c < cblat2.in\n  ./xblat2z < zblat2.in\n  cd TESTING && make -f Makeblat3\n  cd ..\n\t./xblat3s < sblat3.in\n\t./xblat3d < dblat3.in\n\t./xblat3c < cblat3.in\n\t./xblat3z < zblat3.in\n  cd ..\n* 修改make.inc:\n  CC        = gcc\n  BLASLIB      = ../../blas$(PLAT).a\n\n* 编译clapack源码及相关测试：\n  cd INSTALL && make && cd ..\n  cd SRC/ && make && cd ..\n  cd TESTING/MATGEN && make && cd .. && make\n  上述步骤都通过后，在主目录下生成blas_LINUX.a， lapack_LINUX.a二个库，其他程序调用时通过引用这两个库，调用clapack完成线性代数相关计算。  \n \n# 测试\n   为测试环境可正常使用，此处使用clapack，利用cholesky分解求解线性方程组。\n   方程组如下：\n   ```\n   A = “4.16   -3.12  0.56  -0.10\n        -3.12  5.03   -0.83 1.18\n        0.56   -0.83  0.76  0.34\n        -0.10  1.18   0.34  1.18”\n                          \n   b = \"8.7\n        -13.35\n        1.89\n        -4.14\"\n   求解 Ax = b方程组\n   ```\n   \n   c++代码：\n   ```c++\n#include <iostream>\n#include <fstream>\n#include \"blaswrap.h\"\n#include \"f2c.h\"\n#include \"clapack.h\"\n\nusing namespace std;\n\nint main(int argc, char** argv){\n\n  long int k = 4;\n  long int nrhs = 1;\n  long int ldb = k;\n  long int info = 0;\n  double a[10]={4.16, -3.12, 5.03, 0.56, -0.83, 0.76, -0.10, 1.18, 0.34, 1.18};\n  double b[4]={8.7, -13.35, 1.89, -4.14};\n  \n  char matrix_type='U'; \n  dppsv_(&matrix_type, &k, &nrhs, a, b, &k, &info);\n  cout << \"solution:\";\n  for (int i=0; i< k; i++){\n      cout << b[i] << \" \";\n  }\n  cout << endl;\n  return 0;\n}\n   ```\n**编译程序：**\n在目录下新建文件夹example, mkdir example\n保存文件main.cc\ng++ -o main main.cc -I ../INCLUDE -L ../ -lblas -llapack   \n(-I和-L选项需要根据自己机器对应的头文件和库文件目录来写)\n\n**运行程序：**\n./main\n\n输入如下： solution:1 -1 2 -3\n \n# 参考资料\n【1】 lapack@cs.utk.edu, http://www.netlib.org/clapack/\n\n\n","slug":"clapack","published":1,"updated":"2018-02-11T08:33:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjdikgucx0000ga01xo5et13r","content":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<p><strong>LAPACK</strong>：全称Linear Algebra PACKage，美国国家科学基金等资助开发的著名公开软。以Fortran语言编写，提供了丰富函数，用于求解线性方程组、线性最小二乘、特征值和奇异值等相关问题。spark mllib, mxnet等都在底层使用了lapack进行相关的线性代数计算。</p>\n<p><strong>CLAPACK</strong>：使用f2c工具将LAPACK的Fortran代码转换成C语言代码的C语言算法包, 可用于在C语言环境中直接调用线性代数的相关函数功能。</p>\n<p>本文主要是描述如何在linux环境中安装clapack，以CLAPACK-3.2.1为例进行说明，并使用clapack实现cholesky分解过程。</p>\n<h1 id=\"clapack安装\"><a href=\"#clapack安装\" class=\"headerlink\" title=\"clapack安装\"></a>clapack安装</h1><h2 id=\"准备安装文件\"><a href=\"#准备安装文件\" class=\"headerlink\" title=\"准备安装文件\"></a>准备安装文件</h2><ul>\n<li>远程获取文件：wget <a href=\"http://www.netlib.org/clapack/clapack.tgz\" target=\"_blank\" rel=\"noopener\">http://www.netlib.org/clapack/clapack.tgz</a>  </li>\n<li>将clapack.tgz拷贝到准备安装的目录，运行tar -xvf clapack.tgz 完成解压。</li>\n<li>cd CLAPACK-3.2.1 进入CLAPACK主目录。</li>\n<li>cp make.inc.example make.inc </li>\n</ul>\n<p><strong>此时目录下的主要文件目录：</strong></p>\n<ul>\n<li>BLAS：blas C语言源码，clapack需要调用的该底层函数库。</li>\n<li>F2CLIBS：f2c相关函数库</li>\n<li>INCLUDE：clapack, blas, f2c库对应的头文件</li>\n<li>INSTALL：测试函数，对于不同的平台提前测试make.inc对应的配置</li>\n<li>Makefile：构建文件</li>\n<li>make.inc：定义compiler, compile flags and library。</li>\n<li>SRC：LAPACK c语言代码，当我们要查某个函数的具体参数时，可以到这个目录下根据函数的名字找到对应的.c文件</li>\n<li>TESTING：用于对clapack函数测试其正确性</li>\n</ul>\n<h2 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h2><ul>\n<li>编译f2c: make f2clib</li>\n<li>编译blas: make blaslib, 需要注意的是，这里是使用的该clapck包所引用的blas库，没有针对所有机器做优化。如果想针对自己的机器，使用对应的库使速度达到最优，可以参考BLAS/WRAP目录</li>\n<li>运行blas测试程序：<br>cd BLAS/TESTING &amp;&amp; make -f Makeblat2<br>cd ..<br>./xblat2s &lt; sblat2.in<br>./xblat2d &lt; dblat2.in<br>./xblat2c &lt; cblat2.in<br>./xblat2z &lt; zblat2.in<br>cd TESTING &amp;&amp; make -f Makeblat3<br>cd ..<br>  ./xblat3s &lt; sblat3.in<br>  ./xblat3d &lt; dblat3.in<br>  ./xblat3c &lt; cblat3.in<br>  ./xblat3z &lt; zblat3.in<br>cd ..</li>\n<li><p>修改make.inc:<br>CC        = gcc<br>BLASLIB      = ../../blas$(PLAT).a</p>\n</li>\n<li><p>编译clapack源码及相关测试：<br>cd INSTALL &amp;&amp; make &amp;&amp; cd ..<br>cd SRC/ &amp;&amp; make &amp;&amp; cd ..<br>cd TESTING/MATGEN &amp;&amp; make &amp;&amp; cd .. &amp;&amp; make<br>上述步骤都通过后，在主目录下生成blas_LINUX.a， lapack_LINUX.a二个库，其他程序调用时通过引用这两个库，调用clapack完成线性代数相关计算。  </p>\n</li>\n</ul>\n<h1 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h1><p>   为测试环境可正常使用，此处使用clapack，利用cholesky分解求解线性方程组。<br>   方程组如下：<br>   <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = “4.16   -3.12  0.56  -0.10</span><br><span class=\"line\">     -3.12  5.03   -0.83 1.18</span><br><span class=\"line\">     0.56   -0.83  0.76  0.34</span><br><span class=\"line\">     -0.10  1.18   0.34  1.18”</span><br><span class=\"line\">                       </span><br><span class=\"line\">b = &quot;8.7</span><br><span class=\"line\">     -13.35</span><br><span class=\"line\">     1.89</span><br><span class=\"line\">     -4.14&quot;</span><br><span class=\"line\">求解 Ax = b方程组</span><br></pre></td></tr></table></figure></p>\n<p>   c++代码：<br>   <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;fstream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"blaswrap.h\"</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"f2c.h\"</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"clapack.h\"</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">long</span> <span class=\"keyword\">int</span> k = <span class=\"number\">4</span>;</span><br><span class=\"line\">  <span class=\"keyword\">long</span> <span class=\"keyword\">int</span> nrhs = <span class=\"number\">1</span>;</span><br><span class=\"line\">  <span class=\"keyword\">long</span> <span class=\"keyword\">int</span> ldb = k;</span><br><span class=\"line\">  <span class=\"keyword\">long</span> <span class=\"keyword\">int</span> info = <span class=\"number\">0</span>;</span><br><span class=\"line\">  <span class=\"keyword\">double</span> a[<span class=\"number\">10</span>]=&#123;<span class=\"number\">4.16</span>, <span class=\"number\">-3.12</span>, <span class=\"number\">5.03</span>, <span class=\"number\">0.56</span>, <span class=\"number\">-0.83</span>, <span class=\"number\">0.76</span>, <span class=\"number\">-0.10</span>, <span class=\"number\">1.18</span>, <span class=\"number\">0.34</span>, <span class=\"number\">1.18</span>&#125;;</span><br><span class=\"line\">  <span class=\"keyword\">double</span> b[<span class=\"number\">4</span>]=&#123;<span class=\"number\">8.7</span>, <span class=\"number\">-13.35</span>, <span class=\"number\">1.89</span>, <span class=\"number\">-4.14</span>&#125;;</span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">char</span> matrix_type=<span class=\"string\">'U'</span>; </span><br><span class=\"line\">  dppsv_(&amp;matrix_type, &amp;k, &amp;nrhs, a, b, &amp;k, &amp;info);</span><br><span class=\"line\">  <span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">\"solution:\"</span>;</span><br><span class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt; k; i++)&#123;</span><br><span class=\"line\">      <span class=\"built_in\">cout</span> &lt;&lt; b[i] &lt;&lt; <span class=\"string\">\" \"</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"built_in\">cout</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>编译程序：</strong><br>在目录下新建文件夹example, mkdir example<br>保存文件main.cc<br>g++ -o main main.cc -I ../INCLUDE -L ../ -lblas -llapack<br>(-I和-L选项需要根据自己机器对应的头文件和库文件目录来写)</p>\n<p><strong>运行程序：</strong><br>./main</p>\n<p>输入如下： solution:1 -1 2 -3</p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>【1】 lapack@cs.utk.edu, <a href=\"http://www.netlib.org/clapack/\" target=\"_blank\" rel=\"noopener\">http://www.netlib.org/clapack/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<p><strong>LAPACK</strong>：全称Linear Algebra PACKage，美国国家科学基金等资助开发的著名公开软。以Fortran语言编写，提供了丰富函数，用于求解线性方程组、线性最小二乘、特征值和奇异值等相关问题。spark mllib, mxnet等都在底层使用了lapack进行相关的线性代数计算。</p>\n<p><strong>CLAPACK</strong>：使用f2c工具将LAPACK的Fortran代码转换成C语言代码的C语言算法包, 可用于在C语言环境中直接调用线性代数的相关函数功能。</p>\n<p>本文主要是描述如何在linux环境中安装clapack，以CLAPACK-3.2.1为例进行说明，并使用clapack实现cholesky分解过程。</p>\n<h1 id=\"clapack安装\"><a href=\"#clapack安装\" class=\"headerlink\" title=\"clapack安装\"></a>clapack安装</h1><h2 id=\"准备安装文件\"><a href=\"#准备安装文件\" class=\"headerlink\" title=\"准备安装文件\"></a>准备安装文件</h2><ul>\n<li>远程获取文件：wget <a href=\"http://www.netlib.org/clapack/clapack.tgz\" target=\"_blank\" rel=\"noopener\">http://www.netlib.org/clapack/clapack.tgz</a>  </li>\n<li>将clapack.tgz拷贝到准备安装的目录，运行tar -xvf clapack.tgz 完成解压。</li>\n<li>cd CLAPACK-3.2.1 进入CLAPACK主目录。</li>\n<li>cp make.inc.example make.inc </li>\n</ul>\n<p><strong>此时目录下的主要文件目录：</strong></p>\n<ul>\n<li>BLAS：blas C语言源码，clapack需要调用的该底层函数库。</li>\n<li>F2CLIBS：f2c相关函数库</li>\n<li>INCLUDE：clapack, blas, f2c库对应的头文件</li>\n<li>INSTALL：测试函数，对于不同的平台提前测试make.inc对应的配置</li>\n<li>Makefile：构建文件</li>\n<li>make.inc：定义compiler, compile flags and library。</li>\n<li>SRC：LAPACK c语言代码，当我们要查某个函数的具体参数时，可以到这个目录下根据函数的名字找到对应的.c文件</li>\n<li>TESTING：用于对clapack函数测试其正确性</li>\n</ul>\n<h2 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h2><ul>\n<li>编译f2c: make f2clib</li>\n<li>编译blas: make blaslib, 需要注意的是，这里是使用的该clapck包所引用的blas库，没有针对所有机器做优化。如果想针对自己的机器，使用对应的库使速度达到最优，可以参考BLAS/WRAP目录</li>\n<li>运行blas测试程序：<br>cd BLAS/TESTING &amp;&amp; make -f Makeblat2<br>cd ..<br>./xblat2s &lt; sblat2.in<br>./xblat2d &lt; dblat2.in<br>./xblat2c &lt; cblat2.in<br>./xblat2z &lt; zblat2.in<br>cd TESTING &amp;&amp; make -f Makeblat3<br>cd ..<br>  ./xblat3s &lt; sblat3.in<br>  ./xblat3d &lt; dblat3.in<br>  ./xblat3c &lt; cblat3.in<br>  ./xblat3z &lt; zblat3.in<br>cd ..</li>\n<li><p>修改make.inc:<br>CC        = gcc<br>BLASLIB      = ../../blas$(PLAT).a</p>\n</li>\n<li><p>编译clapack源码及相关测试：<br>cd INSTALL &amp;&amp; make &amp;&amp; cd ..<br>cd SRC/ &amp;&amp; make &amp;&amp; cd ..<br>cd TESTING/MATGEN &amp;&amp; make &amp;&amp; cd .. &amp;&amp; make<br>上述步骤都通过后，在主目录下生成blas_LINUX.a， lapack_LINUX.a二个库，其他程序调用时通过引用这两个库，调用clapack完成线性代数相关计算。  </p>\n</li>\n</ul>\n<h1 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h1><p>   为测试环境可正常使用，此处使用clapack，利用cholesky分解求解线性方程组。<br>   方程组如下：<br>   <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = “4.16   -3.12  0.56  -0.10</span><br><span class=\"line\">     -3.12  5.03   -0.83 1.18</span><br><span class=\"line\">     0.56   -0.83  0.76  0.34</span><br><span class=\"line\">     -0.10  1.18   0.34  1.18”</span><br><span class=\"line\">                       </span><br><span class=\"line\">b = &quot;8.7</span><br><span class=\"line\">     -13.35</span><br><span class=\"line\">     1.89</span><br><span class=\"line\">     -4.14&quot;</span><br><span class=\"line\">求解 Ax = b方程组</span><br></pre></td></tr></table></figure></p>\n<p>   c++代码：<br>   <figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;fstream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"blaswrap.h\"</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"f2c.h\"</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">\"clapack.h\"</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">int</span> argc, <span class=\"keyword\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">long</span> <span class=\"keyword\">int</span> k = <span class=\"number\">4</span>;</span><br><span class=\"line\">  <span class=\"keyword\">long</span> <span class=\"keyword\">int</span> nrhs = <span class=\"number\">1</span>;</span><br><span class=\"line\">  <span class=\"keyword\">long</span> <span class=\"keyword\">int</span> ldb = k;</span><br><span class=\"line\">  <span class=\"keyword\">long</span> <span class=\"keyword\">int</span> info = <span class=\"number\">0</span>;</span><br><span class=\"line\">  <span class=\"keyword\">double</span> a[<span class=\"number\">10</span>]=&#123;<span class=\"number\">4.16</span>, <span class=\"number\">-3.12</span>, <span class=\"number\">5.03</span>, <span class=\"number\">0.56</span>, <span class=\"number\">-0.83</span>, <span class=\"number\">0.76</span>, <span class=\"number\">-0.10</span>, <span class=\"number\">1.18</span>, <span class=\"number\">0.34</span>, <span class=\"number\">1.18</span>&#125;;</span><br><span class=\"line\">  <span class=\"keyword\">double</span> b[<span class=\"number\">4</span>]=&#123;<span class=\"number\">8.7</span>, <span class=\"number\">-13.35</span>, <span class=\"number\">1.89</span>, <span class=\"number\">-4.14</span>&#125;;</span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"keyword\">char</span> matrix_type=<span class=\"string\">'U'</span>; </span><br><span class=\"line\">  dppsv_(&amp;matrix_type, &amp;k, &amp;nrhs, a, b, &amp;k, &amp;info);</span><br><span class=\"line\">  <span class=\"built_in\">cout</span> &lt;&lt; <span class=\"string\">\"solution:\"</span>;</span><br><span class=\"line\">  <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt; k; i++)&#123;</span><br><span class=\"line\">      <span class=\"built_in\">cout</span> &lt;&lt; b[i] &lt;&lt; <span class=\"string\">\" \"</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"built_in\">cout</span> &lt;&lt; <span class=\"built_in\">endl</span>;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>编译程序：</strong><br>在目录下新建文件夹example, mkdir example<br>保存文件main.cc<br>g++ -o main main.cc -I ../INCLUDE -L ../ -lblas -llapack<br>(-I和-L选项需要根据自己机器对应的头文件和库文件目录来写)</p>\n<p><strong>运行程序：</strong><br>./main</p>\n<p>输入如下： solution:1 -1 2 -3</p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>【1】 lapack@cs.utk.edu, <a href=\"http://www.netlib.org/clapack/\" target=\"_blank\" rel=\"noopener\">http://www.netlib.org/clapack/</a></p>\n"},{"title":"一种基于性别的推荐方法","date":"2017-03-15T16:00:00.000Z","toc":true,"description":"一种基于性别的推荐方法","mathjax":true,"_content":"\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n# 基于性别的推荐\n**基于内容的推荐**:基于协同过滤掉方法，存在非常明显的缺点：无法解决冷启动问题（包括user冷启动和item冷启动），当用户没有行为时无法进行推荐，或者行为非常稀疏的时候，虽然可以产生推荐列表，但会存在过度泛化问题【1】。相对于协同过滤，基于内容的推荐的方法, 能够较好地应对冷启动问题。通过提取user和item相关内容信息，并根据所提取的信息进行推荐。但是，user和item的内容信息的提取往往需要比较专业的知识，其提取过程需要耗费较大的精力。\n\n**基于性别的推荐**: 基于性别进行推荐属于一种基于内容的推荐方法，通过利用user性别信息、item的性别偏好信息进行推荐。该方法的优点是简单、可以在用户没有任何行为时推荐该性别的用户普遍感兴趣的内容；同时缺点也比较明显：同一性别的用户，其兴趣也可会有较大不同，因此该方法无法做到千人千面。\n\n \n# 推荐方案设计\n   整个推荐方案包括三个步骤：user性别信息获取、item性别偏好计算、基于性别的item推荐。\n\n## user性别信息获取\n\n 用户性别信息主要来自用户注册信息。由于用户注册时，性别可能会填写错误，或者不填写，此时可通过其它方式多用户的性别进行校准和填充。如根据大多数用户的信息，计算item的性别偏好信息。根据该用户在各个item上的行为表现，计算该用户的性别画像。\n\n## item性别偏好计算\n**属性计算**\n\n基于有性别信息的用户，对item的消费行为，计算item的性别偏好。\n \n  $P(C|I\\_j)=\\frac{\\sum\\_iR\\_{ij}U\\_i(C)}{\\sum\\_iR\\_{ij}}（式1）$\n\n其中$P(C|I_j)$表示第j个item对应属性C的偏好得分。\n\n$U\\_i(C)$表示第i个用户对应属性C的概率值，可以根据用户注册信息、用户行为信息挖掘等方式得到。\n\n$R\\_{ij}$表示第i个用户对第j个item的喜欢程度，可对用户在item上的各种行（如点击、点赞、评论、发图、收藏等）分别赋予不同的权值，然后根据用户的行为类型、行为频次等进行加权得到。\n\n**属性平滑**\n\n当平台上用户的数量和活跃度比较接近时，直接使用公式1是没有问题的。但是当不同性别的用户数量相差较大时，根据公式1计算出的item偏好会明显倾向于数量较多的那个性别。例如，平台上的男性用户数量：女性用户数量=5:1， 我们只考虑点击行为，当某个item男女点击比例为5:1时，我们并不能认为该item更容易被男性用户喜欢，而是认为该item对男女的吸引程度相同；男女点击比例为1:1时，该item对女性的吸引程度是更大的。\n\n\n因此，需要对item的性别偏好进行平滑,方法如下：\n\n* 平滑系数计算\n\n  $SMOOTH(C)=\\frac{\\sum\\_i\\sum\\_jR\\_{ij}U\\_i(C)}{\\sum\\_i\\sum\\_{j}R\\_{ij}}（式2)$\n\n* 对公式1进行平滑\n\n  $P\\_{smooth}(C|I\\_j)=\\frac{P(C|I\\_j)}{SMOOTH(C)}（式3)$\n\n**加入其它因素**\n\n  通过公式3得到的属性计算结果修正了平台不同属性用户数量不同带来的干扰，但是依然有些可以改进的地方。比如对于两个不同的item, 其男女偏好得分是非常接近，同样只考虑点击行为。其中一个item A被用户点击的次数为10万、点击的用户数8万等明显多于item B（点击次数100， 点击人数50）, 那么我们可以认为item A的得分是更加可靠的。因此，可考虑加入点击次数等相关因素。\n \n  $P(other|I\\_j) = \\frac{\\sum\\_iR\\_{ij}}{\\sum\\_i\\sum\\_jR\\_{ij}} (式4)$\n   \n  由于我们的目的主要是找出对每个性别更感兴趣的item, 因此，当$P\\_{smooth}(C|I\\_j)$大于0.5时，利用如下公式进一步调整分数。\n   \n  $P\\_{opt}(C|I\\_j)=\\alpha P\\_{smooth}(C|I\\_j) + (1-\\alpha)P\\(other|I\\_j) （式5)$  \n\n  通过公式5，综合考虑了属性偏好及热度因素。对分数差别较小的item，如果都是偏向于属性C（$P\\_{smooth}(C|I\\_j)$大于0.5），可以根据热度等相关因素进行排序;而对于分数差别比较大的item, 则依然可以维持原来的偏好进行顺序。\n  \n**归一化**\n   \n  对上述计算结果进行归一化，如下所示：\n     \n  $P\\_{norm}(C|I\\_j)=\\frac{P\\_{opt}(C|I_j)}{\\sum\\_CP\\_{opt}(C|I\\_j)} （式6)$\n\n\n## 基于性别的item推荐\n\n完成计算item的性别偏好信息之后，可进一步用于解决推荐时遇到的用户冷启动问题。具体方法如下：\n\n* 在线用户筛选及性别获取\n\n  当用户到来时，如果用户具有较多行为，可根据协同过滤相关方法进行推荐（如als）。否则获取用户注册的性别，或者通过其它方式矫正和填充后的性别信息。\n \n* 基于性别信息获取推荐结果\n \n  根据性别信息，选择相关性别偏好得分最高的k个item进行推荐。为实现推荐的多样性，提升用户体验，可以对item进行一定的分类，对于每个类别，分别取前top k个item构成多个队列，每次用户请求时，根据业务需求从每个队列中尽量均匀地选取item进行推荐。\n  \n  \n# 实验\n\n* 离线实验\n\n    对in平台图片进行实验，男女的性别偏好有着较大的区别。实验结果显示，男性最喜欢的图片的是汽车、美女、运动、建筑等，而女性则更喜欢美妆、动漫、美食、宠物等类别的图片。\n\n* 在线实验\n\n    对于新增用户进行实验，分为实验组和对照组，实验组采用基于性别的推荐方案，对照组保持着之前的逻辑不变（基于热度的推荐）。分别评估实验组和对照组用户点击、平均下滑次数。实验结果显示，**男性平均点击次数提升20.8%,平均下滑次数提升19%；女性平均点击次数提升1.1%，平均下滑次数提升1.4%。**女性用户效果提升不明显，其主要原因在于in平台的主要用户是女性，根据热度进行排序基本反映了女性用户对图片的喜欢程度。对男性用户效果明显，主要原因是对照组根据热度推荐，更多推荐的是女性用户喜欢的图片，而实验组基于性别的推荐，则利用热度和性别偏好两方面信息，推荐了男性用户更喜欢的图片。\n    \n# 参考资料\n\n\n【1】HT Cheng, L Koc, J Harmsen, T Shaked, “Wide & Deep Learning for Recommender Systems”, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, 2016.09, pp.7-10\n\n","source":"_posts/gender_based_rs.md","raw":"---\ntitle: 一种基于性别的推荐方法\ndate: 2017-03-16\ntoc: true\ncategories: 推荐系统\ntags: [推荐算法,基于性别的推荐]\ndescription: 一种基于性别的推荐方法\nmathjax: true\n---\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n# 基于性别的推荐\n**基于内容的推荐**:基于协同过滤掉方法，存在非常明显的缺点：无法解决冷启动问题（包括user冷启动和item冷启动），当用户没有行为时无法进行推荐，或者行为非常稀疏的时候，虽然可以产生推荐列表，但会存在过度泛化问题【1】。相对于协同过滤，基于内容的推荐的方法, 能够较好地应对冷启动问题。通过提取user和item相关内容信息，并根据所提取的信息进行推荐。但是，user和item的内容信息的提取往往需要比较专业的知识，其提取过程需要耗费较大的精力。\n\n**基于性别的推荐**: 基于性别进行推荐属于一种基于内容的推荐方法，通过利用user性别信息、item的性别偏好信息进行推荐。该方法的优点是简单、可以在用户没有任何行为时推荐该性别的用户普遍感兴趣的内容；同时缺点也比较明显：同一性别的用户，其兴趣也可会有较大不同，因此该方法无法做到千人千面。\n\n \n# 推荐方案设计\n   整个推荐方案包括三个步骤：user性别信息获取、item性别偏好计算、基于性别的item推荐。\n\n## user性别信息获取\n\n 用户性别信息主要来自用户注册信息。由于用户注册时，性别可能会填写错误，或者不填写，此时可通过其它方式多用户的性别进行校准和填充。如根据大多数用户的信息，计算item的性别偏好信息。根据该用户在各个item上的行为表现，计算该用户的性别画像。\n\n## item性别偏好计算\n**属性计算**\n\n基于有性别信息的用户，对item的消费行为，计算item的性别偏好。\n \n  $P(C|I\\_j)=\\frac{\\sum\\_iR\\_{ij}U\\_i(C)}{\\sum\\_iR\\_{ij}}（式1）$\n\n其中$P(C|I_j)$表示第j个item对应属性C的偏好得分。\n\n$U\\_i(C)$表示第i个用户对应属性C的概率值，可以根据用户注册信息、用户行为信息挖掘等方式得到。\n\n$R\\_{ij}$表示第i个用户对第j个item的喜欢程度，可对用户在item上的各种行（如点击、点赞、评论、发图、收藏等）分别赋予不同的权值，然后根据用户的行为类型、行为频次等进行加权得到。\n\n**属性平滑**\n\n当平台上用户的数量和活跃度比较接近时，直接使用公式1是没有问题的。但是当不同性别的用户数量相差较大时，根据公式1计算出的item偏好会明显倾向于数量较多的那个性别。例如，平台上的男性用户数量：女性用户数量=5:1， 我们只考虑点击行为，当某个item男女点击比例为5:1时，我们并不能认为该item更容易被男性用户喜欢，而是认为该item对男女的吸引程度相同；男女点击比例为1:1时，该item对女性的吸引程度是更大的。\n\n\n因此，需要对item的性别偏好进行平滑,方法如下：\n\n* 平滑系数计算\n\n  $SMOOTH(C)=\\frac{\\sum\\_i\\sum\\_jR\\_{ij}U\\_i(C)}{\\sum\\_i\\sum\\_{j}R\\_{ij}}（式2)$\n\n* 对公式1进行平滑\n\n  $P\\_{smooth}(C|I\\_j)=\\frac{P(C|I\\_j)}{SMOOTH(C)}（式3)$\n\n**加入其它因素**\n\n  通过公式3得到的属性计算结果修正了平台不同属性用户数量不同带来的干扰，但是依然有些可以改进的地方。比如对于两个不同的item, 其男女偏好得分是非常接近，同样只考虑点击行为。其中一个item A被用户点击的次数为10万、点击的用户数8万等明显多于item B（点击次数100， 点击人数50）, 那么我们可以认为item A的得分是更加可靠的。因此，可考虑加入点击次数等相关因素。\n \n  $P(other|I\\_j) = \\frac{\\sum\\_iR\\_{ij}}{\\sum\\_i\\sum\\_jR\\_{ij}} (式4)$\n   \n  由于我们的目的主要是找出对每个性别更感兴趣的item, 因此，当$P\\_{smooth}(C|I\\_j)$大于0.5时，利用如下公式进一步调整分数。\n   \n  $P\\_{opt}(C|I\\_j)=\\alpha P\\_{smooth}(C|I\\_j) + (1-\\alpha)P\\(other|I\\_j) （式5)$  \n\n  通过公式5，综合考虑了属性偏好及热度因素。对分数差别较小的item，如果都是偏向于属性C（$P\\_{smooth}(C|I\\_j)$大于0.5），可以根据热度等相关因素进行排序;而对于分数差别比较大的item, 则依然可以维持原来的偏好进行顺序。\n  \n**归一化**\n   \n  对上述计算结果进行归一化，如下所示：\n     \n  $P\\_{norm}(C|I\\_j)=\\frac{P\\_{opt}(C|I_j)}{\\sum\\_CP\\_{opt}(C|I\\_j)} （式6)$\n\n\n## 基于性别的item推荐\n\n完成计算item的性别偏好信息之后，可进一步用于解决推荐时遇到的用户冷启动问题。具体方法如下：\n\n* 在线用户筛选及性别获取\n\n  当用户到来时，如果用户具有较多行为，可根据协同过滤相关方法进行推荐（如als）。否则获取用户注册的性别，或者通过其它方式矫正和填充后的性别信息。\n \n* 基于性别信息获取推荐结果\n \n  根据性别信息，选择相关性别偏好得分最高的k个item进行推荐。为实现推荐的多样性，提升用户体验，可以对item进行一定的分类，对于每个类别，分别取前top k个item构成多个队列，每次用户请求时，根据业务需求从每个队列中尽量均匀地选取item进行推荐。\n  \n  \n# 实验\n\n* 离线实验\n\n    对in平台图片进行实验，男女的性别偏好有着较大的区别。实验结果显示，男性最喜欢的图片的是汽车、美女、运动、建筑等，而女性则更喜欢美妆、动漫、美食、宠物等类别的图片。\n\n* 在线实验\n\n    对于新增用户进行实验，分为实验组和对照组，实验组采用基于性别的推荐方案，对照组保持着之前的逻辑不变（基于热度的推荐）。分别评估实验组和对照组用户点击、平均下滑次数。实验结果显示，**男性平均点击次数提升20.8%,平均下滑次数提升19%；女性平均点击次数提升1.1%，平均下滑次数提升1.4%。**女性用户效果提升不明显，其主要原因在于in平台的主要用户是女性，根据热度进行排序基本反映了女性用户对图片的喜欢程度。对男性用户效果明显，主要原因是对照组根据热度推荐，更多推荐的是女性用户喜欢的图片，而实验组基于性别的推荐，则利用热度和性别偏好两方面信息，推荐了男性用户更喜欢的图片。\n    \n# 参考资料\n\n\n【1】HT Cheng, L Koc, J Harmsen, T Shaked, “Wide & Deep Learning for Recommender Systems”, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, 2016.09, pp.7-10\n\n","slug":"gender_based_rs","published":1,"updated":"2018-02-11T08:33:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjdikgud20001ga01iusnbywz","content":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n<h1 id=\"基于性别的推荐\"><a href=\"#基于性别的推荐\" class=\"headerlink\" title=\"基于性别的推荐\"></a>基于性别的推荐</h1><p><strong>基于内容的推荐</strong>:基于协同过滤掉方法，存在非常明显的缺点：无法解决冷启动问题（包括user冷启动和item冷启动），当用户没有行为时无法进行推荐，或者行为非常稀疏的时候，虽然可以产生推荐列表，但会存在过度泛化问题【1】。相对于协同过滤，基于内容的推荐的方法, 能够较好地应对冷启动问题。通过提取user和item相关内容信息，并根据所提取的信息进行推荐。但是，user和item的内容信息的提取往往需要比较专业的知识，其提取过程需要耗费较大的精力。</p>\n<p><strong>基于性别的推荐</strong>: 基于性别进行推荐属于一种基于内容的推荐方法，通过利用user性别信息、item的性别偏好信息进行推荐。该方法的优点是简单、可以在用户没有任何行为时推荐该性别的用户普遍感兴趣的内容；同时缺点也比较明显：同一性别的用户，其兴趣也可会有较大不同，因此该方法无法做到千人千面。</p>\n<h1 id=\"推荐方案设计\"><a href=\"#推荐方案设计\" class=\"headerlink\" title=\"推荐方案设计\"></a>推荐方案设计</h1><p>   整个推荐方案包括三个步骤：user性别信息获取、item性别偏好计算、基于性别的item推荐。</p>\n<h2 id=\"user性别信息获取\"><a href=\"#user性别信息获取\" class=\"headerlink\" title=\"user性别信息获取\"></a>user性别信息获取</h2><p> 用户性别信息主要来自用户注册信息。由于用户注册时，性别可能会填写错误，或者不填写，此时可通过其它方式多用户的性别进行校准和填充。如根据大多数用户的信息，计算item的性别偏好信息。根据该用户在各个item上的行为表现，计算该用户的性别画像。</p>\n<h2 id=\"item性别偏好计算\"><a href=\"#item性别偏好计算\" class=\"headerlink\" title=\"item性别偏好计算\"></a>item性别偏好计算</h2><p><strong>属性计算</strong></p>\n<p>基于有性别信息的用户，对item的消费行为，计算item的性别偏好。</p>\n<p>  $P(C|I_j)=\\frac{\\sum_iR_{ij}U_i(C)}{\\sum_iR_{ij}}（式1）$</p>\n<p>其中$P(C|I_j)$表示第j个item对应属性C的偏好得分。</p>\n<p>$U_i(C)$表示第i个用户对应属性C的概率值，可以根据用户注册信息、用户行为信息挖掘等方式得到。</p>\n<p>$R_{ij}$表示第i个用户对第j个item的喜欢程度，可对用户在item上的各种行（如点击、点赞、评论、发图、收藏等）分别赋予不同的权值，然后根据用户的行为类型、行为频次等进行加权得到。</p>\n<p><strong>属性平滑</strong></p>\n<p>当平台上用户的数量和活跃度比较接近时，直接使用公式1是没有问题的。但是当不同性别的用户数量相差较大时，根据公式1计算出的item偏好会明显倾向于数量较多的那个性别。例如，平台上的男性用户数量：女性用户数量=5:1， 我们只考虑点击行为，当某个item男女点击比例为5:1时，我们并不能认为该item更容易被男性用户喜欢，而是认为该item对男女的吸引程度相同；男女点击比例为1:1时，该item对女性的吸引程度是更大的。</p>\n<p>因此，需要对item的性别偏好进行平滑,方法如下：</p>\n<ul>\n<li><p>平滑系数计算</p>\n<p>$SMOOTH(C)=\\frac{\\sum_i\\sum_jR_{ij}U_i(C)}{\\sum_i\\sum_{j}R_{ij}}（式2)$</p>\n</li>\n<li><p>对公式1进行平滑</p>\n<p>$P_{smooth}(C|I_j)=\\frac{P(C|I_j)}{SMOOTH(C)}（式3)$</p>\n</li>\n</ul>\n<p><strong>加入其它因素</strong></p>\n<p>  通过公式3得到的属性计算结果修正了平台不同属性用户数量不同带来的干扰，但是依然有些可以改进的地方。比如对于两个不同的item, 其男女偏好得分是非常接近，同样只考虑点击行为。其中一个item A被用户点击的次数为10万、点击的用户数8万等明显多于item B（点击次数100， 点击人数50）, 那么我们可以认为item A的得分是更加可靠的。因此，可考虑加入点击次数等相关因素。</p>\n<p>  $P(other|I_j) = \\frac{\\sum_iR_{ij}}{\\sum_i\\sum_jR_{ij}} (式4)$</p>\n<p>  由于我们的目的主要是找出对每个性别更感兴趣的item, 因此，当$P_{smooth}(C|I_j)$大于0.5时，利用如下公式进一步调整分数。</p>\n<p>  $P_{opt}(C|I_j)=\\alpha P_{smooth}(C|I_j) + (1-\\alpha)P(other|I_j) （式5)$  </p>\n<p>  通过公式5，综合考虑了属性偏好及热度因素。对分数差别较小的item，如果都是偏向于属性C（$P_{smooth}(C|I_j)$大于0.5），可以根据热度等相关因素进行排序;而对于分数差别比较大的item, 则依然可以维持原来的偏好进行顺序。</p>\n<p><strong>归一化</strong></p>\n<p>  对上述计算结果进行归一化，如下所示：</p>\n<p>  $P_{norm}(C|I_j)=\\frac{P_{opt}(C|I_j)}{\\sum_CP_{opt}(C|I_j)} （式6)$</p>\n<h2 id=\"基于性别的item推荐\"><a href=\"#基于性别的item推荐\" class=\"headerlink\" title=\"基于性别的item推荐\"></a>基于性别的item推荐</h2><p>完成计算item的性别偏好信息之后，可进一步用于解决推荐时遇到的用户冷启动问题。具体方法如下：</p>\n<ul>\n<li><p>在线用户筛选及性别获取</p>\n<p>当用户到来时，如果用户具有较多行为，可根据协同过滤相关方法进行推荐（如als）。否则获取用户注册的性别，或者通过其它方式矫正和填充后的性别信息。</p>\n</li>\n<li><p>基于性别信息获取推荐结果</p>\n<p>根据性别信息，选择相关性别偏好得分最高的k个item进行推荐。为实现推荐的多样性，提升用户体验，可以对item进行一定的分类，对于每个类别，分别取前top k个item构成多个队列，每次用户请求时，根据业务需求从每个队列中尽量均匀地选取item进行推荐。</p>\n</li>\n</ul>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><ul>\n<li><p>离线实验</p>\n<p>  对in平台图片进行实验，男女的性别偏好有着较大的区别。实验结果显示，男性最喜欢的图片的是汽车、美女、运动、建筑等，而女性则更喜欢美妆、动漫、美食、宠物等类别的图片。</p>\n</li>\n<li><p>在线实验</p>\n<p>  对于新增用户进行实验，分为实验组和对照组，实验组采用基于性别的推荐方案，对照组保持着之前的逻辑不变（基于热度的推荐）。分别评估实验组和对照组用户点击、平均下滑次数。实验结果显示，<strong>男性平均点击次数提升20.8%,平均下滑次数提升19%；女性平均点击次数提升1.1%，平均下滑次数提升1.4%。</strong>女性用户效果提升不明显，其主要原因在于in平台的主要用户是女性，根据热度进行排序基本反映了女性用户对图片的喜欢程度。对男性用户效果明显，主要原因是对照组根据热度推荐，更多推荐的是女性用户喜欢的图片，而实验组基于性别的推荐，则利用热度和性别偏好两方面信息，推荐了男性用户更喜欢的图片。</p>\n</li>\n</ul>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>【1】HT Cheng, L Koc, J Harmsen, T Shaked, “Wide &amp; Deep Learning for Recommender Systems”, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, 2016.09, pp.7-10</p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n<h1 id=\"基于性别的推荐\"><a href=\"#基于性别的推荐\" class=\"headerlink\" title=\"基于性别的推荐\"></a>基于性别的推荐</h1><p><strong>基于内容的推荐</strong>:基于协同过滤掉方法，存在非常明显的缺点：无法解决冷启动问题（包括user冷启动和item冷启动），当用户没有行为时无法进行推荐，或者行为非常稀疏的时候，虽然可以产生推荐列表，但会存在过度泛化问题【1】。相对于协同过滤，基于内容的推荐的方法, 能够较好地应对冷启动问题。通过提取user和item相关内容信息，并根据所提取的信息进行推荐。但是，user和item的内容信息的提取往往需要比较专业的知识，其提取过程需要耗费较大的精力。</p>\n<p><strong>基于性别的推荐</strong>: 基于性别进行推荐属于一种基于内容的推荐方法，通过利用user性别信息、item的性别偏好信息进行推荐。该方法的优点是简单、可以在用户没有任何行为时推荐该性别的用户普遍感兴趣的内容；同时缺点也比较明显：同一性别的用户，其兴趣也可会有较大不同，因此该方法无法做到千人千面。</p>\n<h1 id=\"推荐方案设计\"><a href=\"#推荐方案设计\" class=\"headerlink\" title=\"推荐方案设计\"></a>推荐方案设计</h1><p>   整个推荐方案包括三个步骤：user性别信息获取、item性别偏好计算、基于性别的item推荐。</p>\n<h2 id=\"user性别信息获取\"><a href=\"#user性别信息获取\" class=\"headerlink\" title=\"user性别信息获取\"></a>user性别信息获取</h2><p> 用户性别信息主要来自用户注册信息。由于用户注册时，性别可能会填写错误，或者不填写，此时可通过其它方式多用户的性别进行校准和填充。如根据大多数用户的信息，计算item的性别偏好信息。根据该用户在各个item上的行为表现，计算该用户的性别画像。</p>\n<h2 id=\"item性别偏好计算\"><a href=\"#item性别偏好计算\" class=\"headerlink\" title=\"item性别偏好计算\"></a>item性别偏好计算</h2><p><strong>属性计算</strong></p>\n<p>基于有性别信息的用户，对item的消费行为，计算item的性别偏好。</p>\n<p>  $P(C|I_j)=\\frac{\\sum_iR_{ij}U_i(C)}{\\sum_iR_{ij}}（式1）$</p>\n<p>其中$P(C|I_j)$表示第j个item对应属性C的偏好得分。</p>\n<p>$U_i(C)$表示第i个用户对应属性C的概率值，可以根据用户注册信息、用户行为信息挖掘等方式得到。</p>\n<p>$R_{ij}$表示第i个用户对第j个item的喜欢程度，可对用户在item上的各种行（如点击、点赞、评论、发图、收藏等）分别赋予不同的权值，然后根据用户的行为类型、行为频次等进行加权得到。</p>\n<p><strong>属性平滑</strong></p>\n<p>当平台上用户的数量和活跃度比较接近时，直接使用公式1是没有问题的。但是当不同性别的用户数量相差较大时，根据公式1计算出的item偏好会明显倾向于数量较多的那个性别。例如，平台上的男性用户数量：女性用户数量=5:1， 我们只考虑点击行为，当某个item男女点击比例为5:1时，我们并不能认为该item更容易被男性用户喜欢，而是认为该item对男女的吸引程度相同；男女点击比例为1:1时，该item对女性的吸引程度是更大的。</p>\n<p>因此，需要对item的性别偏好进行平滑,方法如下：</p>\n<ul>\n<li><p>平滑系数计算</p>\n<p>$SMOOTH(C)=\\frac{\\sum_i\\sum_jR_{ij}U_i(C)}{\\sum_i\\sum_{j}R_{ij}}（式2)$</p>\n</li>\n<li><p>对公式1进行平滑</p>\n<p>$P_{smooth}(C|I_j)=\\frac{P(C|I_j)}{SMOOTH(C)}（式3)$</p>\n</li>\n</ul>\n<p><strong>加入其它因素</strong></p>\n<p>  通过公式3得到的属性计算结果修正了平台不同属性用户数量不同带来的干扰，但是依然有些可以改进的地方。比如对于两个不同的item, 其男女偏好得分是非常接近，同样只考虑点击行为。其中一个item A被用户点击的次数为10万、点击的用户数8万等明显多于item B（点击次数100， 点击人数50）, 那么我们可以认为item A的得分是更加可靠的。因此，可考虑加入点击次数等相关因素。</p>\n<p>  $P(other|I_j) = \\frac{\\sum_iR_{ij}}{\\sum_i\\sum_jR_{ij}} (式4)$</p>\n<p>  由于我们的目的主要是找出对每个性别更感兴趣的item, 因此，当$P_{smooth}(C|I_j)$大于0.5时，利用如下公式进一步调整分数。</p>\n<p>  $P_{opt}(C|I_j)=\\alpha P_{smooth}(C|I_j) + (1-\\alpha)P(other|I_j) （式5)$  </p>\n<p>  通过公式5，综合考虑了属性偏好及热度因素。对分数差别较小的item，如果都是偏向于属性C（$P_{smooth}(C|I_j)$大于0.5），可以根据热度等相关因素进行排序;而对于分数差别比较大的item, 则依然可以维持原来的偏好进行顺序。</p>\n<p><strong>归一化</strong></p>\n<p>  对上述计算结果进行归一化，如下所示：</p>\n<p>  $P_{norm}(C|I_j)=\\frac{P_{opt}(C|I_j)}{\\sum_CP_{opt}(C|I_j)} （式6)$</p>\n<h2 id=\"基于性别的item推荐\"><a href=\"#基于性别的item推荐\" class=\"headerlink\" title=\"基于性别的item推荐\"></a>基于性别的item推荐</h2><p>完成计算item的性别偏好信息之后，可进一步用于解决推荐时遇到的用户冷启动问题。具体方法如下：</p>\n<ul>\n<li><p>在线用户筛选及性别获取</p>\n<p>当用户到来时，如果用户具有较多行为，可根据协同过滤相关方法进行推荐（如als）。否则获取用户注册的性别，或者通过其它方式矫正和填充后的性别信息。</p>\n</li>\n<li><p>基于性别信息获取推荐结果</p>\n<p>根据性别信息，选择相关性别偏好得分最高的k个item进行推荐。为实现推荐的多样性，提升用户体验，可以对item进行一定的分类，对于每个类别，分别取前top k个item构成多个队列，每次用户请求时，根据业务需求从每个队列中尽量均匀地选取item进行推荐。</p>\n</li>\n</ul>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><ul>\n<li><p>离线实验</p>\n<p>  对in平台图片进行实验，男女的性别偏好有着较大的区别。实验结果显示，男性最喜欢的图片的是汽车、美女、运动、建筑等，而女性则更喜欢美妆、动漫、美食、宠物等类别的图片。</p>\n</li>\n<li><p>在线实验</p>\n<p>  对于新增用户进行实验，分为实验组和对照组，实验组采用基于性别的推荐方案，对照组保持着之前的逻辑不变（基于热度的推荐）。分别评估实验组和对照组用户点击、平均下滑次数。实验结果显示，<strong>男性平均点击次数提升20.8%,平均下滑次数提升19%；女性平均点击次数提升1.1%，平均下滑次数提升1.4%。</strong>女性用户效果提升不明显，其主要原因在于in平台的主要用户是女性，根据热度进行排序基本反映了女性用户对图片的喜欢程度。对男性用户效果明显，主要原因是对照组根据热度推荐，更多推荐的是女性用户喜欢的图片，而实验组基于性别的推荐，则利用热度和性别偏好两方面信息，推荐了男性用户更喜欢的图片。</p>\n</li>\n</ul>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>【1】HT Cheng, L Koc, J Harmsen, T Shaked, “Wide &amp; Deep Learning for Recommender Systems”, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, 2016.09, pp.7-10</p>\n"},{"title":"ALS推荐算法学习与实践","date":"2017-02-21T16:00:00.000Z","toc":true,"description":"als算法学习与实践","mathjax":true,"_content":"\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\nALS（alternating least squares）是一种基础的推荐算法，相对于普通的协同过滤等方法，它不仅能通过降维增加模型的泛化能力，也方便加入其他建模因素（如数据偏差、时间、隐反馈等），大大提升了模型的灵活性。正因为此，ALS算法在Netflix推荐大赛中脱颖而出，在我们具体的工程实践中，也具有非常不错的表现。接下来，从如下几个方面和大家一起学习：ALS算法模型、spark ALS源码理解， ALS推荐实践。如描述有误，欢迎大家指正。\n\n# ALS算法模型\n## 为什么要用ALS模型\n 相对于其他模型，ALS模型优势如下：\n \n* **相对于基于内容的推荐**，ALS属于协同过滤大家族【1】【12】（也有人认为ALS 基于矩阵分解技术，不属于协同过滤范畴【2】），**直接跟进用户行为信息进行建模，不需要获取user和item的内容信息**（很多情况下这些内容信息并不是很好获取，但是相对基于内容的推荐，ALS存在冷启动问题）\n\n* **相对于传统的协同过滤推荐方法（user based、item based）**， ALS算法属于factor model, 通过将数据从原始空间映射到更低维度空间，**去除噪声信息，利用更主要的语义信息对问题建模，能获得更好的推荐效果**。\n\n* **相对于svd分解模型而言**， 两种模型都属于 factor model, 但**svd分解模型更倾向于解决矩阵元素没有缺失的情况， 而通过一定的方式去填充矩阵不仅需要额外的填充成本，填充本身可能影响了数据的真实性**。因此，直接对已知元素进行建模，是一种不错的思路。如【1，3-6】，直接对rating矩阵已知元素$r\\_{ui}$进行建模:\n\n<center>\n$\\sum\\_{u,i\\in\\mathbb K} (r\\_{ui} - \np\\_u^Tq\\_i)^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$ （1）\n</center>\n\n* 针对所建模型1可以用SGD或ALS 两种算法求解。其中**sgd方法相对比较简单，但是当我们要建模的矩阵中已知元素较多时（如隐反馈），采用sgd在每次迭代都要求解所有元素，其时间复杂度是非常大的**。ALS算法在求解某个user （或item）向量时，不依赖其他任何user（item）向量，这个性质使得**ALS算法在每次迭代过程中方便并行化求解，在解决大规模矩阵分解问题时非常具有优势**。 \n\n## ALS模型有什么缺点\n 相对于其它推荐算法，ALS模型具有非常明显的优势：不需要对user和item信息进行建模，能够更加灵活地对各种因素建模，方便大规模并行计算。但ALS模型在如下几方面，又有自己的局限性：\n \n * 冷启动问题\n   \n   包括user的冷启动和item的冷启动。由于rating矩阵的构建，依赖user的显式和隐式反馈信息，对于新的user和item，或者没有相关行为的user或item, 导致无法构建rating矩阵，或者rating矩阵构建不合理。\n   \n   基于内容的推荐能较好地解决冷启动相关问题。如【8】在解决用户的冷启动问题时，首先根据用户之间社交的亲密度，对用户进行聚类，利用相同群体的用户画像来建模自身兴趣。同时，对于item的冷启动问题，可利用item本身对一些关键词、类目、内容等相关信息进行建模。【9】为了提高用户兴趣的准确率和覆盖率，在对用户兴趣建模对时候，将用户兴趣进行更具体的分类（如消费兴趣、生产兴趣、具体的每个行为兴趣等），并针对具体的业务，采用线性回归的方法对各种兴趣利用线性回归的方式进行加权求和，提升用户兴趣准确率和覆盖率。\n \n * 用户临时兴趣\n \n   用户的兴趣是在不断变换的。对于相对较稳定的兴趣，ALS算法可以通过引入时间因素进行建模，如公式4。 但对于临时的兴趣变换，ALS算法是无法捕获的。\n   \n   **一种简单且有效的方法**\n   \n   将item划分为多个类别，每个类别对应一种兴趣。用户每次点击某个类别的item之后，认为该用户存在一种临时兴趣，通过动态增加相应类别的比例的item，迎合用户当前的消费需求。该方法的难点在于如何调整比例，才能让用户感到有很多自己喜欢的item, 同时又不会让用户感觉内容的单调。\n   \n   **淘宝的一些实践**\n   \n   为了有效获取用户的即时兴趣，给用户推荐最合适的产品，淘宝进行了比较多的实践【10】，分别如下所述。\n   * GBDT+FTRL模型\n   \n     由于GBDT模型比较擅长挖具有区分度的特征，其使用GBDT模型进行特征挖掘，将得到的特征输送给FTRL进行在线学习。输送给GBDT的特征包括两部分：一部分用户基础行为的次数、CTR等；另一部分是来自match粗选阶段的的特征，该部分特征来自不同的粗选模型输出.\n   \n   * Wide & Deep Learning模型\n   \n     借鉴google论文思想【11】，利用wide模型 + deep模型 + LR，其中wide子结构通过特征交叉学习特征间的共现，deep子结构则输入具有泛化能力的离散特征和连续特征，wide模型和deep模型学习到的结果，再利用LR模型预测相应的得分。\n     \n   * Adaptive-Online-Learning\n   \n     保留每一时刻学习到的模型，根据业务指标，得到每个模型等权重信息，融合出最优的结果。该方法能够比较好地综合利用用户长期喝短期兴趣。\n     \n   * Reinforcement Learning\n     \n     该方法思想是通过定义每个步骤的奖励，当用户每次到来的时候，根据用户的累积奖励值，进行个性化推荐。\n     \n   **腾讯的LSTM实践**\n     \n     为解决音乐的推荐问题，腾讯采用的是LSTM深度学习方法【12】，将用户听的歌曲序列，抽取特征输入到LSTM网络进行训练。为防止有些用户对应的歌曲序列较短问题，其对这些数据的训练采用特殊处理，相关数据缺失的序列不进行状态更新。同时，为加快训练速度，将每次权值的训练过程通过矩阵的方式实现并发计算。另外，为降低soft max过程时间复杂度，采用Hierarchical softmax过程替代普通的softmax。\n\n  \n## ALS模型是什么\n### 基本概念\n\nALS模型属于隐语义模型，通过对用户行为矩阵R进行矩阵分解，得到user factor向量矩阵P、item factor向量矩阵Q. \n\n$R = P^T Q$ 。其中R、$P^T$、$Q^T$矩阵的定义如表1-表3所示。\n    \n潜在语义空间对应的各个factor代表不同的属性信息，user向量描述了user对各种属性的喜好程度，item向量描述了item所具备的各种属性强度，二者在潜在语义空间的相似度描述了user对item的喜好程度,在进行推荐时，根据该喜好程度计算推荐结果。\n<center>表1: rating矩阵R</center>\n\n |item1|item2|item3|item4\n-|-|-|-|-|\nuser1|$r\\_{11}$|$r\\_{12}$| $r\\_{13}$| $r\\_{14}$\nuser2|$r\\_{21}$|$r\\_{22}$| $r\\_{23}$| $r\\_{24}$\nuser3|$r\\_{31}$|$r\\_{32}$| $r\\_{33}$| $r\\_{34}$\nuser4|$r\\_{41}$|$r\\_{42}$| $r\\_{43}$| $r\\_{44}$\nuser5|$r\\_{51}$|$r\\_{52}$| $r\\_{53}$| $r\\_{54}$\n \n\n<center>表2：user矩阵$P^T$</center>\n\n | factor1 | factor2 | factor3 \n-|-|-|-|\nuser1|$p\\_{11}$|$p\\_{12}$| $p\\_{13}$\nuser2|$p\\_{21}$|$p\\_{22}$| $p\\_{23}$\nuser3|$p\\_{31}$|$p\\_{32}$| $p\\_{33}$\nuser4|$p\\_{41}$|$p\\_{42}$| $p\\_{43}$\nuser5|$p\\_{51}$|$p\\_{52}$| $p\\_{53}$\n\n\n<center>表3:item矩阵$Q^T$</center>\n\n | factor1 | factor2 | factor3 \n-|-|-|-|\nitem1|$q\\_{11}$|$q\\_{12}$| $q\\_{13}$\nitem2|$q\\_{21}$|$q\\_{22}$| $q\\_{23}$\nitem3|$q\\_{31}$|$q\\_{32}$| $q\\_{33}$\nitem4|$q\\_{41}$|$q\\_{42}$| $q\\_{43}$\n\n\n### 目标函数\n  \n$MIN\\_{PQ} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$  (2)\n\n其中${(r\\_{ui} - p\\_u^Tq\\_i）}^2$ 目的在于最小化分解误差，$\\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$ 为正则项。\n\n### 目标函数求解\n\n由于目标函数中$p\\_u, q_i$都是未知变量，该问题是非凸的。当我们固定其中一个变量，解另外一个变量时，问题则变成凸问题，这是ALS求解的主要思想。在实际求解过程中分为如下几个步骤：\n\n1. 随机初始化所有的变量$p\\_u, q\\_i$。\n  \n2. 固定所有的$q\\_i$变量，求得$q\\_i$变量为当前值时$p\\_u$的最优值。\n  \n3. 固定所有的$p\\_u$变量，求得$p\\_u$变量为当前值时$q\\_i$的最优值。\n  \n4. 如果满足终止条件，则终止。否则，迭代执行2，3两步。\n\n通过不断执行步骤2和步骤3，使得误差越来越小，直到收敛或达到指定次数而终止。通过可导函数性质我们知道，当对变量求导结果等于0当时候，函数可以取得极值。具体到公式2，固定一个变量，对另一变量求导结果等于0时，可以达到极小值。\n \n我们令$L = \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - p\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n固定所有$q\\_i$, 对$p\\_u$求导\n  \n$-\\frac{\\alpha L}{2\\alpha p\\_{uk}} = \\sum\\_{i} {q\\_{ik}(r\\_{ui} - p\\_u^Tq\\_i）} - \\lambda p\\_{uk} = 0$\n\n=> $\\sum\\_{i} {q\\_{i}(r\\_{ui} - p\\_u^Tq\\_i）} - \\lambda p\\_{u} = 0$\n\n=> $(\\sum\\_{i} {q\\_i q\\_i^T} + \\lambda E) p\\_u = \\sum\\_{i}q\\_i r\\_{ui}$\n\n=> $p\\_u = (\\sum\\_{i} {q\\_i q\\_i^T} + \\lambda E)^{-1}\\sum\\_{i}q\\_i r\\_{ui}$\n\n=> $ p\\_u = (Q\\_{u,i\\in\\mathbb K} Q\\_{u,i\\in\\mathbb K}^T + \\lambda E)^{-1}Q\\_{u,i\\in\\mathbb K}R\\_{u,i\\in\\mathbb K}^T$\n\n其中，$q\\_{u,i\\in\\mathbb K}$ 表示和user $u$有行为关联的item对应的向量矩阵，$r\\_{u,i\\in\\mathbb K}^T$表示和user $u$有行为关联的item对应rating元素构成的向量的转置。\n\n**更加灵活的ALS建模**\n\n相对于传统的协同协同过滤方法，ALS能更好的考虑其他因素，如数据偏差、时间等\n\n1. 引入数据偏差\n    \n    user偏差：不同的用户，可能具有不同的评分标准。如用户在给item打分时，有的用户可能可能更倾向于给所有item打高分， 而有的挑剔用户会给所有item打分偏低\n    \n    item偏差：有的热门item可能所有用户都会倾向于打高分，而有的item可能本身大多数人会倾向于打低分\n    \n    考虑use和item偏差的ALS建模：\n    $MIN\\_{PQB} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - u - b\\_u - b\\_i-\np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i+b\\_u^2+b\\_i^2)$  (3)\n\n2. 引入时间因素\n    \n    用户偏好、rating矩阵，都可能随时间变化，item对应的属性不随时间变化，因此可进行如下建模\n$MIN\\_{PQB} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui}（t） - u - b\\_u(t) - b\\_i(t)-\np\\_u(t)^Tq\\_i）}^2 + \\lambda(p\\_u(t)^Tp\\_u(t)+q\\_i^Tq\\_i+b\\_u(t)^2+b\\_i(t)^2)$  (4)\n    \n3. 引入隐反馈数据因素\n    \n    很多时候，并没有用户对item明确的打分数据，此时可通过搜集用户隐反馈数据（浏览、点击、点赞等），进行隐反馈建模。有一点需要注意，此时不只是对$r\\_{ui}$大于0对用户行为建模，而是所有$r\\_{ui}$元素建模。模型如公式5所示：\n    \n    $MIN\\_{PQB} \\sum\\_{u,i} {c\\_{ui}(p\\_{ui} - u - b\\_u - b\\_i-\np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i+b\\_u^2+b\\_i^2)$  (5)\n \n    $p\\_{ui}$ 表示user u是否有相关行为表示喜欢item i, $c\\_{ui}$描述user u 对item i的喜欢程度，其定义如公式6和公式7所示\n \n    $\np\\_{ui} = \n\\begin{cases} \n1,  & r\\_{ui}>0\\\\\\\\\n0,  & r\\_{ui}=0\n\\end{cases}\n$（6）\n    \n    $c\\_{ui} = 1 + \\alpha r\\_{ui}$（7）\n\n\n\n# spark ALS源码理解\n    \n为加深对ALS算法的理解，该部分主要分析spark mllib中ALS源码的实现，大体上分为2部分：ALS模型训练、ALS模型推荐\n\n## ALS 模型训练\n\n### ALS 伴生类\n    \nALS 伴生对象提供外部调用 ALS模型训练的入口。通过传入相关参数， 返回训练好的模型对象MatrixFactorizationModel。\n\n\n```scala\nobject ALS {\n  def train(\n      ratings: RDD[Rating], //rating元素 （user, item, rate）\n      rank: Int, //隐语义个数\n      iterations: Int, //迭代次数\n      lambda: Double, //正则惩罚项\n      blocks: Int, //数据block个数\n      seed: Long //随机数种子\n    ): MatrixFactorizationModel = {\n    new ALS(blocks, blocks, rank, iterations, lambda, fALSe, 1.0, seed).run(ratings)\n  }\n\n  def trainImplicit(\n      ratings: RDD[Rating], // rating元素 （user, item, rate）\n      rank: Int, //隐语义个数\n      iterations: Int, //迭代次数\n      lambda: Double, //正则惩罚项\n      blocks: Int, //数据block个数\n      alpha: Double //计算$c_ui$时用的alpha参数\n    ): MatrixFactorizationModel = {\n    new ALS(blocks, blocks, rank, iterations, lambda, true, alpha).run(ratings)\n  }\n   //另外还有一些其他接口，因最终都通过调用上面2个函数，此处将其省略\n}\n```\n\n### ALS 私有类\n    \n定义了ALS类对应的各个参数，以及各个参数的设定方法。并定义了run方法供伴随类进行调用，该方法返回训练结果MatrixFactorizationModel给ALS伴随类。\n\n    \n```scala\nclass ALS private (\n    private var numUserBlocks: Int, //用户数据block个数\n    private var numProductBlocks: Int, //item数据block个数\n    private var rank: Int, //隐语义个数\n    private var iterations: Int, //迭代次数\n    private var lambda: Double, //正则惩罚项\n    private var implicitPrefs: Boolean, //是否使用隐反馈模型\n    private var alpha: Double, //计算$c_ui$时用的alpha参数\n    private var seed: Long = System.nanoTime() //随机数种子,默认为当前时间戳\n  ) extends Serializable with Logging {\n\n  //设置block个数\n  def setBlocks(numBlocks: Int): this.type = {\n    this.numUserBlocks = numBlocks\n    this.numProductBlocks = numBlocks\n    this\n  }\n  \n  // 另外对其他参数变量也有相关函数实现，因基本都是赋值操作，此处将其省略\n  \n  //run方法，通过输入rating数据，完成训练兵返回结果MatrixFactorizationModel\n  def run(ratings: RDD[Rating]): MatrixFactorizationModel = {\n    require(!ratings.isEmpty(), s\"No ratings available from $ratings\")\n\n    val sc = ratings.context\n    //设置user block个数\n    val numUserBlocks = if (this.numUserBlocks == -1) {\n      math.max(sc.defaultParallelism, ratings.partitions.length / 2)\n    } else {\n      this.numUserBlocks\n    }\n    //设置item block个数\n    val numProductBlocks = if (this.numProductBlocks == -1) {\n      math.max(sc.defaultParallelism, ratings.partitions.length / 2)\n    } else {\n      this.numProductBlocks\n    }\n    //调用NewALS.train方法完成矩阵分解，生成user factor和item factor向量,该方法是整个ALS算法的核心实现\n    val (floatUserFactors, floatProdFactors) = NewALS.train[Int](\n      ratings = ratings.map(r => NewALS.Rating(r.user, r.product, r.rating.toFloat)),\n      rank = rank,\n      numUserBlocks = numUserBlocks,\n      numItemBlocks = numProductBlocks,\n      maxIter = iterations,\n      regParam = lambda,\n      implicitPrefs = implicitPrefs,\n      alpha = alpha,\n      nonnegative = nonnegative,\n      intermediateRDDStorageLevel = intermediateRDDStorageLevel,\n      finalRDDStorageLevel = StorageLevel.NONE,\n      checkpointInterval = checkpointInterval,\n      seed = seed)\n   \n    val userFactors = floatUserFactors\n      .mapValues(_.map(_.toDouble))\n      .setName(\"users\")\n      .persist(finalRDDStorageLevel)\n    val prodFactors = floatProdFactors\n      .mapValues(_.map(_.toDouble))\n      .setName(\"products\")\n      .persist(finalRDDStorageLevel)\n    if (finalRDDStorageLevel != StorageLevel.NONE) {\n      userFactors.count()\n      prodFactors.count()\n    }\n    //生成和返回ALS模型 MatrixFactorizationModel\n    new MatrixFactorizationModel(rank, userFactors, prodFactors)\n  }\n}\n```\n\n\n### NewALS.train方法\n    \n被ALS私有类的run方法调用，用于计算user factor和item factor向量。\n\n```scala\ndef train[ID: ClassTag]( // scalastyle:ignore\n      ratings: RDD[Rating[ID]],\n      rank: Int = 10,\n      numUserBlocks: Int = 10,\n      numItemBlocks: Int = 10,\n      maxIter: Int = 10,\n      regParam: Double = 1.0,\n      implicitPrefs: Boolean = fALSe,\n      alpha: Double = 1.0,\n      nonnegative: Boolean = fALSe,\n      intermediateRDDStorageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK,\n      finalRDDStorageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK,\n      checkpointInterval: Int = 10,\n      seed: Long = 0L)(\n      implicit ord: Ordering[ID]): (RDD[(ID, Array[Float])], RDD[(ID, Array[Float])]) = {\n    require(!ratings.isEmpty(), s\"No ratings available from $ratings\")\n    require(intermediateRDDStorageLevel != StorageLevel.NONE,\n      \"ALS is not designed to run without persisting intermediate RDDs.\")\n    val sc = ratings.sparkContext\n    //根据block个数，构建哈稀器。\n    val userPart = new ALSPartitioner(numUserBlocks)\n    val itemPart = new ALSPartitioner(numItemBlocks)\n    //构建索引编码器，根据block编号和block内索引进行编码，同时可将编码后结果快速解码为block编号和block内索引号。具体实现是通过block个数，确定block编码需要的二进制位数，以及block内索引位数，通过这些位数利用逻辑操作即可实现编码和解码\n\n    val userLocalIndexEncoder = new LocalIndexEncoder(userPart.numPartitions)\n    val itemLocalIndexEncoder = new LocalIndexEncoder(itemPart.numPartitions)\n    //构建求解器\n    val solver = if (nonnegative) new NNLSSolver else new CholeskySolver\n    //对rating矩阵进行分块，得到((user_blockID, item_blockID),rating(user, item, rating))\n\n    val blockRatings = partitionRatings(ratings, userPart, itemPart)\n      .persist(intermediateRDDStorageLevel)\n    //构建user inblock和outblock数据，inblock数据记录每个user对应的所有item的地址，及对应rating信息。 outblock记录当前block的哪些user数据会被哪些block用上\n    val (userInBlocks, userOutBlocks) =\n      makeBlocks(\"user\", blockRatings, userPart, itemPart, intermediateRDDStorageLevel)\n    // materialize blockRatings and user blocks\n    userOutBlocks.count()\n    //交换blockrating中的user, item数据，用于构造item的inblcok和outblock信息\n    val swappedBlockRatings = blockRatings.map {\n      case ((userBlockId, itemBlockId), RatingBlock(userIds, itemIds, localRatings)) =>\n        ((itemBlockId, userBlockId), RatingBlock(itemIds, userIds, localRatings))\n    }\n    val (itemInBlocks, itemOutBlocks) =\n      makeBlocks(\"item\", swappedBlockRatings, itemPart, userPart, intermediateRDDStorageLevel)\n    // materialize item blocks\n    itemOutBlocks.count()\n    val seedGen = new XORShiftRandom(seed)\n    //随机初始化user factor和item factor\n    var userFactors = initialize(userInBlocks, rank, seedGen.nextLong())\n    var itemFactors = initialize(itemInBlocks, rank, seedGen.nextLong())\n    var previousCheckpointFile: Option[String] = None\n    val shouldCheckpoint: Int => Boolean = (iter) =>\n      sc.checkpointDir.isDefined && checkpointInterval != -1 && (iter % checkpointInterval == 0)\n    val deletePreviousCheckpointFile: () => Unit = () =>\n      previousCheckpointFile.foreach { file =>\n        try {\n          val checkpointFile = new Path(file)\n          checkpointFile.getFileSystem(sc.hadoopConfiguration).delete(checkpointFile, true)\n        } catch {\n          case e: IOException =>\n            logWarning(s\"Cannot delete checkpoint file $file:\", e)\n        }\n      }\n    //针对隐反馈，迭代求解\n    if (implicitPrefs) {\n      for (iter <- 1 to maxIter) {  //迭代总次数maxIter\n        userFactors.setName(s\"userFactors-$iter\").persist(intermediateRDDStorageLevel)\n        val previousItemFactors = itemFactors\n        //固定user factor，优化item factor\n        itemFactors = computeFactors(userFactors, userOutBlocks, itemInBlocks, rank, regParam,\n          userLocalIndexEncoder, implicitPrefs, alpha, solver)\n        previousItemFactors.unpersist()\n        itemFactors.setName(s\"itemFactors-$iter\").persist(intermediateRDDStorageLevel)\n        // TODO: Generalize PeriodicGraphCheckpointer and use it here.\n        val deps = itemFactors.dependencies\n        if (shouldCheckpoint(iter)) {\n          itemFactors.checkpoint() // itemFactors gets materialized in computeFactors\n        }\n        val previousUserFactors = userFactors\n        //根据item factore, 优化user factor\n        userFactors = computeFactors(itemFactors, itemOutBlocks, userInBlocks, rank, regParam,\n          itemLocalIndexEncoder, implicitPrefs, alpha, solver)\n        if (shouldCheckpoint(iter)) {\n          ALS.cleanShuffleDependencies(sc, deps)\n          deletePreviousCheckpointFile()\n          previousCheckpointFile = itemFactors.getCheckpointFile\n        }\n        previousUserFactors.unpersist()\n      }\n    } else { //针对显示反馈，迭代求解\n      for (iter <- 0 until maxIter) { //迭代总次数maxIter\n        //固定user factor，优化item factor\n        itemFactors = computeFactors(userFactors, userOutBlocks, itemInBlocks, rank, regParam,\n          userLocalIndexEncoder, solver = solver)\n        if (shouldCheckpoint(iter)) {\n          val deps = itemFactors.dependencies\n          itemFactors.checkpoint()\n          itemFactors.count() // checkpoint item factors and cut lineage\n          ALS.cleanShuffleDependencies(sc, deps)\n          deletePreviousCheckpointFile()\n          previousCheckpointFile = itemFactors.getCheckpointFile\n        }\n        //根据item factore, 优化user factor\n        userFactors = computeFactors(itemFactors, itemOutBlocks, userInBlocks, rank, regParam,\n          itemLocalIndexEncoder, solver = solver)\n      }\n    }\n    //将user id 和 factor拼接在一起\n    val userIdAndFactors = userInBlocks\n      .mapValues(_.srcIds)\n      .join(userFactors)\n      .mapPartitions({ items =>\n        items.flatMap { case (_, (ids, factors)) =>\n          ids.view.zip(factors)\n        }\n      // Preserve the partitioning because IDs are consistent with the partitioners in userInBlocks\n      // and userFactors.\n      }, preservesPartitioning = true)\n      .setName(\"userFactors\")\n      .persist(finalRDDStorageLevel)\n    //将item id 和 factor拼接在一起\n    val itemIdAndFactors = itemInBlocks\n      .mapValues(_.srcIds)\n      .join(itemFactors)\n      .mapPartitions({ items =>\n        items.flatMap { case (_, (ids, factors)) =>\n          ids.view.zip(factors)\n        }\n      }, preservesPartitioning = true)\n      .setName(\"itemFactors\")\n      .persist(finalRDDStorageLevel)\n    if (finalRDDStorageLevel != StorageLevel.NONE) {\n      userIdAndFactors.count()\n      itemFactors.unpersist()\n      itemIdAndFactors.count()\n      userInBlocks.unpersist()\n      userOutBlocks.unpersist()\n      itemInBlocks.unpersist()\n      itemOutBlocks.unpersist()\n      blockRatings.unpersist()\n    }\n    //返回user factor和item factor数据\n    (userIdAndFactors, itemIdAndFactors)\n  }\n```\n\n### 构建哈希器 \n  \n   构建哈希器，用于计算user或item id对应的block编号。\n    \n```scala\nclass HashPartitioner(partitions: Int) extends Partitioner {\n  require(partitions >= 0, s\"Number of partitions ($partitions) cannot be negative.\")\n  //block总数\n  def numPartitions: Int = partitions\n  //通过求余计算block 编号\n  def getPartition(key: Any): Int = key match {\n    case null => 0\n    case _ => Utils.nonNegativeMod(key.hashCode, numPartitions)\n  }\n  //判断2个哈希器是否相等\n  override def equALS(other: Any): Boolean = other match {\n    case h: HashPartitioner =>\n      h.numPartitions == numPartitions\n    case _ =>\n      fALSe\n  }\n  override def hashCode: Int = numPartitions\n}\n```\n\n### 构建地址编码解码器\n\n构建地址编码解码器，根据block编号和block内索引对地址进行编码，同时可将编码后地址解码为block编号和block内索引号。具体实现是通过block个数确定block编码需要的二进制位数，以及block内索引位数，通过这些位数利用逻辑操作即可实现地址的编码和解码。\n    \n```scala\nprivate[recommendation] class LocalIndexEncoder(numBlocks: Int) extends Serializable {\n\n    require(numBlocks > 0, s\"numBlocks must be positive but found $numBlocks.\")\n    //block内部索引使用的二进制位数\n    private[this] final val numLocalIndexBits =\n      math.min(java.lang.Integer.numberOfLeadingZeros(numBlocks - 1), 31)\n    private[this] final val localIndexMask = (1 << numLocalIndexBits) - 1\n    \n    //根据block编号和block内索引值，对地址编码\n    def encode(blockId: Int, localIndex: Int): Int = {\n      require(blockId < numBlocks)\n      require((localIndex & ~localIndexMask) == 0)\n      (blockId << numLocalIndexBits) | localIndex\n    }\n\n    //根据编码后地址，得到block编号\n    def blockId(encoded: Int): Int = {\n      encoded >>> numLocalIndexBits\n    }\n\n    //根据编码地址，得到block内部索引\n    def localIndex(encoded: Int): Int = {\n      encoded & localIndexMask\n    }\n  }\n```\n\n### partition rating\n\n格式化rating数据，将rating数据分块，根据user和product的id哈希后的结果，得到对应的块索引。最终返回（src_block_id, dst_block_id）(src_id数组，dst_id数组，rating数组)\n\n    \n```scala\nprivate def partitionRatings[ID: ClassTag](\n      ratings: RDD[Rating[ID]],\n      srcPart: Partitioner,\n      dstPart: Partitioner): RDD[((Int, Int), RatingBlock[ID])] = {\n    //获得总block数\n    val numPartitions = srcPart.numPartitions * dstPart.numPartitions\n    //在rating的每个分区，计算每个rating元素对应的src_block_id和dst_block_id, 并放到对应的块索引中。然后，对所有分区的元素按照块索引进行聚合，并返回聚合结果\n    ratings.mapPartitions { iter =>\n      //生成numPartitions个一维数组，存储对应block的rating记录\n      val builders = Array.fill(numPartitions)(new RatingBlockBuilder[ID])\n      iter.flatMap { r =>\n        val srcBlockId = srcPart.getPartition(r.user) //user block id\n        val dstBlockId = dstPart.getPartition(r.item) //item block id\n        val idx = srcBlockId + srcPart.numPartitions * dstBlockId //数组索引计算\n        //将对应的rating元素放在builders对应元素中\n        val builder = builders(idx) \n        builder.add(r) \n        if (builder.size >= 2048) { // 2048 * (3 * 4) = 24k\n          //如果某个block内数据量较多，直接得到结果\n          builders(idx) = new RatingBlockBuilder\n          Iterator.single(((srcBlockId, dstBlockId), builder.build()))\n        } else {\n          Iterator.empty\n        }\n      } ++ {\n        //对builders数组内元素，计算对应的src_block_id和dst_block_id,并将对应rating数据放在其中\n        builders.view.zipWithIndex.filter(_._1.size > 0).map { case (block, idx) =>\n          val srcBlockId = idx % srcPart.numPartitions\n          val dstBlockId = idx / srcPart.numPartitions\n          ((srcBlockId, dstBlockId), block.build())\n        }\n      }\n    }.groupByKey().mapValues { blocks =>\n      //对不同分区计算出的的rating元素进行聚合\n      val builder = new RatingBlockBuilder[ID]\n      blocks.foreach(builder.merge)\n      builder.build() //value为 （src_id数组，dst_id数组，对应的rating数组）\n    }.setName(\"ratingBlocks\")\n  }\n\n```\n\n### 构造in_block, 和out_block\n  \n在分布式计算中，不同节点的通信是影响程序效率重要原因，通过合理的设计分区，使得不同节点交换数据尽量少，可以有效的提升运行效率。\n     \n由上述章节中对目标函数求解推导，可以得知，每个用户向量的计算依赖于所有和它关联的item向量。如果不做任何优化，则每次优化user向量时，所有user向量的计算，都需要从其他节点得到对应item向量。如果节点A上有多个user和节点B上的某一item关联，则节点B需要向节点A传输多次item向量数据，实际上这是不必要的。优化的思路是，通过合理的分区，提前计算好所有节点需要从其它节点获取的item向量数据，将其缓存在本地，计算每个user向量时，直接从本地读取，可以大大减少需要传输的数据量，提升程序执行的效率。\n     \n在源码中，通过out block缓存当前节点需要向其它节点传输的数据， in block用于缓存当前节点需要的数据索引。当其他节点信息传输到本地时，通过读取in block内索引信息，来从本地获取其它节点传过来的数据。更加详细的描述可参考【7】\n    \nin block 结构： （block_id, Inblock(src_id数组, src_ptr, dst_id地址数组， rating数组）)\nout block结构： （block_id， array[array[int]]） （二维数组存储发往每个block的src_id索引）\n\n    \n```scala\nprivate def makeBlocks[ID: ClassTag](\n      prefix: String,\n      ratingBlocks: RDD[((Int, Int), RatingBlock[ID])],\n      srcPart: Partitioner,\n      dstPart: Partitioner,\n      storageLevel: StorageLevel)(\n      implicit srcOrd: Ordering[ID]): (RDD[(Int, InBlock[ID])], RDD[(Int, OutBlock)]) = {\n    //根据ratingBlocks.map计算inBlocks\n    val inBlocks = ratingBlocks.map {\n      case ((srcBlockId, dstBlockId), RatingBlock(srcIds, dstIds, ratings)) =>\n        val start = System.nanoTime()\n        //dst id去重复\n        val dstIdSet = new OpenHashSet[ID](1 << 20) \n        dstIds.foreach(dstIdSet.add)  \n        //dst id 去重结果进行排序\n        val sortedDstIds = new Array[ID](dstIdSet.size)\n        var i = 0\n        var pos = dstIdSet.nextPos(0)\n        while (pos != -1) {\n          sortedDstIds(i) = dstIdSet.getValue(pos)\n          pos = dstIdSet.nextPos(pos + 1)\n          i += 1\n        }\n        assert(i == dstIdSet.size)\n        Sorting.quickSort(sortedDstIds)\n        //得到dst id 对应的去重和排序后的索引值\n        val dstIdToLocalIndex = new OpenHashMap[ID, Int](sortedDstIds.length)\n        i = 0\n        while (i < sortedDstIds.length) {\n          dstIdToLocalIndex.update(sortedDstIds(i), i)\n          i += 1\n        }\n        logDebug(\n          \"Converting to local indices took \" + (System.nanoTime() - start) / 1e9 + \" seconds.\")\n        val dstLocalIndices = dstIds.map(dstIdToLocalIndex.apply)\n        (srcBlockId, (dstBlockId, srcIds, dstLocalIndices, ratings))\n    }.groupByKey(new ALSPartitioner(srcPart.numPartitions)) //根据src block id进行聚合\n      .mapValues { iter =>\n        val builder =\n          new UncompressedInBlockBuilder[ID](new LocalIndexEncoder(dstPart.numPartitions))\n        //将dstBlockId和dstLocalIndices编码，并汇总数据\n        iter.foreach { case (dstBlockId, srcIds, dstLocalIndices, ratings) =>\n          builder.add(dstBlockId, srcIds, dstLocalIndices, ratings)\n        }\n        //对结果进行压缩存储，结果格式为（uniqueSrcId数组, dstPtrs数组, dstEncodedIndices数组, ratings数组）\n        builder.build().compress()\n      }.setName(prefix + \"InBlocks\")\n      .persist(storageLevel)\n        \n    //根据inBlocks计算outBlocks\n    val outBlocks = inBlocks.mapValues { case InBlock(srcIds, dstPtrs, dstEncodedIndices, _) =>\n      //构造编码器\n      val encoder = new LocalIndexEncoder(dstPart.numPartitions)\n      //定义ArrayBuilder数组，存储发往每个out block的 src id信息\n      val activeIds = Array.fill(dstPart.numPartitions)(mutable.ArrayBuilder.make[Int])\n      var i = 0\n      val seen = new Array[Boolean](dstPart.numPartitions)\n      //依次计算当前src id是否发往每一个block id\n      while (i < srcIds.length) {\n        var j = dstPtrs(i)\n        ju.Arrays.fill(seen, fALSe)\n        while (j < dstPtrs(i + 1)) {\n          val dstBlockId = encoder.blockId(dstEncodedIndices(j))\n          if (!seen(dstBlockId)) {\n            activeIds(dstBlockId) += i // add the local index in this out-block\n            seen(dstBlockId) = true\n          }\n          j += 1\n        }\n        i += 1\n      }\n      activeIds.map { x =>\n        x.result()\n      }\n    }.setName(prefix + \"OutBlocks\")\n      .persist(storageLevel)\n    (inBlocks, outBlocks)  //返回结果\n  }\n```\n\n#### inblock compress\n\n  对inblock 中间结果压缩存储，返回结果格式为（uniqueSrcId数组, dstPtrs数组, dstEncodedIndices数组, ratings数组）\n\n    \n```scala\ndef compress(): InBlock[ID] = {\n  val sz = length\n  assert(sz > 0, \"Empty in-link block should not exist.\")\n  sort()\n  val uniqueSrcIdsBuilder = mutable.ArrayBuilder.make[ID]\n  val dstCountsBuilder = mutable.ArrayBuilder.make[Int]\n  var preSrcId = srcIds(0)\n  uniqueSrcIdsBuilder += preSrcId\n  var curCount = 1\n  var i = 1\n  var j = 0\n  //得到去重后的src id数组， 以及每个src id的数量\n  while (i < sz) {\n    val srcId = srcIds(i)\n    if (srcId != preSrcId) {\n      uniqueSrcIdsBuilder += srcId\n      dstCountsBuilder += curCount\n      preSrcId = srcId\n      j += 1\n      curCount = 0\n    }\n    curCount += 1\n    i += 1\n  }\n  dstCountsBuilder += curCount\n  val uniqueSrcIds = uniqueSrcIdsBuilder.result()\n  val numUniqueSrdIds = uniqueSrcIds.length\n  val dstCounts = dstCountsBuilder.result()\n  val dstPtrs = new Array[Int](numUniqueSrdIds + 1)\n  var sum = 0\n  //将src id和dst id关系通过dstPtrs进行压缩存储\n  i = 0\n  while (i < numUniqueSrdIds) {\n    sum += dstCounts(i)\n    i += 1\n    dstPtrs(i) = sum\n  }\n  InBlock(uniqueSrcIds, dstPtrs, dstEncodedIndices, ratings)\n}\n```\n\n### computeFactor\n\n  根据srcFactorBlocks、srcOutBlocks、dstInBlocks, 计算dstFactorBlocks\n\n    \n```scala\nprivate def computeFactors[ID](\n    srcFactorBlocks: RDD[(Int, FactorBlock)],\n    srcOutBlocks: RDD[(Int, OutBlock)],\n    dstInBlocks: RDD[(Int, InBlock[ID])],\n    rank: Int,\n    regParam: Double,\n    srcEncoder: LocalIndexEncoder,\n    implicitPrefs: Boolean = fALSe,\n    alpha: Double = 1.0,\n    solver: LeastSquaresNESolver): RDD[(Int, FactorBlock)] = {\n  val numSrcBlocks = srcFactorBlocks.partitions.length  //src block数量\n  val YtY = if (implicitPrefs) Some(computeYtY(srcFactorBlocks, rank)) else None\n  //根据srcOut，得到每个dstBlock对应的srcBlockID 和srcFactor数组\n  val srcOut = srcOutBlocks.join(srcFactorBlocks).flatMap {\n    case (srcBlockId, (srcOutBlock, srcFactors)) =>\n      \n      srcOutBlock.view.zipWithIndex.map { case (activeIndices, dstBlockId) =>\n        (dstBlockId, (srcBlockId, activeIndices.map(idx => srcFactors(idx))))\n      }\n  }\n  //根据dstBlockId 对srcBlockID, array[srcFactor]进行聚合\n  val merged = srcOut.groupByKey(new ALSPartitioner(dstInBlocks.partitions.length))\n  //对每个dstBlockID, 计算其中每个dstID对应的隐语义向量\n  dstInBlocks.join(merged).mapValues {\n    case (InBlock(dstIds, srcPtrs, srcEncodedIndices, ratings), srcFactors) =>\n      //得到每个block对应的src factor向量集合\nval sortedSrcFactors = new Array[FactorBlock](numSrcBlocks)\n      srcFactors.foreach { case (srcBlockId, factors) =>\n        sortedSrcFactors(srcBlockId) = factors\n      }\n      //对每个dstID, 获取对应的srcFactor及对应rating, 计算该dstID对应的隐语义向量\n      val dstFactors = new Array[Array[Float]](dstIds.length)\n      var j = 0\n      val ls = new NormalEquation(rank)\n      while (j < dstIds.length) {\n        ls.reset()\n        if (implicitPrefs) {\n          ls.merge(YtY.get)\n        }\n        var i = srcPtrs(j)\n        var numExplicits = 0\n        while (i < srcPtrs(j + 1)) { //依次得到每个srcFactor及rating值\n          val encoded = srcEncodedIndices(i)\n          val blockId = srcEncoder.blockId(encoded)\n          val localIndex = srcEncoder.localIndex(encoded)\n          //sortedSrcFactors通过blockId和localIndex进行索引，得到需要的factor向量。之前这里困惑挺久，一直感觉从srcOut传过来的factor向量只是一个子集，通过localIndex访问不正确，实际上这里的localIndex和srcOut那里存储的localindex是不需要对应的。因为同一个src id 本身的src local index不等于其它block对应的 dst localindex\n          val srcFactor = sortedSrcFactors(blockId)(localIndex)\n          val rating = ratings(i)\n          if (implicitPrefs) {\n            // Extension to the original paper to handle b < 0. confidence is a function of |b|\n            // instead so that it is never negative. c1 is confidence - 1.0.\n            val c1 = alpha * math.abs(rating)\n            // For rating <= 0, the corresponding preference is 0. So the term below is only added\n            // for rating > 0. Because YtY is already added, we need to adjust the scaling here.\n            if (rating > 0) {\n              numExplicits += 1\n              ls.add(srcFactor, (c1 + 1.0) / c1, c1)\n            }\n          } else {\n            ls.add(srcFactor, rating)\n            numExplicits += 1\n          }\n          i += 1\n        }\n        // Weight lambda by the number of explicit ratings based on the ALS-WR paper.\n        dstFactors(j) = solver.solve(ls, numExplicits * regParam)\n        j += 1\n      }\n      dstFactors\n  }\n}\n```\n\n\n## ALS 模型推荐\n    \n**模型参数** \n\n```\n val rank: Int,      //隐语义个数\n val userFeatures: RDD[(Int, Array[Double])], //user factor数组, 存储user id 及对应的factor向量\n val productFeatures: RDD[(Int, Array[Double])]) //item factor数组，存储item id及对应的factor向量\n    \n```\n\n**对所有用户进行推荐**\n\n调用recommendForAll函数，首先对user向量和item向量分块并以矩阵形式存储，然后对二者做笛卡尔积，并计算每个user和每个item的得分，最终以user为key, 取topK个item及对应的得分，作为推荐结果. 计算topK时借助于小顶堆\n\n```\n  private def recommendForAll(\n      rank: Int,\n      srcFeatures: RDD[(Int, Array[Double])],\n      dstFeatures: RDD[(Int, Array[Double])],\n      num: Int): RDD[(Int, Array[(Int, Double)])] = {\n    //对user向量和item向量分块并以矩阵形式存储\n    val srcBlocks = blockify(rank, srcFeatures)\n    val dstBlocks = blockify(rank, dstFeatures)\n    //笛卡尔积，依次对每个组合计算user对item的偏好\n    val ratings = srcBlocks.cartesian(dstBlocks).flatMap {\n      case ((srcIds, srcFactors), (dstIds, dstFactors)) =>\n        val m = srcIds.length\n        val n = dstIds.length\n        val ratings = srcFactors.transpose.multiply(dstFactors)\n        val output = new Array[(Int, (Int, Double))](m * n)\n        var k = 0\n        ratings.foreachActive { (i, j, r) =>\n          output(k) = (srcIds(i), (dstIds(j), r))\n          k += 1\n        }\n        output.toSeq\n    }\n    //根据user id作为key, 得到喜好分数最高的num个item\n    ratings.topByKey(num)(Ordering.by(_._2))\n  }\n\n\n  // 对user向量和item向量分块并以矩阵形式存储, 结果的每个元组分别是对应的id数组和factor构成的矩阵\n  private def blockify(\n      rank: Int,\n      features: RDD[(Int, Array[Double])]): RDD[(Array[Int], DenseMatrix)] = {\n    val blockSize = 4096 // TODO: tune the block size\n    val blockStorage = rank * blockSize\n    features.mapPartitions { iter =>\n      iter.grouped(blockSize).map { grouped =>\n        val ids = mutable.ArrayBuilder.make[Int]\n        ids.sizeHint(blockSize)\n        val factors = mutable.ArrayBuilder.make[Double]\n        factors.sizeHint(blockStorage)\n        var i = 0\n        grouped.foreach { case (id, factor) =>\n          ids += id\n          factors ++= factor\n          i += 1\n        }\n        (ids.result(), new DenseMatrix(rank, i, factors.result()))\n      }\n    }\n  }\n\n```\n\n# ALS推荐实践\n\n我们的平台是图片社交，每个用户都可以在平台上浏览图片，并进行点赞、评论等。推荐算法主要用于给用户推荐其最可能感兴趣的图片，最终提升用户体验。\n\n## 离线实验\n\n我们平台暂时无法得到用户的显式评分数据，但是可以得到用户点击、点赞、评论等相关行为信息。因此，比较适合用隐反馈矩阵分解模型。\n\n### 构造数据集\n* 数据预处理\n  \n  从2周的用户行为数据中，过滤无行为用户数据，spam图片数据和spam用户数据。\n\n* 构建rating元素\n\n  对预处理之后的数据，根据用户每天的图片交互行为，分别对点击、点赞和评论等分别赋予不同的权值，得到rating矩阵. \n\n* 生成训练集和测试集\n\n  对于得到的rating数据，随机划分为两部分 $A:B = 7:3$，如果分别直接作为训练集和测试集是有问题的，因为$B$中的user或者item是有可能在$A$中没有出现过，这样会影响评估结果。 我们采用的方法是如果B数据中某个rating元素的user或item没有在A出现，则将该元素放到$A$中用作训练集。最终$A$和新加进来的元素共同构成训练集$A^1$， $B$留下的数据 $B^1$ 作为测试集。\n  \n\n### 离线训练和评估\n\n* 离线训练\n\n利用spark mllib库，对训练集构成的rating矩阵，建立隐反馈矩阵分解模型，并完成进行矩阵分解，生成user factor和item factor。\n\n* 评估\n\n调用模型的recommendForAll函数，对测试集所有user进行item推荐，并计算召回率和准确率。根据召回率和准确率，进行参数优化。\n\n* 评估指标\n\n假定$P_i$为用户$i$的预测结果，$P$为所有的预测结果，每个结果记录格式为（user, item）， $T$为测试集,每条记录格式为（user， item）。各种指标的的计算如下：\n\n召回率: $R= \\frac{|P \\bigcap T|}  {|T|}$\n\n准确率: $P= \\frac{|P \\bigcap T|}  {|P|}$\n\nF1:  $F= \\frac{2PR}  {P+R} $\n\n离散度：$\\frac{1}{N^2}\\sum\\_i\\sum\\_j\\frac{|P\\_i \\bigcap P\\_j|}{|P\\_i \\bigcup P\\_j|}$\n\n除了上述指标之外，我们还对用户连续多天推荐结果的差异性、用户覆盖率、图片覆盖率等指标进行评估。\n\n## 在线ab测试\n\nabtest方案： 将als算法计算出的结果，定期写入到线上，作为线上的一种推荐来源。对实验组用户同时采用新策略和旧策略进行推荐，对照组用户只采用旧策略进行推荐。\n\n从2个维度进行评估：\n\n* 评估实验组和对照组用户在abtest上线前后点击率\n* 评估实验组用户在新旧两种策略推荐图片的点击率\n\n测试一定时间后，交换对照组和实验组用户，按照上述2个维度重新进行评估\n\n\n\n\n\n# 参考文献\n\n【1】Y Koren，R Bell，C Volinsky, \"Matrix Factorization Techniques for Recommender Systems\", 《Computer》, 2009.08; 42(8):30-37 \n\n【2】洪亮劼, \"知人知面需知心——人工智能技术在推荐系统中的应用\", 2016.11, http://mp.weixin.qq.com/s/JuaM8d52-f8AzTjEPnCl7g\n\n【3】S. Funk, \"Netflix Update: Try This at Home\", 2006.12, http://sifter.org/~simon/journal/20061211.html\n\n【4】Y. Koren, \"Factorization Meets the Neighborhood: A Mul-tifaceted Collaborative Filtering Model\", Proc. 14th ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining, ACM Press, 2008, pp.426-434\n\n【5】A. Paterek, \"Improving Regularized Singular Value De-composition for Collaborative Filtering\" Proc. KDD Cup and Workshop, ACM Press, 2007, pp.39-42\n\n【6】G. Takács et al., \"Major Components of the Gravity Recom- mendation System\", SIGKDD Explorations, 2007.09, vol.9, pp.80-84\n\n【7】孟祥瑞, \"ALS 在 Spark MLlib 中的实现\", 2015.05, http://www.csdn.net/article/2015-05-07/2824641\n\n【8】Zhen-ming Yuan, et al., \"A microblog recommendation algorithm based on social tagging and a temporal interest evolution model\", Frontiers of Information Technology & Electronic Engineering, 2015.07,\nVolume 16, Issue 7, pp 532–540 \n\n【9】Z Zhao, Z Cheng, L Hong, EH Chi, \"Improving User Topic Interest Profiles by Behavior Factorization\", Proceedings of the 24th International Conference on World Wide Web, 2015.05, pp.1406-1416\r\r【10】阿里技术，\"淘宝搜索/推荐系统背后深度强化学习与自适应在线学习的实践之路\", 2017.02, http://url.cn/451740J\n\n【11】HT Cheng, L Koc, J Harmsen, T Shaked, \"Wide & Deep Learning for Recommender Systems\", Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, 2016.09,  pp.7-10\n\n【12】黄安埠, \"递归的艺术 - 深度递归网络在序列式推荐的应用\", 2016.10, http://mp.weixin.qq.com/s?__biz=MzA3MDQ4MzQzMg==&mid=2665690422&idx=1&sn=9bd671983a85286149b51c908b686899&chksm=842bb9b1b35c30a7eedb8d03e173aa8f43465db90e11075ac0c73b1784582f21eb93dcbd3e65&scene=0%23wechat_redirect\n","source":"_posts/als.md","raw":"---\ntitle: ALS推荐算法学习与实践\ndate: 2017-02-22\ntoc: true\ncategories: 推荐系统\ntags: [推荐算法,协同过滤,矩阵分解,隐语义模型]\ndescription: als算法学习与实践\nmathjax: true\n---\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\nALS（alternating least squares）是一种基础的推荐算法，相对于普通的协同过滤等方法，它不仅能通过降维增加模型的泛化能力，也方便加入其他建模因素（如数据偏差、时间、隐反馈等），大大提升了模型的灵活性。正因为此，ALS算法在Netflix推荐大赛中脱颖而出，在我们具体的工程实践中，也具有非常不错的表现。接下来，从如下几个方面和大家一起学习：ALS算法模型、spark ALS源码理解， ALS推荐实践。如描述有误，欢迎大家指正。\n\n# ALS算法模型\n## 为什么要用ALS模型\n 相对于其他模型，ALS模型优势如下：\n \n* **相对于基于内容的推荐**，ALS属于协同过滤大家族【1】【12】（也有人认为ALS 基于矩阵分解技术，不属于协同过滤范畴【2】），**直接跟进用户行为信息进行建模，不需要获取user和item的内容信息**（很多情况下这些内容信息并不是很好获取，但是相对基于内容的推荐，ALS存在冷启动问题）\n\n* **相对于传统的协同过滤推荐方法（user based、item based）**， ALS算法属于factor model, 通过将数据从原始空间映射到更低维度空间，**去除噪声信息，利用更主要的语义信息对问题建模，能获得更好的推荐效果**。\n\n* **相对于svd分解模型而言**， 两种模型都属于 factor model, 但**svd分解模型更倾向于解决矩阵元素没有缺失的情况， 而通过一定的方式去填充矩阵不仅需要额外的填充成本，填充本身可能影响了数据的真实性**。因此，直接对已知元素进行建模，是一种不错的思路。如【1，3-6】，直接对rating矩阵已知元素$r\\_{ui}$进行建模:\n\n<center>\n$\\sum\\_{u,i\\in\\mathbb K} (r\\_{ui} - \np\\_u^Tq\\_i)^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$ （1）\n</center>\n\n* 针对所建模型1可以用SGD或ALS 两种算法求解。其中**sgd方法相对比较简单，但是当我们要建模的矩阵中已知元素较多时（如隐反馈），采用sgd在每次迭代都要求解所有元素，其时间复杂度是非常大的**。ALS算法在求解某个user （或item）向量时，不依赖其他任何user（item）向量，这个性质使得**ALS算法在每次迭代过程中方便并行化求解，在解决大规模矩阵分解问题时非常具有优势**。 \n\n## ALS模型有什么缺点\n 相对于其它推荐算法，ALS模型具有非常明显的优势：不需要对user和item信息进行建模，能够更加灵活地对各种因素建模，方便大规模并行计算。但ALS模型在如下几方面，又有自己的局限性：\n \n * 冷启动问题\n   \n   包括user的冷启动和item的冷启动。由于rating矩阵的构建，依赖user的显式和隐式反馈信息，对于新的user和item，或者没有相关行为的user或item, 导致无法构建rating矩阵，或者rating矩阵构建不合理。\n   \n   基于内容的推荐能较好地解决冷启动相关问题。如【8】在解决用户的冷启动问题时，首先根据用户之间社交的亲密度，对用户进行聚类，利用相同群体的用户画像来建模自身兴趣。同时，对于item的冷启动问题，可利用item本身对一些关键词、类目、内容等相关信息进行建模。【9】为了提高用户兴趣的准确率和覆盖率，在对用户兴趣建模对时候，将用户兴趣进行更具体的分类（如消费兴趣、生产兴趣、具体的每个行为兴趣等），并针对具体的业务，采用线性回归的方法对各种兴趣利用线性回归的方式进行加权求和，提升用户兴趣准确率和覆盖率。\n \n * 用户临时兴趣\n \n   用户的兴趣是在不断变换的。对于相对较稳定的兴趣，ALS算法可以通过引入时间因素进行建模，如公式4。 但对于临时的兴趣变换，ALS算法是无法捕获的。\n   \n   **一种简单且有效的方法**\n   \n   将item划分为多个类别，每个类别对应一种兴趣。用户每次点击某个类别的item之后，认为该用户存在一种临时兴趣，通过动态增加相应类别的比例的item，迎合用户当前的消费需求。该方法的难点在于如何调整比例，才能让用户感到有很多自己喜欢的item, 同时又不会让用户感觉内容的单调。\n   \n   **淘宝的一些实践**\n   \n   为了有效获取用户的即时兴趣，给用户推荐最合适的产品，淘宝进行了比较多的实践【10】，分别如下所述。\n   * GBDT+FTRL模型\n   \n     由于GBDT模型比较擅长挖具有区分度的特征，其使用GBDT模型进行特征挖掘，将得到的特征输送给FTRL进行在线学习。输送给GBDT的特征包括两部分：一部分用户基础行为的次数、CTR等；另一部分是来自match粗选阶段的的特征，该部分特征来自不同的粗选模型输出.\n   \n   * Wide & Deep Learning模型\n   \n     借鉴google论文思想【11】，利用wide模型 + deep模型 + LR，其中wide子结构通过特征交叉学习特征间的共现，deep子结构则输入具有泛化能力的离散特征和连续特征，wide模型和deep模型学习到的结果，再利用LR模型预测相应的得分。\n     \n   * Adaptive-Online-Learning\n   \n     保留每一时刻学习到的模型，根据业务指标，得到每个模型等权重信息，融合出最优的结果。该方法能够比较好地综合利用用户长期喝短期兴趣。\n     \n   * Reinforcement Learning\n     \n     该方法思想是通过定义每个步骤的奖励，当用户每次到来的时候，根据用户的累积奖励值，进行个性化推荐。\n     \n   **腾讯的LSTM实践**\n     \n     为解决音乐的推荐问题，腾讯采用的是LSTM深度学习方法【12】，将用户听的歌曲序列，抽取特征输入到LSTM网络进行训练。为防止有些用户对应的歌曲序列较短问题，其对这些数据的训练采用特殊处理，相关数据缺失的序列不进行状态更新。同时，为加快训练速度，将每次权值的训练过程通过矩阵的方式实现并发计算。另外，为降低soft max过程时间复杂度，采用Hierarchical softmax过程替代普通的softmax。\n\n  \n## ALS模型是什么\n### 基本概念\n\nALS模型属于隐语义模型，通过对用户行为矩阵R进行矩阵分解，得到user factor向量矩阵P、item factor向量矩阵Q. \n\n$R = P^T Q$ 。其中R、$P^T$、$Q^T$矩阵的定义如表1-表3所示。\n    \n潜在语义空间对应的各个factor代表不同的属性信息，user向量描述了user对各种属性的喜好程度，item向量描述了item所具备的各种属性强度，二者在潜在语义空间的相似度描述了user对item的喜好程度,在进行推荐时，根据该喜好程度计算推荐结果。\n<center>表1: rating矩阵R</center>\n\n |item1|item2|item3|item4\n-|-|-|-|-|\nuser1|$r\\_{11}$|$r\\_{12}$| $r\\_{13}$| $r\\_{14}$\nuser2|$r\\_{21}$|$r\\_{22}$| $r\\_{23}$| $r\\_{24}$\nuser3|$r\\_{31}$|$r\\_{32}$| $r\\_{33}$| $r\\_{34}$\nuser4|$r\\_{41}$|$r\\_{42}$| $r\\_{43}$| $r\\_{44}$\nuser5|$r\\_{51}$|$r\\_{52}$| $r\\_{53}$| $r\\_{54}$\n \n\n<center>表2：user矩阵$P^T$</center>\n\n | factor1 | factor2 | factor3 \n-|-|-|-|\nuser1|$p\\_{11}$|$p\\_{12}$| $p\\_{13}$\nuser2|$p\\_{21}$|$p\\_{22}$| $p\\_{23}$\nuser3|$p\\_{31}$|$p\\_{32}$| $p\\_{33}$\nuser4|$p\\_{41}$|$p\\_{42}$| $p\\_{43}$\nuser5|$p\\_{51}$|$p\\_{52}$| $p\\_{53}$\n\n\n<center>表3:item矩阵$Q^T$</center>\n\n | factor1 | factor2 | factor3 \n-|-|-|-|\nitem1|$q\\_{11}$|$q\\_{12}$| $q\\_{13}$\nitem2|$q\\_{21}$|$q\\_{22}$| $q\\_{23}$\nitem3|$q\\_{31}$|$q\\_{32}$| $q\\_{33}$\nitem4|$q\\_{41}$|$q\\_{42}$| $q\\_{43}$\n\n\n### 目标函数\n  \n$MIN\\_{PQ} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$  (2)\n\n其中${(r\\_{ui} - p\\_u^Tq\\_i）}^2$ 目的在于最小化分解误差，$\\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$ 为正则项。\n\n### 目标函数求解\n\n由于目标函数中$p\\_u, q_i$都是未知变量，该问题是非凸的。当我们固定其中一个变量，解另外一个变量时，问题则变成凸问题，这是ALS求解的主要思想。在实际求解过程中分为如下几个步骤：\n\n1. 随机初始化所有的变量$p\\_u, q\\_i$。\n  \n2. 固定所有的$q\\_i$变量，求得$q\\_i$变量为当前值时$p\\_u$的最优值。\n  \n3. 固定所有的$p\\_u$变量，求得$p\\_u$变量为当前值时$q\\_i$的最优值。\n  \n4. 如果满足终止条件，则终止。否则，迭代执行2，3两步。\n\n通过不断执行步骤2和步骤3，使得误差越来越小，直到收敛或达到指定次数而终止。通过可导函数性质我们知道，当对变量求导结果等于0当时候，函数可以取得极值。具体到公式2，固定一个变量，对另一变量求导结果等于0时，可以达到极小值。\n \n我们令$L = \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - p\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n固定所有$q\\_i$, 对$p\\_u$求导\n  \n$-\\frac{\\alpha L}{2\\alpha p\\_{uk}} = \\sum\\_{i} {q\\_{ik}(r\\_{ui} - p\\_u^Tq\\_i）} - \\lambda p\\_{uk} = 0$\n\n=> $\\sum\\_{i} {q\\_{i}(r\\_{ui} - p\\_u^Tq\\_i）} - \\lambda p\\_{u} = 0$\n\n=> $(\\sum\\_{i} {q\\_i q\\_i^T} + \\lambda E) p\\_u = \\sum\\_{i}q\\_i r\\_{ui}$\n\n=> $p\\_u = (\\sum\\_{i} {q\\_i q\\_i^T} + \\lambda E)^{-1}\\sum\\_{i}q\\_i r\\_{ui}$\n\n=> $ p\\_u = (Q\\_{u,i\\in\\mathbb K} Q\\_{u,i\\in\\mathbb K}^T + \\lambda E)^{-1}Q\\_{u,i\\in\\mathbb K}R\\_{u,i\\in\\mathbb K}^T$\n\n其中，$q\\_{u,i\\in\\mathbb K}$ 表示和user $u$有行为关联的item对应的向量矩阵，$r\\_{u,i\\in\\mathbb K}^T$表示和user $u$有行为关联的item对应rating元素构成的向量的转置。\n\n**更加灵活的ALS建模**\n\n相对于传统的协同协同过滤方法，ALS能更好的考虑其他因素，如数据偏差、时间等\n\n1. 引入数据偏差\n    \n    user偏差：不同的用户，可能具有不同的评分标准。如用户在给item打分时，有的用户可能可能更倾向于给所有item打高分， 而有的挑剔用户会给所有item打分偏低\n    \n    item偏差：有的热门item可能所有用户都会倾向于打高分，而有的item可能本身大多数人会倾向于打低分\n    \n    考虑use和item偏差的ALS建模：\n    $MIN\\_{PQB} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - u - b\\_u - b\\_i-\np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i+b\\_u^2+b\\_i^2)$  (3)\n\n2. 引入时间因素\n    \n    用户偏好、rating矩阵，都可能随时间变化，item对应的属性不随时间变化，因此可进行如下建模\n$MIN\\_{PQB} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui}（t） - u - b\\_u(t) - b\\_i(t)-\np\\_u(t)^Tq\\_i）}^2 + \\lambda(p\\_u(t)^Tp\\_u(t)+q\\_i^Tq\\_i+b\\_u(t)^2+b\\_i(t)^2)$  (4)\n    \n3. 引入隐反馈数据因素\n    \n    很多时候，并没有用户对item明确的打分数据，此时可通过搜集用户隐反馈数据（浏览、点击、点赞等），进行隐反馈建模。有一点需要注意，此时不只是对$r\\_{ui}$大于0对用户行为建模，而是所有$r\\_{ui}$元素建模。模型如公式5所示：\n    \n    $MIN\\_{PQB} \\sum\\_{u,i} {c\\_{ui}(p\\_{ui} - u - b\\_u - b\\_i-\np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i+b\\_u^2+b\\_i^2)$  (5)\n \n    $p\\_{ui}$ 表示user u是否有相关行为表示喜欢item i, $c\\_{ui}$描述user u 对item i的喜欢程度，其定义如公式6和公式7所示\n \n    $\np\\_{ui} = \n\\begin{cases} \n1,  & r\\_{ui}>0\\\\\\\\\n0,  & r\\_{ui}=0\n\\end{cases}\n$（6）\n    \n    $c\\_{ui} = 1 + \\alpha r\\_{ui}$（7）\n\n\n\n# spark ALS源码理解\n    \n为加深对ALS算法的理解，该部分主要分析spark mllib中ALS源码的实现，大体上分为2部分：ALS模型训练、ALS模型推荐\n\n## ALS 模型训练\n\n### ALS 伴生类\n    \nALS 伴生对象提供外部调用 ALS模型训练的入口。通过传入相关参数， 返回训练好的模型对象MatrixFactorizationModel。\n\n\n```scala\nobject ALS {\n  def train(\n      ratings: RDD[Rating], //rating元素 （user, item, rate）\n      rank: Int, //隐语义个数\n      iterations: Int, //迭代次数\n      lambda: Double, //正则惩罚项\n      blocks: Int, //数据block个数\n      seed: Long //随机数种子\n    ): MatrixFactorizationModel = {\n    new ALS(blocks, blocks, rank, iterations, lambda, fALSe, 1.0, seed).run(ratings)\n  }\n\n  def trainImplicit(\n      ratings: RDD[Rating], // rating元素 （user, item, rate）\n      rank: Int, //隐语义个数\n      iterations: Int, //迭代次数\n      lambda: Double, //正则惩罚项\n      blocks: Int, //数据block个数\n      alpha: Double //计算$c_ui$时用的alpha参数\n    ): MatrixFactorizationModel = {\n    new ALS(blocks, blocks, rank, iterations, lambda, true, alpha).run(ratings)\n  }\n   //另外还有一些其他接口，因最终都通过调用上面2个函数，此处将其省略\n}\n```\n\n### ALS 私有类\n    \n定义了ALS类对应的各个参数，以及各个参数的设定方法。并定义了run方法供伴随类进行调用，该方法返回训练结果MatrixFactorizationModel给ALS伴随类。\n\n    \n```scala\nclass ALS private (\n    private var numUserBlocks: Int, //用户数据block个数\n    private var numProductBlocks: Int, //item数据block个数\n    private var rank: Int, //隐语义个数\n    private var iterations: Int, //迭代次数\n    private var lambda: Double, //正则惩罚项\n    private var implicitPrefs: Boolean, //是否使用隐反馈模型\n    private var alpha: Double, //计算$c_ui$时用的alpha参数\n    private var seed: Long = System.nanoTime() //随机数种子,默认为当前时间戳\n  ) extends Serializable with Logging {\n\n  //设置block个数\n  def setBlocks(numBlocks: Int): this.type = {\n    this.numUserBlocks = numBlocks\n    this.numProductBlocks = numBlocks\n    this\n  }\n  \n  // 另外对其他参数变量也有相关函数实现，因基本都是赋值操作，此处将其省略\n  \n  //run方法，通过输入rating数据，完成训练兵返回结果MatrixFactorizationModel\n  def run(ratings: RDD[Rating]): MatrixFactorizationModel = {\n    require(!ratings.isEmpty(), s\"No ratings available from $ratings\")\n\n    val sc = ratings.context\n    //设置user block个数\n    val numUserBlocks = if (this.numUserBlocks == -1) {\n      math.max(sc.defaultParallelism, ratings.partitions.length / 2)\n    } else {\n      this.numUserBlocks\n    }\n    //设置item block个数\n    val numProductBlocks = if (this.numProductBlocks == -1) {\n      math.max(sc.defaultParallelism, ratings.partitions.length / 2)\n    } else {\n      this.numProductBlocks\n    }\n    //调用NewALS.train方法完成矩阵分解，生成user factor和item factor向量,该方法是整个ALS算法的核心实现\n    val (floatUserFactors, floatProdFactors) = NewALS.train[Int](\n      ratings = ratings.map(r => NewALS.Rating(r.user, r.product, r.rating.toFloat)),\n      rank = rank,\n      numUserBlocks = numUserBlocks,\n      numItemBlocks = numProductBlocks,\n      maxIter = iterations,\n      regParam = lambda,\n      implicitPrefs = implicitPrefs,\n      alpha = alpha,\n      nonnegative = nonnegative,\n      intermediateRDDStorageLevel = intermediateRDDStorageLevel,\n      finalRDDStorageLevel = StorageLevel.NONE,\n      checkpointInterval = checkpointInterval,\n      seed = seed)\n   \n    val userFactors = floatUserFactors\n      .mapValues(_.map(_.toDouble))\n      .setName(\"users\")\n      .persist(finalRDDStorageLevel)\n    val prodFactors = floatProdFactors\n      .mapValues(_.map(_.toDouble))\n      .setName(\"products\")\n      .persist(finalRDDStorageLevel)\n    if (finalRDDStorageLevel != StorageLevel.NONE) {\n      userFactors.count()\n      prodFactors.count()\n    }\n    //生成和返回ALS模型 MatrixFactorizationModel\n    new MatrixFactorizationModel(rank, userFactors, prodFactors)\n  }\n}\n```\n\n\n### NewALS.train方法\n    \n被ALS私有类的run方法调用，用于计算user factor和item factor向量。\n\n```scala\ndef train[ID: ClassTag]( // scalastyle:ignore\n      ratings: RDD[Rating[ID]],\n      rank: Int = 10,\n      numUserBlocks: Int = 10,\n      numItemBlocks: Int = 10,\n      maxIter: Int = 10,\n      regParam: Double = 1.0,\n      implicitPrefs: Boolean = fALSe,\n      alpha: Double = 1.0,\n      nonnegative: Boolean = fALSe,\n      intermediateRDDStorageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK,\n      finalRDDStorageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK,\n      checkpointInterval: Int = 10,\n      seed: Long = 0L)(\n      implicit ord: Ordering[ID]): (RDD[(ID, Array[Float])], RDD[(ID, Array[Float])]) = {\n    require(!ratings.isEmpty(), s\"No ratings available from $ratings\")\n    require(intermediateRDDStorageLevel != StorageLevel.NONE,\n      \"ALS is not designed to run without persisting intermediate RDDs.\")\n    val sc = ratings.sparkContext\n    //根据block个数，构建哈稀器。\n    val userPart = new ALSPartitioner(numUserBlocks)\n    val itemPart = new ALSPartitioner(numItemBlocks)\n    //构建索引编码器，根据block编号和block内索引进行编码，同时可将编码后结果快速解码为block编号和block内索引号。具体实现是通过block个数，确定block编码需要的二进制位数，以及block内索引位数，通过这些位数利用逻辑操作即可实现编码和解码\n\n    val userLocalIndexEncoder = new LocalIndexEncoder(userPart.numPartitions)\n    val itemLocalIndexEncoder = new LocalIndexEncoder(itemPart.numPartitions)\n    //构建求解器\n    val solver = if (nonnegative) new NNLSSolver else new CholeskySolver\n    //对rating矩阵进行分块，得到((user_blockID, item_blockID),rating(user, item, rating))\n\n    val blockRatings = partitionRatings(ratings, userPart, itemPart)\n      .persist(intermediateRDDStorageLevel)\n    //构建user inblock和outblock数据，inblock数据记录每个user对应的所有item的地址，及对应rating信息。 outblock记录当前block的哪些user数据会被哪些block用上\n    val (userInBlocks, userOutBlocks) =\n      makeBlocks(\"user\", blockRatings, userPart, itemPart, intermediateRDDStorageLevel)\n    // materialize blockRatings and user blocks\n    userOutBlocks.count()\n    //交换blockrating中的user, item数据，用于构造item的inblcok和outblock信息\n    val swappedBlockRatings = blockRatings.map {\n      case ((userBlockId, itemBlockId), RatingBlock(userIds, itemIds, localRatings)) =>\n        ((itemBlockId, userBlockId), RatingBlock(itemIds, userIds, localRatings))\n    }\n    val (itemInBlocks, itemOutBlocks) =\n      makeBlocks(\"item\", swappedBlockRatings, itemPart, userPart, intermediateRDDStorageLevel)\n    // materialize item blocks\n    itemOutBlocks.count()\n    val seedGen = new XORShiftRandom(seed)\n    //随机初始化user factor和item factor\n    var userFactors = initialize(userInBlocks, rank, seedGen.nextLong())\n    var itemFactors = initialize(itemInBlocks, rank, seedGen.nextLong())\n    var previousCheckpointFile: Option[String] = None\n    val shouldCheckpoint: Int => Boolean = (iter) =>\n      sc.checkpointDir.isDefined && checkpointInterval != -1 && (iter % checkpointInterval == 0)\n    val deletePreviousCheckpointFile: () => Unit = () =>\n      previousCheckpointFile.foreach { file =>\n        try {\n          val checkpointFile = new Path(file)\n          checkpointFile.getFileSystem(sc.hadoopConfiguration).delete(checkpointFile, true)\n        } catch {\n          case e: IOException =>\n            logWarning(s\"Cannot delete checkpoint file $file:\", e)\n        }\n      }\n    //针对隐反馈，迭代求解\n    if (implicitPrefs) {\n      for (iter <- 1 to maxIter) {  //迭代总次数maxIter\n        userFactors.setName(s\"userFactors-$iter\").persist(intermediateRDDStorageLevel)\n        val previousItemFactors = itemFactors\n        //固定user factor，优化item factor\n        itemFactors = computeFactors(userFactors, userOutBlocks, itemInBlocks, rank, regParam,\n          userLocalIndexEncoder, implicitPrefs, alpha, solver)\n        previousItemFactors.unpersist()\n        itemFactors.setName(s\"itemFactors-$iter\").persist(intermediateRDDStorageLevel)\n        // TODO: Generalize PeriodicGraphCheckpointer and use it here.\n        val deps = itemFactors.dependencies\n        if (shouldCheckpoint(iter)) {\n          itemFactors.checkpoint() // itemFactors gets materialized in computeFactors\n        }\n        val previousUserFactors = userFactors\n        //根据item factore, 优化user factor\n        userFactors = computeFactors(itemFactors, itemOutBlocks, userInBlocks, rank, regParam,\n          itemLocalIndexEncoder, implicitPrefs, alpha, solver)\n        if (shouldCheckpoint(iter)) {\n          ALS.cleanShuffleDependencies(sc, deps)\n          deletePreviousCheckpointFile()\n          previousCheckpointFile = itemFactors.getCheckpointFile\n        }\n        previousUserFactors.unpersist()\n      }\n    } else { //针对显示反馈，迭代求解\n      for (iter <- 0 until maxIter) { //迭代总次数maxIter\n        //固定user factor，优化item factor\n        itemFactors = computeFactors(userFactors, userOutBlocks, itemInBlocks, rank, regParam,\n          userLocalIndexEncoder, solver = solver)\n        if (shouldCheckpoint(iter)) {\n          val deps = itemFactors.dependencies\n          itemFactors.checkpoint()\n          itemFactors.count() // checkpoint item factors and cut lineage\n          ALS.cleanShuffleDependencies(sc, deps)\n          deletePreviousCheckpointFile()\n          previousCheckpointFile = itemFactors.getCheckpointFile\n        }\n        //根据item factore, 优化user factor\n        userFactors = computeFactors(itemFactors, itemOutBlocks, userInBlocks, rank, regParam,\n          itemLocalIndexEncoder, solver = solver)\n      }\n    }\n    //将user id 和 factor拼接在一起\n    val userIdAndFactors = userInBlocks\n      .mapValues(_.srcIds)\n      .join(userFactors)\n      .mapPartitions({ items =>\n        items.flatMap { case (_, (ids, factors)) =>\n          ids.view.zip(factors)\n        }\n      // Preserve the partitioning because IDs are consistent with the partitioners in userInBlocks\n      // and userFactors.\n      }, preservesPartitioning = true)\n      .setName(\"userFactors\")\n      .persist(finalRDDStorageLevel)\n    //将item id 和 factor拼接在一起\n    val itemIdAndFactors = itemInBlocks\n      .mapValues(_.srcIds)\n      .join(itemFactors)\n      .mapPartitions({ items =>\n        items.flatMap { case (_, (ids, factors)) =>\n          ids.view.zip(factors)\n        }\n      }, preservesPartitioning = true)\n      .setName(\"itemFactors\")\n      .persist(finalRDDStorageLevel)\n    if (finalRDDStorageLevel != StorageLevel.NONE) {\n      userIdAndFactors.count()\n      itemFactors.unpersist()\n      itemIdAndFactors.count()\n      userInBlocks.unpersist()\n      userOutBlocks.unpersist()\n      itemInBlocks.unpersist()\n      itemOutBlocks.unpersist()\n      blockRatings.unpersist()\n    }\n    //返回user factor和item factor数据\n    (userIdAndFactors, itemIdAndFactors)\n  }\n```\n\n### 构建哈希器 \n  \n   构建哈希器，用于计算user或item id对应的block编号。\n    \n```scala\nclass HashPartitioner(partitions: Int) extends Partitioner {\n  require(partitions >= 0, s\"Number of partitions ($partitions) cannot be negative.\")\n  //block总数\n  def numPartitions: Int = partitions\n  //通过求余计算block 编号\n  def getPartition(key: Any): Int = key match {\n    case null => 0\n    case _ => Utils.nonNegativeMod(key.hashCode, numPartitions)\n  }\n  //判断2个哈希器是否相等\n  override def equALS(other: Any): Boolean = other match {\n    case h: HashPartitioner =>\n      h.numPartitions == numPartitions\n    case _ =>\n      fALSe\n  }\n  override def hashCode: Int = numPartitions\n}\n```\n\n### 构建地址编码解码器\n\n构建地址编码解码器，根据block编号和block内索引对地址进行编码，同时可将编码后地址解码为block编号和block内索引号。具体实现是通过block个数确定block编码需要的二进制位数，以及block内索引位数，通过这些位数利用逻辑操作即可实现地址的编码和解码。\n    \n```scala\nprivate[recommendation] class LocalIndexEncoder(numBlocks: Int) extends Serializable {\n\n    require(numBlocks > 0, s\"numBlocks must be positive but found $numBlocks.\")\n    //block内部索引使用的二进制位数\n    private[this] final val numLocalIndexBits =\n      math.min(java.lang.Integer.numberOfLeadingZeros(numBlocks - 1), 31)\n    private[this] final val localIndexMask = (1 << numLocalIndexBits) - 1\n    \n    //根据block编号和block内索引值，对地址编码\n    def encode(blockId: Int, localIndex: Int): Int = {\n      require(blockId < numBlocks)\n      require((localIndex & ~localIndexMask) == 0)\n      (blockId << numLocalIndexBits) | localIndex\n    }\n\n    //根据编码后地址，得到block编号\n    def blockId(encoded: Int): Int = {\n      encoded >>> numLocalIndexBits\n    }\n\n    //根据编码地址，得到block内部索引\n    def localIndex(encoded: Int): Int = {\n      encoded & localIndexMask\n    }\n  }\n```\n\n### partition rating\n\n格式化rating数据，将rating数据分块，根据user和product的id哈希后的结果，得到对应的块索引。最终返回（src_block_id, dst_block_id）(src_id数组，dst_id数组，rating数组)\n\n    \n```scala\nprivate def partitionRatings[ID: ClassTag](\n      ratings: RDD[Rating[ID]],\n      srcPart: Partitioner,\n      dstPart: Partitioner): RDD[((Int, Int), RatingBlock[ID])] = {\n    //获得总block数\n    val numPartitions = srcPart.numPartitions * dstPart.numPartitions\n    //在rating的每个分区，计算每个rating元素对应的src_block_id和dst_block_id, 并放到对应的块索引中。然后，对所有分区的元素按照块索引进行聚合，并返回聚合结果\n    ratings.mapPartitions { iter =>\n      //生成numPartitions个一维数组，存储对应block的rating记录\n      val builders = Array.fill(numPartitions)(new RatingBlockBuilder[ID])\n      iter.flatMap { r =>\n        val srcBlockId = srcPart.getPartition(r.user) //user block id\n        val dstBlockId = dstPart.getPartition(r.item) //item block id\n        val idx = srcBlockId + srcPart.numPartitions * dstBlockId //数组索引计算\n        //将对应的rating元素放在builders对应元素中\n        val builder = builders(idx) \n        builder.add(r) \n        if (builder.size >= 2048) { // 2048 * (3 * 4) = 24k\n          //如果某个block内数据量较多，直接得到结果\n          builders(idx) = new RatingBlockBuilder\n          Iterator.single(((srcBlockId, dstBlockId), builder.build()))\n        } else {\n          Iterator.empty\n        }\n      } ++ {\n        //对builders数组内元素，计算对应的src_block_id和dst_block_id,并将对应rating数据放在其中\n        builders.view.zipWithIndex.filter(_._1.size > 0).map { case (block, idx) =>\n          val srcBlockId = idx % srcPart.numPartitions\n          val dstBlockId = idx / srcPart.numPartitions\n          ((srcBlockId, dstBlockId), block.build())\n        }\n      }\n    }.groupByKey().mapValues { blocks =>\n      //对不同分区计算出的的rating元素进行聚合\n      val builder = new RatingBlockBuilder[ID]\n      blocks.foreach(builder.merge)\n      builder.build() //value为 （src_id数组，dst_id数组，对应的rating数组）\n    }.setName(\"ratingBlocks\")\n  }\n\n```\n\n### 构造in_block, 和out_block\n  \n在分布式计算中，不同节点的通信是影响程序效率重要原因，通过合理的设计分区，使得不同节点交换数据尽量少，可以有效的提升运行效率。\n     \n由上述章节中对目标函数求解推导，可以得知，每个用户向量的计算依赖于所有和它关联的item向量。如果不做任何优化，则每次优化user向量时，所有user向量的计算，都需要从其他节点得到对应item向量。如果节点A上有多个user和节点B上的某一item关联，则节点B需要向节点A传输多次item向量数据，实际上这是不必要的。优化的思路是，通过合理的分区，提前计算好所有节点需要从其它节点获取的item向量数据，将其缓存在本地，计算每个user向量时，直接从本地读取，可以大大减少需要传输的数据量，提升程序执行的效率。\n     \n在源码中，通过out block缓存当前节点需要向其它节点传输的数据， in block用于缓存当前节点需要的数据索引。当其他节点信息传输到本地时，通过读取in block内索引信息，来从本地获取其它节点传过来的数据。更加详细的描述可参考【7】\n    \nin block 结构： （block_id, Inblock(src_id数组, src_ptr, dst_id地址数组， rating数组）)\nout block结构： （block_id， array[array[int]]） （二维数组存储发往每个block的src_id索引）\n\n    \n```scala\nprivate def makeBlocks[ID: ClassTag](\n      prefix: String,\n      ratingBlocks: RDD[((Int, Int), RatingBlock[ID])],\n      srcPart: Partitioner,\n      dstPart: Partitioner,\n      storageLevel: StorageLevel)(\n      implicit srcOrd: Ordering[ID]): (RDD[(Int, InBlock[ID])], RDD[(Int, OutBlock)]) = {\n    //根据ratingBlocks.map计算inBlocks\n    val inBlocks = ratingBlocks.map {\n      case ((srcBlockId, dstBlockId), RatingBlock(srcIds, dstIds, ratings)) =>\n        val start = System.nanoTime()\n        //dst id去重复\n        val dstIdSet = new OpenHashSet[ID](1 << 20) \n        dstIds.foreach(dstIdSet.add)  \n        //dst id 去重结果进行排序\n        val sortedDstIds = new Array[ID](dstIdSet.size)\n        var i = 0\n        var pos = dstIdSet.nextPos(0)\n        while (pos != -1) {\n          sortedDstIds(i) = dstIdSet.getValue(pos)\n          pos = dstIdSet.nextPos(pos + 1)\n          i += 1\n        }\n        assert(i == dstIdSet.size)\n        Sorting.quickSort(sortedDstIds)\n        //得到dst id 对应的去重和排序后的索引值\n        val dstIdToLocalIndex = new OpenHashMap[ID, Int](sortedDstIds.length)\n        i = 0\n        while (i < sortedDstIds.length) {\n          dstIdToLocalIndex.update(sortedDstIds(i), i)\n          i += 1\n        }\n        logDebug(\n          \"Converting to local indices took \" + (System.nanoTime() - start) / 1e9 + \" seconds.\")\n        val dstLocalIndices = dstIds.map(dstIdToLocalIndex.apply)\n        (srcBlockId, (dstBlockId, srcIds, dstLocalIndices, ratings))\n    }.groupByKey(new ALSPartitioner(srcPart.numPartitions)) //根据src block id进行聚合\n      .mapValues { iter =>\n        val builder =\n          new UncompressedInBlockBuilder[ID](new LocalIndexEncoder(dstPart.numPartitions))\n        //将dstBlockId和dstLocalIndices编码，并汇总数据\n        iter.foreach { case (dstBlockId, srcIds, dstLocalIndices, ratings) =>\n          builder.add(dstBlockId, srcIds, dstLocalIndices, ratings)\n        }\n        //对结果进行压缩存储，结果格式为（uniqueSrcId数组, dstPtrs数组, dstEncodedIndices数组, ratings数组）\n        builder.build().compress()\n      }.setName(prefix + \"InBlocks\")\n      .persist(storageLevel)\n        \n    //根据inBlocks计算outBlocks\n    val outBlocks = inBlocks.mapValues { case InBlock(srcIds, dstPtrs, dstEncodedIndices, _) =>\n      //构造编码器\n      val encoder = new LocalIndexEncoder(dstPart.numPartitions)\n      //定义ArrayBuilder数组，存储发往每个out block的 src id信息\n      val activeIds = Array.fill(dstPart.numPartitions)(mutable.ArrayBuilder.make[Int])\n      var i = 0\n      val seen = new Array[Boolean](dstPart.numPartitions)\n      //依次计算当前src id是否发往每一个block id\n      while (i < srcIds.length) {\n        var j = dstPtrs(i)\n        ju.Arrays.fill(seen, fALSe)\n        while (j < dstPtrs(i + 1)) {\n          val dstBlockId = encoder.blockId(dstEncodedIndices(j))\n          if (!seen(dstBlockId)) {\n            activeIds(dstBlockId) += i // add the local index in this out-block\n            seen(dstBlockId) = true\n          }\n          j += 1\n        }\n        i += 1\n      }\n      activeIds.map { x =>\n        x.result()\n      }\n    }.setName(prefix + \"OutBlocks\")\n      .persist(storageLevel)\n    (inBlocks, outBlocks)  //返回结果\n  }\n```\n\n#### inblock compress\n\n  对inblock 中间结果压缩存储，返回结果格式为（uniqueSrcId数组, dstPtrs数组, dstEncodedIndices数组, ratings数组）\n\n    \n```scala\ndef compress(): InBlock[ID] = {\n  val sz = length\n  assert(sz > 0, \"Empty in-link block should not exist.\")\n  sort()\n  val uniqueSrcIdsBuilder = mutable.ArrayBuilder.make[ID]\n  val dstCountsBuilder = mutable.ArrayBuilder.make[Int]\n  var preSrcId = srcIds(0)\n  uniqueSrcIdsBuilder += preSrcId\n  var curCount = 1\n  var i = 1\n  var j = 0\n  //得到去重后的src id数组， 以及每个src id的数量\n  while (i < sz) {\n    val srcId = srcIds(i)\n    if (srcId != preSrcId) {\n      uniqueSrcIdsBuilder += srcId\n      dstCountsBuilder += curCount\n      preSrcId = srcId\n      j += 1\n      curCount = 0\n    }\n    curCount += 1\n    i += 1\n  }\n  dstCountsBuilder += curCount\n  val uniqueSrcIds = uniqueSrcIdsBuilder.result()\n  val numUniqueSrdIds = uniqueSrcIds.length\n  val dstCounts = dstCountsBuilder.result()\n  val dstPtrs = new Array[Int](numUniqueSrdIds + 1)\n  var sum = 0\n  //将src id和dst id关系通过dstPtrs进行压缩存储\n  i = 0\n  while (i < numUniqueSrdIds) {\n    sum += dstCounts(i)\n    i += 1\n    dstPtrs(i) = sum\n  }\n  InBlock(uniqueSrcIds, dstPtrs, dstEncodedIndices, ratings)\n}\n```\n\n### computeFactor\n\n  根据srcFactorBlocks、srcOutBlocks、dstInBlocks, 计算dstFactorBlocks\n\n    \n```scala\nprivate def computeFactors[ID](\n    srcFactorBlocks: RDD[(Int, FactorBlock)],\n    srcOutBlocks: RDD[(Int, OutBlock)],\n    dstInBlocks: RDD[(Int, InBlock[ID])],\n    rank: Int,\n    regParam: Double,\n    srcEncoder: LocalIndexEncoder,\n    implicitPrefs: Boolean = fALSe,\n    alpha: Double = 1.0,\n    solver: LeastSquaresNESolver): RDD[(Int, FactorBlock)] = {\n  val numSrcBlocks = srcFactorBlocks.partitions.length  //src block数量\n  val YtY = if (implicitPrefs) Some(computeYtY(srcFactorBlocks, rank)) else None\n  //根据srcOut，得到每个dstBlock对应的srcBlockID 和srcFactor数组\n  val srcOut = srcOutBlocks.join(srcFactorBlocks).flatMap {\n    case (srcBlockId, (srcOutBlock, srcFactors)) =>\n      \n      srcOutBlock.view.zipWithIndex.map { case (activeIndices, dstBlockId) =>\n        (dstBlockId, (srcBlockId, activeIndices.map(idx => srcFactors(idx))))\n      }\n  }\n  //根据dstBlockId 对srcBlockID, array[srcFactor]进行聚合\n  val merged = srcOut.groupByKey(new ALSPartitioner(dstInBlocks.partitions.length))\n  //对每个dstBlockID, 计算其中每个dstID对应的隐语义向量\n  dstInBlocks.join(merged).mapValues {\n    case (InBlock(dstIds, srcPtrs, srcEncodedIndices, ratings), srcFactors) =>\n      //得到每个block对应的src factor向量集合\nval sortedSrcFactors = new Array[FactorBlock](numSrcBlocks)\n      srcFactors.foreach { case (srcBlockId, factors) =>\n        sortedSrcFactors(srcBlockId) = factors\n      }\n      //对每个dstID, 获取对应的srcFactor及对应rating, 计算该dstID对应的隐语义向量\n      val dstFactors = new Array[Array[Float]](dstIds.length)\n      var j = 0\n      val ls = new NormalEquation(rank)\n      while (j < dstIds.length) {\n        ls.reset()\n        if (implicitPrefs) {\n          ls.merge(YtY.get)\n        }\n        var i = srcPtrs(j)\n        var numExplicits = 0\n        while (i < srcPtrs(j + 1)) { //依次得到每个srcFactor及rating值\n          val encoded = srcEncodedIndices(i)\n          val blockId = srcEncoder.blockId(encoded)\n          val localIndex = srcEncoder.localIndex(encoded)\n          //sortedSrcFactors通过blockId和localIndex进行索引，得到需要的factor向量。之前这里困惑挺久，一直感觉从srcOut传过来的factor向量只是一个子集，通过localIndex访问不正确，实际上这里的localIndex和srcOut那里存储的localindex是不需要对应的。因为同一个src id 本身的src local index不等于其它block对应的 dst localindex\n          val srcFactor = sortedSrcFactors(blockId)(localIndex)\n          val rating = ratings(i)\n          if (implicitPrefs) {\n            // Extension to the original paper to handle b < 0. confidence is a function of |b|\n            // instead so that it is never negative. c1 is confidence - 1.0.\n            val c1 = alpha * math.abs(rating)\n            // For rating <= 0, the corresponding preference is 0. So the term below is only added\n            // for rating > 0. Because YtY is already added, we need to adjust the scaling here.\n            if (rating > 0) {\n              numExplicits += 1\n              ls.add(srcFactor, (c1 + 1.0) / c1, c1)\n            }\n          } else {\n            ls.add(srcFactor, rating)\n            numExplicits += 1\n          }\n          i += 1\n        }\n        // Weight lambda by the number of explicit ratings based on the ALS-WR paper.\n        dstFactors(j) = solver.solve(ls, numExplicits * regParam)\n        j += 1\n      }\n      dstFactors\n  }\n}\n```\n\n\n## ALS 模型推荐\n    \n**模型参数** \n\n```\n val rank: Int,      //隐语义个数\n val userFeatures: RDD[(Int, Array[Double])], //user factor数组, 存储user id 及对应的factor向量\n val productFeatures: RDD[(Int, Array[Double])]) //item factor数组，存储item id及对应的factor向量\n    \n```\n\n**对所有用户进行推荐**\n\n调用recommendForAll函数，首先对user向量和item向量分块并以矩阵形式存储，然后对二者做笛卡尔积，并计算每个user和每个item的得分，最终以user为key, 取topK个item及对应的得分，作为推荐结果. 计算topK时借助于小顶堆\n\n```\n  private def recommendForAll(\n      rank: Int,\n      srcFeatures: RDD[(Int, Array[Double])],\n      dstFeatures: RDD[(Int, Array[Double])],\n      num: Int): RDD[(Int, Array[(Int, Double)])] = {\n    //对user向量和item向量分块并以矩阵形式存储\n    val srcBlocks = blockify(rank, srcFeatures)\n    val dstBlocks = blockify(rank, dstFeatures)\n    //笛卡尔积，依次对每个组合计算user对item的偏好\n    val ratings = srcBlocks.cartesian(dstBlocks).flatMap {\n      case ((srcIds, srcFactors), (dstIds, dstFactors)) =>\n        val m = srcIds.length\n        val n = dstIds.length\n        val ratings = srcFactors.transpose.multiply(dstFactors)\n        val output = new Array[(Int, (Int, Double))](m * n)\n        var k = 0\n        ratings.foreachActive { (i, j, r) =>\n          output(k) = (srcIds(i), (dstIds(j), r))\n          k += 1\n        }\n        output.toSeq\n    }\n    //根据user id作为key, 得到喜好分数最高的num个item\n    ratings.topByKey(num)(Ordering.by(_._2))\n  }\n\n\n  // 对user向量和item向量分块并以矩阵形式存储, 结果的每个元组分别是对应的id数组和factor构成的矩阵\n  private def blockify(\n      rank: Int,\n      features: RDD[(Int, Array[Double])]): RDD[(Array[Int], DenseMatrix)] = {\n    val blockSize = 4096 // TODO: tune the block size\n    val blockStorage = rank * blockSize\n    features.mapPartitions { iter =>\n      iter.grouped(blockSize).map { grouped =>\n        val ids = mutable.ArrayBuilder.make[Int]\n        ids.sizeHint(blockSize)\n        val factors = mutable.ArrayBuilder.make[Double]\n        factors.sizeHint(blockStorage)\n        var i = 0\n        grouped.foreach { case (id, factor) =>\n          ids += id\n          factors ++= factor\n          i += 1\n        }\n        (ids.result(), new DenseMatrix(rank, i, factors.result()))\n      }\n    }\n  }\n\n```\n\n# ALS推荐实践\n\n我们的平台是图片社交，每个用户都可以在平台上浏览图片，并进行点赞、评论等。推荐算法主要用于给用户推荐其最可能感兴趣的图片，最终提升用户体验。\n\n## 离线实验\n\n我们平台暂时无法得到用户的显式评分数据，但是可以得到用户点击、点赞、评论等相关行为信息。因此，比较适合用隐反馈矩阵分解模型。\n\n### 构造数据集\n* 数据预处理\n  \n  从2周的用户行为数据中，过滤无行为用户数据，spam图片数据和spam用户数据。\n\n* 构建rating元素\n\n  对预处理之后的数据，根据用户每天的图片交互行为，分别对点击、点赞和评论等分别赋予不同的权值，得到rating矩阵. \n\n* 生成训练集和测试集\n\n  对于得到的rating数据，随机划分为两部分 $A:B = 7:3$，如果分别直接作为训练集和测试集是有问题的，因为$B$中的user或者item是有可能在$A$中没有出现过，这样会影响评估结果。 我们采用的方法是如果B数据中某个rating元素的user或item没有在A出现，则将该元素放到$A$中用作训练集。最终$A$和新加进来的元素共同构成训练集$A^1$， $B$留下的数据 $B^1$ 作为测试集。\n  \n\n### 离线训练和评估\n\n* 离线训练\n\n利用spark mllib库，对训练集构成的rating矩阵，建立隐反馈矩阵分解模型，并完成进行矩阵分解，生成user factor和item factor。\n\n* 评估\n\n调用模型的recommendForAll函数，对测试集所有user进行item推荐，并计算召回率和准确率。根据召回率和准确率，进行参数优化。\n\n* 评估指标\n\n假定$P_i$为用户$i$的预测结果，$P$为所有的预测结果，每个结果记录格式为（user, item）， $T$为测试集,每条记录格式为（user， item）。各种指标的的计算如下：\n\n召回率: $R= \\frac{|P \\bigcap T|}  {|T|}$\n\n准确率: $P= \\frac{|P \\bigcap T|}  {|P|}$\n\nF1:  $F= \\frac{2PR}  {P+R} $\n\n离散度：$\\frac{1}{N^2}\\sum\\_i\\sum\\_j\\frac{|P\\_i \\bigcap P\\_j|}{|P\\_i \\bigcup P\\_j|}$\n\n除了上述指标之外，我们还对用户连续多天推荐结果的差异性、用户覆盖率、图片覆盖率等指标进行评估。\n\n## 在线ab测试\n\nabtest方案： 将als算法计算出的结果，定期写入到线上，作为线上的一种推荐来源。对实验组用户同时采用新策略和旧策略进行推荐，对照组用户只采用旧策略进行推荐。\n\n从2个维度进行评估：\n\n* 评估实验组和对照组用户在abtest上线前后点击率\n* 评估实验组用户在新旧两种策略推荐图片的点击率\n\n测试一定时间后，交换对照组和实验组用户，按照上述2个维度重新进行评估\n\n\n\n\n\n# 参考文献\n\n【1】Y Koren，R Bell，C Volinsky, \"Matrix Factorization Techniques for Recommender Systems\", 《Computer》, 2009.08; 42(8):30-37 \n\n【2】洪亮劼, \"知人知面需知心——人工智能技术在推荐系统中的应用\", 2016.11, http://mp.weixin.qq.com/s/JuaM8d52-f8AzTjEPnCl7g\n\n【3】S. Funk, \"Netflix Update: Try This at Home\", 2006.12, http://sifter.org/~simon/journal/20061211.html\n\n【4】Y. Koren, \"Factorization Meets the Neighborhood: A Mul-tifaceted Collaborative Filtering Model\", Proc. 14th ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining, ACM Press, 2008, pp.426-434\n\n【5】A. Paterek, \"Improving Regularized Singular Value De-composition for Collaborative Filtering\" Proc. KDD Cup and Workshop, ACM Press, 2007, pp.39-42\n\n【6】G. Takács et al., \"Major Components of the Gravity Recom- mendation System\", SIGKDD Explorations, 2007.09, vol.9, pp.80-84\n\n【7】孟祥瑞, \"ALS 在 Spark MLlib 中的实现\", 2015.05, http://www.csdn.net/article/2015-05-07/2824641\n\n【8】Zhen-ming Yuan, et al., \"A microblog recommendation algorithm based on social tagging and a temporal interest evolution model\", Frontiers of Information Technology & Electronic Engineering, 2015.07,\nVolume 16, Issue 7, pp 532–540 \n\n【9】Z Zhao, Z Cheng, L Hong, EH Chi, \"Improving User Topic Interest Profiles by Behavior Factorization\", Proceedings of the 24th International Conference on World Wide Web, 2015.05, pp.1406-1416\r\r【10】阿里技术，\"淘宝搜索/推荐系统背后深度强化学习与自适应在线学习的实践之路\", 2017.02, http://url.cn/451740J\n\n【11】HT Cheng, L Koc, J Harmsen, T Shaked, \"Wide & Deep Learning for Recommender Systems\", Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, 2016.09,  pp.7-10\n\n【12】黄安埠, \"递归的艺术 - 深度递归网络在序列式推荐的应用\", 2016.10, http://mp.weixin.qq.com/s?__biz=MzA3MDQ4MzQzMg==&mid=2665690422&idx=1&sn=9bd671983a85286149b51c908b686899&chksm=842bb9b1b35c30a7eedb8d03e173aa8f43465db90e11075ac0c73b1784582f21eb93dcbd3e65&scene=0%23wechat_redirect\n","slug":"als","published":1,"updated":"2018-02-11T08:33:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjdikgud70004ga010cpm9k6g","content":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<p>ALS（alternating least squares）是一种基础的推荐算法，相对于普通的协同过滤等方法，它不仅能通过降维增加模型的泛化能力，也方便加入其他建模因素（如数据偏差、时间、隐反馈等），大大提升了模型的灵活性。正因为此，ALS算法在Netflix推荐大赛中脱颖而出，在我们具体的工程实践中，也具有非常不错的表现。接下来，从如下几个方面和大家一起学习：ALS算法模型、spark ALS源码理解， ALS推荐实践。如描述有误，欢迎大家指正。</p>\n<h1 id=\"ALS算法模型\"><a href=\"#ALS算法模型\" class=\"headerlink\" title=\"ALS算法模型\"></a>ALS算法模型</h1><h2 id=\"为什么要用ALS模型\"><a href=\"#为什么要用ALS模型\" class=\"headerlink\" title=\"为什么要用ALS模型\"></a>为什么要用ALS模型</h2><p> 相对于其他模型，ALS模型优势如下：</p>\n<ul>\n<li><p><strong>相对于基于内容的推荐</strong>，ALS属于协同过滤大家族【1】【12】（也有人认为ALS 基于矩阵分解技术，不属于协同过滤范畴【2】），<strong>直接跟进用户行为信息进行建模，不需要获取user和item的内容信息</strong>（很多情况下这些内容信息并不是很好获取，但是相对基于内容的推荐，ALS存在冷启动问题）</p>\n</li>\n<li><p><strong>相对于传统的协同过滤推荐方法（user based、item based）</strong>， ALS算法属于factor model, 通过将数据从原始空间映射到更低维度空间，<strong>去除噪声信息，利用更主要的语义信息对问题建模，能获得更好的推荐效果</strong>。</p>\n</li>\n<li><p><strong>相对于svd分解模型而言</strong>， 两种模型都属于 factor model, 但<strong>svd分解模型更倾向于解决矩阵元素没有缺失的情况， 而通过一定的方式去填充矩阵不仅需要额外的填充成本，填充本身可能影响了数据的真实性</strong>。因此，直接对已知元素进行建模，是一种不错的思路。如【1，3-6】，直接对rating矩阵已知元素$r_{ui}$进行建模:</p>\n</li>\n</ul>\n<center><br>$\\sum_{u,i\\in\\mathbb K} (r_{ui} -<br>p_u^Tq_i)^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$ （1）<br></center>\n\n<ul>\n<li>针对所建模型1可以用SGD或ALS 两种算法求解。其中<strong>sgd方法相对比较简单，但是当我们要建模的矩阵中已知元素较多时（如隐反馈），采用sgd在每次迭代都要求解所有元素，其时间复杂度是非常大的</strong>。ALS算法在求解某个user （或item）向量时，不依赖其他任何user（item）向量，这个性质使得<strong>ALS算法在每次迭代过程中方便并行化求解，在解决大规模矩阵分解问题时非常具有优势</strong>。 </li>\n</ul>\n<h2 id=\"ALS模型有什么缺点\"><a href=\"#ALS模型有什么缺点\" class=\"headerlink\" title=\"ALS模型有什么缺点\"></a>ALS模型有什么缺点</h2><p> 相对于其它推荐算法，ALS模型具有非常明显的优势：不需要对user和item信息进行建模，能够更加灵活地对各种因素建模，方便大规模并行计算。但ALS模型在如下几方面，又有自己的局限性：</p>\n<ul>\n<li><p>冷启动问题</p>\n<p>包括user的冷启动和item的冷启动。由于rating矩阵的构建，依赖user的显式和隐式反馈信息，对于新的user和item，或者没有相关行为的user或item, 导致无法构建rating矩阵，或者rating矩阵构建不合理。</p>\n<p>基于内容的推荐能较好地解决冷启动相关问题。如【8】在解决用户的冷启动问题时，首先根据用户之间社交的亲密度，对用户进行聚类，利用相同群体的用户画像来建模自身兴趣。同时，对于item的冷启动问题，可利用item本身对一些关键词、类目、内容等相关信息进行建模。【9】为了提高用户兴趣的准确率和覆盖率，在对用户兴趣建模对时候，将用户兴趣进行更具体的分类（如消费兴趣、生产兴趣、具体的每个行为兴趣等），并针对具体的业务，采用线性回归的方法对各种兴趣利用线性回归的方式进行加权求和，提升用户兴趣准确率和覆盖率。</p>\n</li>\n<li><p>用户临时兴趣</p>\n<p>用户的兴趣是在不断变换的。对于相对较稳定的兴趣，ALS算法可以通过引入时间因素进行建模，如公式4。 但对于临时的兴趣变换，ALS算法是无法捕获的。</p>\n<p><strong>一种简单且有效的方法</strong></p>\n<p>将item划分为多个类别，每个类别对应一种兴趣。用户每次点击某个类别的item之后，认为该用户存在一种临时兴趣，通过动态增加相应类别的比例的item，迎合用户当前的消费需求。该方法的难点在于如何调整比例，才能让用户感到有很多自己喜欢的item, 同时又不会让用户感觉内容的单调。</p>\n<p><strong>淘宝的一些实践</strong></p>\n<p>为了有效获取用户的即时兴趣，给用户推荐最合适的产品，淘宝进行了比较多的实践【10】，分别如下所述。</p>\n<ul>\n<li><p>GBDT+FTRL模型</p>\n<p>由于GBDT模型比较擅长挖具有区分度的特征，其使用GBDT模型进行特征挖掘，将得到的特征输送给FTRL进行在线学习。输送给GBDT的特征包括两部分：一部分用户基础行为的次数、CTR等；另一部分是来自match粗选阶段的的特征，该部分特征来自不同的粗选模型输出.</p>\n</li>\n<li><p>Wide &amp; Deep Learning模型</p>\n<p>借鉴google论文思想【11】，利用wide模型 + deep模型 + LR，其中wide子结构通过特征交叉学习特征间的共现，deep子结构则输入具有泛化能力的离散特征和连续特征，wide模型和deep模型学习到的结果，再利用LR模型预测相应的得分。</p>\n</li>\n<li><p>Adaptive-Online-Learning</p>\n<p>保留每一时刻学习到的模型，根据业务指标，得到每个模型等权重信息，融合出最优的结果。该方法能够比较好地综合利用用户长期喝短期兴趣。</p>\n</li>\n<li><p>Reinforcement Learning</p>\n<p>该方法思想是通过定义每个步骤的奖励，当用户每次到来的时候，根据用户的累积奖励值，进行个性化推荐。</p>\n</li>\n</ul>\n<p><strong>腾讯的LSTM实践</strong></p>\n<p>  为解决音乐的推荐问题，腾讯采用的是LSTM深度学习方法【12】，将用户听的歌曲序列，抽取特征输入到LSTM网络进行训练。为防止有些用户对应的歌曲序列较短问题，其对这些数据的训练采用特殊处理，相关数据缺失的序列不进行状态更新。同时，为加快训练速度，将每次权值的训练过程通过矩阵的方式实现并发计算。另外，为降低soft max过程时间复杂度，采用Hierarchical softmax过程替代普通的softmax。</p>\n</li>\n</ul>\n<h2 id=\"ALS模型是什么\"><a href=\"#ALS模型是什么\" class=\"headerlink\" title=\"ALS模型是什么\"></a>ALS模型是什么</h2><h3 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h3><p>ALS模型属于隐语义模型，通过对用户行为矩阵R进行矩阵分解，得到user factor向量矩阵P、item factor向量矩阵Q. </p>\n<p>$R = P^T Q$ 。其中R、$P^T$、$Q^T$矩阵的定义如表1-表3所示。</p>\n<p>潜在语义空间对应的各个factor代表不同的属性信息，user向量描述了user对各种属性的喜好程度，item向量描述了item所具备的各种属性强度，二者在潜在语义空间的相似度描述了user对item的喜好程度,在进行推荐时，根据该喜好程度计算推荐结果。</p>\n<center>表1: rating矩阵R</center>\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th>item1</th>\n<th>item2</th>\n<th>item3</th>\n<th>item4</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>user1</td>\n<td>$r_{11}$</td>\n<td>$r_{12}$</td>\n<td>$r_{13}$</td>\n<td>$r_{14}$</td>\n</tr>\n<tr>\n<td>user2</td>\n<td>$r_{21}$</td>\n<td>$r_{22}$</td>\n<td>$r_{23}$</td>\n<td>$r_{24}$</td>\n</tr>\n<tr>\n<td>user3</td>\n<td>$r_{31}$</td>\n<td>$r_{32}$</td>\n<td>$r_{33}$</td>\n<td>$r_{34}$</td>\n</tr>\n<tr>\n<td>user4</td>\n<td>$r_{41}$</td>\n<td>$r_{42}$</td>\n<td>$r_{43}$</td>\n<td>$r_{44}$</td>\n</tr>\n<tr>\n<td>user5</td>\n<td>$r_{51}$</td>\n<td>$r_{52}$</td>\n<td>$r_{53}$</td>\n<td>$r_{54}$</td>\n</tr>\n</tbody>\n</table>\n<center>表2：user矩阵$P^T$</center>\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th>factor1</th>\n<th>factor2</th>\n<th>factor3 </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>user1</td>\n<td>$p_{11}$</td>\n<td>$p_{12}$</td>\n<td>$p_{13}$</td>\n</tr>\n<tr>\n<td>user2</td>\n<td>$p_{21}$</td>\n<td>$p_{22}$</td>\n<td>$p_{23}$</td>\n</tr>\n<tr>\n<td>user3</td>\n<td>$p_{31}$</td>\n<td>$p_{32}$</td>\n<td>$p_{33}$</td>\n</tr>\n<tr>\n<td>user4</td>\n<td>$p_{41}$</td>\n<td>$p_{42}$</td>\n<td>$p_{43}$</td>\n</tr>\n<tr>\n<td>user5</td>\n<td>$p_{51}$</td>\n<td>$p_{52}$</td>\n<td>$p_{53}$</td>\n</tr>\n</tbody>\n</table>\n<center>表3:item矩阵$Q^T$</center>\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th>factor1</th>\n<th>factor2</th>\n<th>factor3 </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>item1</td>\n<td>$q_{11}$</td>\n<td>$q_{12}$</td>\n<td>$q_{13}$</td>\n</tr>\n<tr>\n<td>item2</td>\n<td>$q_{21}$</td>\n<td>$q_{22}$</td>\n<td>$q_{23}$</td>\n</tr>\n<tr>\n<td>item3</td>\n<td>$q_{31}$</td>\n<td>$q_{32}$</td>\n<td>$q_{33}$</td>\n</tr>\n<tr>\n<td>item4</td>\n<td>$q_{41}$</td>\n<td>$q_{42}$</td>\n<td>$q_{43}$</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h3><p>$MIN_{PQ} \\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$  (2)</p>\n<p>其中${(r_{ui} - p_u^Tq_i）}^2$ 目的在于最小化分解误差，$\\lambda(p_u^Tp_u+q_i^Tq_i)$ 为正则项。</p>\n<h3 id=\"目标函数求解\"><a href=\"#目标函数求解\" class=\"headerlink\" title=\"目标函数求解\"></a>目标函数求解</h3><p>由于目标函数中$p_u, q_i$都是未知变量，该问题是非凸的。当我们固定其中一个变量，解另外一个变量时，问题则变成凸问题，这是ALS求解的主要思想。在实际求解过程中分为如下几个步骤：</p>\n<ol>\n<li><p>随机初始化所有的变量$p_u, q_i$。</p>\n</li>\n<li><p>固定所有的$q_i$变量，求得$q_i$变量为当前值时$p_u$的最优值。</p>\n</li>\n<li><p>固定所有的$p_u$变量，求得$p_u$变量为当前值时$q_i$的最优值。</p>\n</li>\n<li><p>如果满足终止条件，则终止。否则，迭代执行2，3两步。</p>\n</li>\n</ol>\n<p>通过不断执行步骤2和步骤3，使得误差越来越小，直到收敛或达到指定次数而终止。通过可导函数性质我们知道，当对变量求导结果等于0当时候，函数可以取得极值。具体到公式2，固定一个变量，对另一变量求导结果等于0时，可以达到极小值。</p>\n<p>我们令$L = \\sum_{u,i\\in\\mathbb K} {(r_{ui} - p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>固定所有$q_i$, 对$p_u$求导</p>\n<p>$-\\frac{\\alpha L}{2\\alpha p_{uk}} = \\sum_{i} {q_{ik}(r_{ui} - p_u^Tq_i）} - \\lambda p_{uk} = 0$</p>\n<p>=&gt; $\\sum_{i} {q_{i}(r_{ui} - p_u^Tq_i）} - \\lambda p_{u} = 0$</p>\n<p>=&gt; $(\\sum_{i} {q_i q_i^T} + \\lambda E) p_u = \\sum_{i}q_i r_{ui}$</p>\n<p>=&gt; $p_u = (\\sum_{i} {q_i q_i^T} + \\lambda E)^{-1}\\sum_{i}q_i r_{ui}$</p>\n<p>=&gt; $ p_u = (Q_{u,i\\in\\mathbb K} Q_{u,i\\in\\mathbb K}^T + \\lambda E)^{-1}Q_{u,i\\in\\mathbb K}R_{u,i\\in\\mathbb K}^T$</p>\n<p>其中，$q_{u,i\\in\\mathbb K}$ 表示和user $u$有行为关联的item对应的向量矩阵，$r_{u,i\\in\\mathbb K}^T$表示和user $u$有行为关联的item对应rating元素构成的向量的转置。</p>\n<p><strong>更加灵活的ALS建模</strong></p>\n<p>相对于传统的协同协同过滤方法，ALS能更好的考虑其他因素，如数据偏差、时间等</p>\n<ol>\n<li><p>引入数据偏差</p>\n<p> user偏差：不同的用户，可能具有不同的评分标准。如用户在给item打分时，有的用户可能可能更倾向于给所有item打高分， 而有的挑剔用户会给所有item打分偏低</p>\n<p> item偏差：有的热门item可能所有用户都会倾向于打高分，而有的item可能本身大多数人会倾向于打低分</p>\n<p> 考虑use和item偏差的ALS建模：<br> $MIN_{PQB} \\sum_{u,i\\in\\mathbb K} {(r_{ui} - u - b_u - b_i-<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i+b_u^2+b_i^2)$  (3)</p>\n</li>\n<li><p>引入时间因素</p>\n<p> 用户偏好、rating矩阵，都可能随时间变化，item对应的属性不随时间变化，因此可进行如下建模<br>$MIN_{PQB} \\sum_{u,i\\in\\mathbb K} {(r_{ui}（t） - u - b_u(t) - b_i(t)-<br>p_u(t)^Tq_i）}^2 + \\lambda(p_u(t)^Tp_u(t)+q_i^Tq_i+b_u(t)^2+b_i(t)^2)$  (4)</p>\n</li>\n<li><p>引入隐反馈数据因素</p>\n<p> 很多时候，并没有用户对item明确的打分数据，此时可通过搜集用户隐反馈数据（浏览、点击、点赞等），进行隐反馈建模。有一点需要注意，此时不只是对$r_{ui}$大于0对用户行为建模，而是所有$r_{ui}$元素建模。模型如公式5所示：</p>\n<p> $MIN_{PQB} \\sum_{u,i} {c_{ui}(p_{ui} - u - b_u - b_i-<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i+b_u^2+b_i^2)$  (5)</p>\n<p> $p_{ui}$ 表示user u是否有相关行为表示喜欢item i, $c_{ui}$描述user u 对item i的喜欢程度，其定义如公式6和公式7所示</p>\n<p> $<br>p_{ui} =<br>\\begin{cases}<br>1,  &amp; r_{ui}&gt;0\\\\<br>0,  &amp; r_{ui}=0<br>\\end{cases}<br>$（6）</p>\n<p> $c_{ui} = 1 + \\alpha r_{ui}$（7）</p>\n</li>\n</ol>\n<h1 id=\"spark-ALS源码理解\"><a href=\"#spark-ALS源码理解\" class=\"headerlink\" title=\"spark ALS源码理解\"></a>spark ALS源码理解</h1><p>为加深对ALS算法的理解，该部分主要分析spark mllib中ALS源码的实现，大体上分为2部分：ALS模型训练、ALS模型推荐</p>\n<h2 id=\"ALS-模型训练\"><a href=\"#ALS-模型训练\" class=\"headerlink\" title=\"ALS 模型训练\"></a>ALS 模型训练</h2><h3 id=\"ALS-伴生类\"><a href=\"#ALS-伴生类\" class=\"headerlink\" title=\"ALS 伴生类\"></a>ALS 伴生类</h3><p>ALS 伴生对象提供外部调用 ALS模型训练的入口。通过传入相关参数， 返回训练好的模型对象MatrixFactorizationModel。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">ALS</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span></span>(</span><br><span class=\"line\">      ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>], <span class=\"comment\">//rating元素 （user, item, rate）</span></span><br><span class=\"line\">      rank: <span class=\"type\">Int</span>, <span class=\"comment\">//隐语义个数</span></span><br><span class=\"line\">      iterations: <span class=\"type\">Int</span>, <span class=\"comment\">//迭代次数</span></span><br><span class=\"line\">      lambda: <span class=\"type\">Double</span>, <span class=\"comment\">//正则惩罚项</span></span><br><span class=\"line\">      blocks: <span class=\"type\">Int</span>, <span class=\"comment\">//数据block个数</span></span><br><span class=\"line\">      seed: <span class=\"type\">Long</span> <span class=\"comment\">//随机数种子</span></span><br><span class=\"line\">    ): <span class=\"type\">MatrixFactorizationModel</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ALS</span>(blocks, blocks, rank, iterations, lambda, fALSe, <span class=\"number\">1.0</span>, seed).run(ratings)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trainImplicit</span></span>(</span><br><span class=\"line\">      ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>], <span class=\"comment\">// rating元素 （user, item, rate）</span></span><br><span class=\"line\">      rank: <span class=\"type\">Int</span>, <span class=\"comment\">//隐语义个数</span></span><br><span class=\"line\">      iterations: <span class=\"type\">Int</span>, <span class=\"comment\">//迭代次数</span></span><br><span class=\"line\">      lambda: <span class=\"type\">Double</span>, <span class=\"comment\">//正则惩罚项</span></span><br><span class=\"line\">      blocks: <span class=\"type\">Int</span>, <span class=\"comment\">//数据block个数</span></span><br><span class=\"line\">      alpha: <span class=\"type\">Double</span> <span class=\"comment\">//计算$c_ui$时用的alpha参数</span></span><br><span class=\"line\">    ): <span class=\"type\">MatrixFactorizationModel</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ALS</span>(blocks, blocks, rank, iterations, lambda, <span class=\"literal\">true</span>, alpha).run(ratings)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   <span class=\"comment\">//另外还有一些其他接口，因最终都通过调用上面2个函数，此处将其省略</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"ALS-私有类\"><a href=\"#ALS-私有类\" class=\"headerlink\" title=\"ALS 私有类\"></a>ALS 私有类</h3><p>定义了ALS类对应的各个参数，以及各个参数的设定方法。并定义了run方法供伴随类进行调用，该方法返回训练结果MatrixFactorizationModel给ALS伴随类。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ALS</span> <span class=\"title\">private</span> (<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var numUserBlocks: <span class=\"type\">Int</span>, //用户数据block个数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var numProductBlocks: <span class=\"type\">Int</span>, //item数据block个数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var rank: <span class=\"type\">Int</span>, //隐语义个数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var iterations: <span class=\"type\">Int</span>, //迭代次数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var lambda: <span class=\"type\">Double</span>, //正则惩罚项</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var implicitPrefs: <span class=\"type\">Boolean</span>, //是否使用隐反馈模型</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var alpha: <span class=\"type\">Double</span>, //计算$c_ui$时用的alpha参数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var seed: <span class=\"type\">Long</span> = <span class=\"type\">System</span>.nanoTime(</span>) <span class=\"title\">//随机数种子</span>,<span class=\"title\">默认为当前时间戳</span></span></span><br><span class=\"line\"><span class=\"class\">  ) <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> <span class=\"keyword\">with</span> <span class=\"title\">Logging</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">//设置block个数</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">setBlocks</span></span>(numBlocks: <span class=\"type\">Int</span>): <span class=\"keyword\">this</span>.<span class=\"keyword\">type</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.numUserBlocks = numBlocks</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.numProductBlocks = numBlocks</span><br><span class=\"line\">    <span class=\"keyword\">this</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"comment\">// 另外对其他参数变量也有相关函数实现，因基本都是赋值操作，此处将其省略</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"comment\">//run方法，通过输入rating数据，完成训练兵返回结果MatrixFactorizationModel</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>(ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>]): <span class=\"type\">MatrixFactorizationModel</span> = &#123;</span><br><span class=\"line\">    require(!ratings.isEmpty(), <span class=\"string\">s\"No ratings available from <span class=\"subst\">$ratings</span>\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = ratings.context</span><br><span class=\"line\">    <span class=\"comment\">//设置user block个数</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> numUserBlocks = <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.numUserBlocks == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">      math.max(sc.defaultParallelism, ratings.partitions.length / <span class=\"number\">2</span>)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">this</span>.numUserBlocks</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//设置item block个数</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> numProductBlocks = <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.numProductBlocks == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">      math.max(sc.defaultParallelism, ratings.partitions.length / <span class=\"number\">2</span>)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">this</span>.numProductBlocks</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//调用NewALS.train方法完成矩阵分解，生成user factor和item factor向量,该方法是整个ALS算法的核心实现</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> (floatUserFactors, floatProdFactors) = <span class=\"type\">NewALS</span>.train[<span class=\"type\">Int</span>](</span><br><span class=\"line\">      ratings = ratings.map(r =&gt; <span class=\"type\">NewALS</span>.<span class=\"type\">Rating</span>(r.user, r.product, r.rating.toFloat)),</span><br><span class=\"line\">      rank = rank,</span><br><span class=\"line\">      numUserBlocks = numUserBlocks,</span><br><span class=\"line\">      numItemBlocks = numProductBlocks,</span><br><span class=\"line\">      maxIter = iterations,</span><br><span class=\"line\">      regParam = lambda,</span><br><span class=\"line\">      implicitPrefs = implicitPrefs,</span><br><span class=\"line\">      alpha = alpha,</span><br><span class=\"line\">      nonnegative = nonnegative,</span><br><span class=\"line\">      intermediateRDDStorageLevel = intermediateRDDStorageLevel,</span><br><span class=\"line\">      finalRDDStorageLevel = <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>,</span><br><span class=\"line\">      checkpointInterval = checkpointInterval,</span><br><span class=\"line\">      seed = seed)</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">val</span> userFactors = floatUserFactors</span><br><span class=\"line\">      .mapValues(_.map(_.toDouble))</span><br><span class=\"line\">      .setName(<span class=\"string\">\"users\"</span>)</span><br><span class=\"line\">      .persist(finalRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> prodFactors = floatProdFactors</span><br><span class=\"line\">      .mapValues(_.map(_.toDouble))</span><br><span class=\"line\">      .setName(<span class=\"string\">\"products\"</span>)</span><br><span class=\"line\">      .persist(finalRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (finalRDDStorageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;</span><br><span class=\"line\">      userFactors.count()</span><br><span class=\"line\">      prodFactors.count()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//生成和返回ALS模型 MatrixFactorizationModel</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">MatrixFactorizationModel</span>(rank, userFactors, prodFactors)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"NewALS-train方法\"><a href=\"#NewALS-train方法\" class=\"headerlink\" title=\"NewALS.train方法\"></a>NewALS.train方法</h3><p>被ALS私有类的run方法调用，用于计算user factor和item factor向量。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span></span>[<span class=\"type\">ID</span>: <span class=\"type\">ClassTag</span>]( <span class=\"comment\">// scalastyle:ignore</span></span><br><span class=\"line\">      ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>[<span class=\"type\">ID</span>]],</span><br><span class=\"line\">      rank: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      numUserBlocks: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      numItemBlocks: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      maxIter: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      regParam: <span class=\"type\">Double</span> = <span class=\"number\">1.0</span>,</span><br><span class=\"line\">      implicitPrefs: <span class=\"type\">Boolean</span> = fALSe,</span><br><span class=\"line\">      alpha: <span class=\"type\">Double</span> = <span class=\"number\">1.0</span>,</span><br><span class=\"line\">      nonnegative: <span class=\"type\">Boolean</span> = fALSe,</span><br><span class=\"line\">      intermediateRDDStorageLevel: <span class=\"type\">StorageLevel</span> = <span class=\"type\">StorageLevel</span>.<span class=\"type\">MEMORY_AND_DISK</span>,</span><br><span class=\"line\">      finalRDDStorageLevel: <span class=\"type\">StorageLevel</span> = <span class=\"type\">StorageLevel</span>.<span class=\"type\">MEMORY_AND_DISK</span>,</span><br><span class=\"line\">      checkpointInterval: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      seed: <span class=\"type\">Long</span> = <span class=\"number\">0</span>L)(</span><br><span class=\"line\">      <span class=\"keyword\">implicit</span> ord: <span class=\"type\">Ordering</span>[<span class=\"type\">ID</span>]): (<span class=\"type\">RDD</span>[(<span class=\"type\">ID</span>, <span class=\"type\">Array</span>[<span class=\"type\">Float</span>])], <span class=\"type\">RDD</span>[(<span class=\"type\">ID</span>, <span class=\"type\">Array</span>[<span class=\"type\">Float</span>])]) = &#123;</span><br><span class=\"line\">    require(!ratings.isEmpty(), <span class=\"string\">s\"No ratings available from <span class=\"subst\">$ratings</span>\"</span>)</span><br><span class=\"line\">    require(intermediateRDDStorageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>,</span><br><span class=\"line\">      <span class=\"string\">\"ALS is not designed to run without persisting intermediate RDDs.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = ratings.sparkContext</span><br><span class=\"line\">    <span class=\"comment\">//根据block个数，构建哈稀器。</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> userPart = <span class=\"keyword\">new</span> <span class=\"type\">ALSPartitioner</span>(numUserBlocks)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> itemPart = <span class=\"keyword\">new</span> <span class=\"type\">ALSPartitioner</span>(numItemBlocks)</span><br><span class=\"line\">    <span class=\"comment\">//构建索引编码器，根据block编号和block内索引进行编码，同时可将编码后结果快速解码为block编号和block内索引号。具体实现是通过block个数，确定block编码需要的二进制位数，以及block内索引位数，通过这些位数利用逻辑操作即可实现编码和解码</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> userLocalIndexEncoder = <span class=\"keyword\">new</span> <span class=\"type\">LocalIndexEncoder</span>(userPart.numPartitions)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> itemLocalIndexEncoder = <span class=\"keyword\">new</span> <span class=\"type\">LocalIndexEncoder</span>(itemPart.numPartitions)</span><br><span class=\"line\">    <span class=\"comment\">//构建求解器</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> solver = <span class=\"keyword\">if</span> (nonnegative) <span class=\"keyword\">new</span> <span class=\"type\">NNLSSolver</span> <span class=\"keyword\">else</span> <span class=\"keyword\">new</span> <span class=\"type\">CholeskySolver</span></span><br><span class=\"line\">    <span class=\"comment\">//对rating矩阵进行分块，得到((user_blockID, item_blockID),rating(user, item, rating))</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockRatings = partitionRatings(ratings, userPart, itemPart)</span><br><span class=\"line\">      .persist(intermediateRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"comment\">//构建user inblock和outblock数据，inblock数据记录每个user对应的所有item的地址，及对应rating信息。 outblock记录当前block的哪些user数据会被哪些block用上</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> (userInBlocks, userOutBlocks) =</span><br><span class=\"line\">      makeBlocks(<span class=\"string\">\"user\"</span>, blockRatings, userPart, itemPart, intermediateRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"comment\">// materialize blockRatings and user blocks</span></span><br><span class=\"line\">    userOutBlocks.count()</span><br><span class=\"line\">    <span class=\"comment\">//交换blockrating中的user, item数据，用于构造item的inblcok和outblock信息</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> swappedBlockRatings = blockRatings.map &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> ((userBlockId, itemBlockId), <span class=\"type\">RatingBlock</span>(userIds, itemIds, localRatings)) =&gt;</span><br><span class=\"line\">        ((itemBlockId, userBlockId), <span class=\"type\">RatingBlock</span>(itemIds, userIds, localRatings))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> (itemInBlocks, itemOutBlocks) =</span><br><span class=\"line\">      makeBlocks(<span class=\"string\">\"item\"</span>, swappedBlockRatings, itemPart, userPart, intermediateRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"comment\">// materialize item blocks</span></span><br><span class=\"line\">    itemOutBlocks.count()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> seedGen = <span class=\"keyword\">new</span> <span class=\"type\">XORShiftRandom</span>(seed)</span><br><span class=\"line\">    <span class=\"comment\">//随机初始化user factor和item factor</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> userFactors = initialize(userInBlocks, rank, seedGen.nextLong())</span><br><span class=\"line\">    <span class=\"keyword\">var</span> itemFactors = initialize(itemInBlocks, rank, seedGen.nextLong())</span><br><span class=\"line\">    <span class=\"keyword\">var</span> previousCheckpointFile: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> shouldCheckpoint: <span class=\"type\">Int</span> =&gt; <span class=\"type\">Boolean</span> = (iter) =&gt;</span><br><span class=\"line\">      sc.checkpointDir.isDefined &amp;&amp; checkpointInterval != <span class=\"number\">-1</span> &amp;&amp; (iter % checkpointInterval == <span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deletePreviousCheckpointFile: () =&gt; <span class=\"type\">Unit</span> = () =&gt;</span><br><span class=\"line\">      previousCheckpointFile.foreach &#123; file =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> checkpointFile = <span class=\"keyword\">new</span> <span class=\"type\">Path</span>(file)</span><br><span class=\"line\">          checkpointFile.getFileSystem(sc.hadoopConfiguration).delete(checkpointFile, <span class=\"literal\">true</span>)</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">          <span class=\"keyword\">case</span> e: <span class=\"type\">IOException</span> =&gt;</span><br><span class=\"line\">            logWarning(<span class=\"string\">s\"Cannot delete checkpoint file <span class=\"subst\">$file</span>:\"</span>, e)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    <span class=\"comment\">//针对隐反馈，迭代求解</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (implicitPrefs) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">for</span> (iter &lt;- <span class=\"number\">1</span> to maxIter) &#123;  <span class=\"comment\">//迭代总次数maxIter</span></span><br><span class=\"line\">        userFactors.setName(<span class=\"string\">s\"userFactors-<span class=\"subst\">$iter</span>\"</span>).persist(intermediateRDDStorageLevel)</span><br><span class=\"line\">        <span class=\"keyword\">val</span> previousItemFactors = itemFactors</span><br><span class=\"line\">        <span class=\"comment\">//固定user factor，优化item factor</span></span><br><span class=\"line\">        itemFactors = computeFactors(userFactors, userOutBlocks, itemInBlocks, rank, regParam,</span><br><span class=\"line\">          userLocalIndexEncoder, implicitPrefs, alpha, solver)</span><br><span class=\"line\">        previousItemFactors.unpersist()</span><br><span class=\"line\">        itemFactors.setName(<span class=\"string\">s\"itemFactors-<span class=\"subst\">$iter</span>\"</span>).persist(intermediateRDDStorageLevel)</span><br><span class=\"line\">        <span class=\"comment\">// <span class=\"doctag\">TODO:</span> Generalize PeriodicGraphCheckpointer and use it here.</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> deps = itemFactors.dependencies</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (shouldCheckpoint(iter)) &#123;</span><br><span class=\"line\">          itemFactors.checkpoint() <span class=\"comment\">// itemFactors gets materialized in computeFactors</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> previousUserFactors = userFactors</span><br><span class=\"line\">        <span class=\"comment\">//根据item factore, 优化user factor</span></span><br><span class=\"line\">        userFactors = computeFactors(itemFactors, itemOutBlocks, userInBlocks, rank, regParam,</span><br><span class=\"line\">          itemLocalIndexEncoder, implicitPrefs, alpha, solver)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (shouldCheckpoint(iter)) &#123;</span><br><span class=\"line\">          <span class=\"type\">ALS</span>.cleanShuffleDependencies(sc, deps)</span><br><span class=\"line\">          deletePreviousCheckpointFile()</span><br><span class=\"line\">          previousCheckpointFile = itemFactors.getCheckpointFile</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        previousUserFactors.unpersist()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">//针对显示反馈，迭代求解</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> (iter &lt;- <span class=\"number\">0</span> until maxIter) &#123; <span class=\"comment\">//迭代总次数maxIter</span></span><br><span class=\"line\">        <span class=\"comment\">//固定user factor，优化item factor</span></span><br><span class=\"line\">        itemFactors = computeFactors(userFactors, userOutBlocks, itemInBlocks, rank, regParam,</span><br><span class=\"line\">          userLocalIndexEncoder, solver = solver)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (shouldCheckpoint(iter)) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> deps = itemFactors.dependencies</span><br><span class=\"line\">          itemFactors.checkpoint()</span><br><span class=\"line\">          itemFactors.count() <span class=\"comment\">// checkpoint item factors and cut lineage</span></span><br><span class=\"line\">          <span class=\"type\">ALS</span>.cleanShuffleDependencies(sc, deps)</span><br><span class=\"line\">          deletePreviousCheckpointFile()</span><br><span class=\"line\">          previousCheckpointFile = itemFactors.getCheckpointFile</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//根据item factore, 优化user factor</span></span><br><span class=\"line\">        userFactors = computeFactors(itemFactors, itemOutBlocks, userInBlocks, rank, regParam,</span><br><span class=\"line\">          itemLocalIndexEncoder, solver = solver)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//将user id 和 factor拼接在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> userIdAndFactors = userInBlocks</span><br><span class=\"line\">      .mapValues(_.srcIds)</span><br><span class=\"line\">      .join(userFactors)</span><br><span class=\"line\">      .mapPartitions(&#123; items =&gt;</span><br><span class=\"line\">        items.flatMap &#123; <span class=\"keyword\">case</span> (_, (ids, factors)) =&gt;</span><br><span class=\"line\">          ids.view.zip(factors)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      <span class=\"comment\">// Preserve the partitioning because IDs are consistent with the partitioners in userInBlocks</span></span><br><span class=\"line\">      <span class=\"comment\">// and userFactors.</span></span><br><span class=\"line\">      &#125;, preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      .setName(<span class=\"string\">\"userFactors\"</span>)</span><br><span class=\"line\">      .persist(finalRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"comment\">//将item id 和 factor拼接在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> itemIdAndFactors = itemInBlocks</span><br><span class=\"line\">      .mapValues(_.srcIds)</span><br><span class=\"line\">      .join(itemFactors)</span><br><span class=\"line\">      .mapPartitions(&#123; items =&gt;</span><br><span class=\"line\">        items.flatMap &#123; <span class=\"keyword\">case</span> (_, (ids, factors)) =&gt;</span><br><span class=\"line\">          ids.view.zip(factors)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;, preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      .setName(<span class=\"string\">\"itemFactors\"</span>)</span><br><span class=\"line\">      .persist(finalRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (finalRDDStorageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;</span><br><span class=\"line\">      userIdAndFactors.count()</span><br><span class=\"line\">      itemFactors.unpersist()</span><br><span class=\"line\">      itemIdAndFactors.count()</span><br><span class=\"line\">      userInBlocks.unpersist()</span><br><span class=\"line\">      userOutBlocks.unpersist()</span><br><span class=\"line\">      itemInBlocks.unpersist()</span><br><span class=\"line\">      itemOutBlocks.unpersist()</span><br><span class=\"line\">      blockRatings.unpersist()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//返回user factor和item factor数据</span></span><br><span class=\"line\">    (userIdAndFactors, itemIdAndFactors)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"构建哈希器\"><a href=\"#构建哈希器\" class=\"headerlink\" title=\"构建哈希器\"></a>构建哈希器</h3><p>   构建哈希器，用于计算user或item id对应的block编号。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HashPartitioner</span>(<span class=\"params\">partitions: <span class=\"type\">Int</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span> </span>&#123;</span><br><span class=\"line\">  require(partitions &gt;= <span class=\"number\">0</span>, <span class=\"string\">s\"Number of partitions (<span class=\"subst\">$partitions</span>) cannot be negative.\"</span>)</span><br><span class=\"line\">  <span class=\"comment\">//block总数</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">numPartitions</span></span>: <span class=\"type\">Int</span> = partitions</span><br><span class=\"line\">  <span class=\"comment\">//通过求余计算block 编号</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartition</span></span>(key: <span class=\"type\">Any</span>): <span class=\"type\">Int</span> = key <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"literal\">null</span> =&gt; <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">case</span> _ =&gt; <span class=\"type\">Utils</span>.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">//判断2个哈希器是否相等</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">equALS</span></span>(other: <span class=\"type\">Any</span>): <span class=\"type\">Boolean</span> = other <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> h: <span class=\"type\">HashPartitioner</span> =&gt;</span><br><span class=\"line\">      h.numPartitions == numPartitions</span><br><span class=\"line\">    <span class=\"keyword\">case</span> _ =&gt;</span><br><span class=\"line\">      fALSe</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hashCode</span></span>: <span class=\"type\">Int</span> = numPartitions</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"构建地址编码解码器\"><a href=\"#构建地址编码解码器\" class=\"headerlink\" title=\"构建地址编码解码器\"></a>构建地址编码解码器</h3><p>构建地址编码解码器，根据block编号和block内索引对地址进行编码，同时可将编码后地址解码为block编号和block内索引号。具体实现是通过block个数确定block编码需要的二进制位数，以及block内索引位数，通过这些位数利用逻辑操作即可实现地址的编码和解码。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[recommendation] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LocalIndexEncoder</span>(<span class=\"params\">numBlocks: <span class=\"type\">Int</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    require(numBlocks &gt; <span class=\"number\">0</span>, <span class=\"string\">s\"numBlocks must be positive but found <span class=\"subst\">$numBlocks</span>.\"</span>)</span><br><span class=\"line\">    <span class=\"comment\">//block内部索引使用的二进制位数</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">final</span> <span class=\"keyword\">val</span> numLocalIndexBits =</span><br><span class=\"line\">      math.min(java.lang.<span class=\"type\">Integer</span>.numberOfLeadingZeros(numBlocks - <span class=\"number\">1</span>), <span class=\"number\">31</span>)</span><br><span class=\"line\">    <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">final</span> <span class=\"keyword\">val</span> localIndexMask = (<span class=\"number\">1</span> &lt;&lt; numLocalIndexBits) - <span class=\"number\">1</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//根据block编号和block内索引值，对地址编码</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encode</span></span>(blockId: <span class=\"type\">Int</span>, localIndex: <span class=\"type\">Int</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">      require(blockId &lt; numBlocks)</span><br><span class=\"line\">      require((localIndex &amp; ~localIndexMask) == <span class=\"number\">0</span>)</span><br><span class=\"line\">      (blockId &lt;&lt; numLocalIndexBits) | localIndex</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//根据编码后地址，得到block编号</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">blockId</span></span>(encoded: <span class=\"type\">Int</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">      encoded &gt;&gt;&gt; numLocalIndexBits</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//根据编码地址，得到block内部索引</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">localIndex</span></span>(encoded: <span class=\"type\">Int</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">      encoded &amp; localIndexMask</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"partition-rating\"><a href=\"#partition-rating\" class=\"headerlink\" title=\"partition rating\"></a>partition rating</h3><p>格式化rating数据，将rating数据分块，根据user和product的id哈希后的结果，得到对应的块索引。最终返回（src_block_id, dst_block_id）(src_id数组，dst_id数组，rating数组)</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionRatings</span></span>[<span class=\"type\">ID</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">      ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>[<span class=\"type\">ID</span>]],</span><br><span class=\"line\">      srcPart: <span class=\"type\">Partitioner</span>,</span><br><span class=\"line\">      dstPart: <span class=\"type\">Partitioner</span>): <span class=\"type\">RDD</span>[((<span class=\"type\">Int</span>, <span class=\"type\">Int</span>), <span class=\"type\">RatingBlock</span>[<span class=\"type\">ID</span>])] = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//获得总block数</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> numPartitions = srcPart.numPartitions * dstPart.numPartitions</span><br><span class=\"line\">    <span class=\"comment\">//在rating的每个分区，计算每个rating元素对应的src_block_id和dst_block_id, 并放到对应的块索引中。然后，对所有分区的元素按照块索引进行聚合，并返回聚合结果</span></span><br><span class=\"line\">    ratings.mapPartitions &#123; iter =&gt;</span><br><span class=\"line\">      <span class=\"comment\">//生成numPartitions个一维数组，存储对应block的rating记录</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> builders = <span class=\"type\">Array</span>.fill(numPartitions)(<span class=\"keyword\">new</span> <span class=\"type\">RatingBlockBuilder</span>[<span class=\"type\">ID</span>])</span><br><span class=\"line\">      iter.flatMap &#123; r =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> srcBlockId = srcPart.getPartition(r.user) <span class=\"comment\">//user block id</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> dstBlockId = dstPart.getPartition(r.item) <span class=\"comment\">//item block id</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> idx = srcBlockId + srcPart.numPartitions * dstBlockId <span class=\"comment\">//数组索引计算</span></span><br><span class=\"line\">        <span class=\"comment\">//将对应的rating元素放在builders对应元素中</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> builder = builders(idx) </span><br><span class=\"line\">        builder.add(r) </span><br><span class=\"line\">        <span class=\"keyword\">if</span> (builder.size &gt;= <span class=\"number\">2048</span>) &#123; <span class=\"comment\">// 2048 * (3 * 4) = 24k</span></span><br><span class=\"line\">          <span class=\"comment\">//如果某个block内数据量较多，直接得到结果</span></span><br><span class=\"line\">          builders(idx) = <span class=\"keyword\">new</span> <span class=\"type\">RatingBlockBuilder</span></span><br><span class=\"line\">          <span class=\"type\">Iterator</span>.single(((srcBlockId, dstBlockId), builder.build()))</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">          <span class=\"type\">Iterator</span>.empty</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; ++ &#123;</span><br><span class=\"line\">        <span class=\"comment\">//对builders数组内元素，计算对应的src_block_id和dst_block_id,并将对应rating数据放在其中</span></span><br><span class=\"line\">        builders.view.zipWithIndex.filter(_._1.size &gt; <span class=\"number\">0</span>).map &#123; <span class=\"keyword\">case</span> (block, idx) =&gt;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> srcBlockId = idx % srcPart.numPartitions</span><br><span class=\"line\">          <span class=\"keyword\">val</span> dstBlockId = idx / srcPart.numPartitions</span><br><span class=\"line\">          ((srcBlockId, dstBlockId), block.build())</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.groupByKey().mapValues &#123; blocks =&gt;</span><br><span class=\"line\">      <span class=\"comment\">//对不同分区计算出的的rating元素进行聚合</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> builder = <span class=\"keyword\">new</span> <span class=\"type\">RatingBlockBuilder</span>[<span class=\"type\">ID</span>]</span><br><span class=\"line\">      blocks.foreach(builder.merge)</span><br><span class=\"line\">      builder.build() <span class=\"comment\">//value为 （src_id数组，dst_id数组，对应的rating数组）</span></span><br><span class=\"line\">    &#125;.setName(<span class=\"string\">\"ratingBlocks\"</span>)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"构造in-block-和out-block\"><a href=\"#构造in-block-和out-block\" class=\"headerlink\" title=\"构造in_block, 和out_block\"></a>构造in_block, 和out_block</h3><p>在分布式计算中，不同节点的通信是影响程序效率重要原因，通过合理的设计分区，使得不同节点交换数据尽量少，可以有效的提升运行效率。</p>\n<p>由上述章节中对目标函数求解推导，可以得知，每个用户向量的计算依赖于所有和它关联的item向量。如果不做任何优化，则每次优化user向量时，所有user向量的计算，都需要从其他节点得到对应item向量。如果节点A上有多个user和节点B上的某一item关联，则节点B需要向节点A传输多次item向量数据，实际上这是不必要的。优化的思路是，通过合理的分区，提前计算好所有节点需要从其它节点获取的item向量数据，将其缓存在本地，计算每个user向量时，直接从本地读取，可以大大减少需要传输的数据量，提升程序执行的效率。</p>\n<p>在源码中，通过out block缓存当前节点需要向其它节点传输的数据， in block用于缓存当前节点需要的数据索引。当其他节点信息传输到本地时，通过读取in block内索引信息，来从本地获取其它节点传过来的数据。更加详细的描述可参考【7】</p>\n<p>in block 结构： （block_id, Inblock(src_id数组, src_ptr, dst_id地址数组， rating数组）)<br>out block结构： （block_id， array[array[int]]） （二维数组存储发往每个block的src_id索引）</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">makeBlocks</span></span>[<span class=\"type\">ID</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">      prefix: <span class=\"type\">String</span>,</span><br><span class=\"line\">      ratingBlocks: <span class=\"type\">RDD</span>[((<span class=\"type\">Int</span>, <span class=\"type\">Int</span>), <span class=\"type\">RatingBlock</span>[<span class=\"type\">ID</span>])],</span><br><span class=\"line\">      srcPart: <span class=\"type\">Partitioner</span>,</span><br><span class=\"line\">      dstPart: <span class=\"type\">Partitioner</span>,</span><br><span class=\"line\">      storageLevel: <span class=\"type\">StorageLevel</span>)(</span><br><span class=\"line\">      <span class=\"keyword\">implicit</span> srcOrd: <span class=\"type\">Ordering</span>[<span class=\"type\">ID</span>]): (<span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">InBlock</span>[<span class=\"type\">ID</span>])], <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">OutBlock</span>)]) = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//根据ratingBlocks.map计算inBlocks</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> inBlocks = ratingBlocks.map &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> ((srcBlockId, dstBlockId), <span class=\"type\">RatingBlock</span>(srcIds, dstIds, ratings)) =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> start = <span class=\"type\">System</span>.nanoTime()</span><br><span class=\"line\">        <span class=\"comment\">//dst id去重复</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> dstIdSet = <span class=\"keyword\">new</span> <span class=\"type\">OpenHashSet</span>[<span class=\"type\">ID</span>](<span class=\"number\">1</span> &lt;&lt; <span class=\"number\">20</span>) </span><br><span class=\"line\">        dstIds.foreach(dstIdSet.add)  </span><br><span class=\"line\">        <span class=\"comment\">//dst id 去重结果进行排序</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> sortedDstIds = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">ID</span>](dstIdSet.size)</span><br><span class=\"line\">        <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">var</span> pos = dstIdSet.nextPos(<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (pos != <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">          sortedDstIds(i) = dstIdSet.getValue(pos)</span><br><span class=\"line\">          pos = dstIdSet.nextPos(pos + <span class=\"number\">1</span>)</span><br><span class=\"line\">          i += <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        assert(i == dstIdSet.size)</span><br><span class=\"line\">        <span class=\"type\">Sorting</span>.quickSort(sortedDstIds)</span><br><span class=\"line\">        <span class=\"comment\">//得到dst id 对应的去重和排序后的索引值</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> dstIdToLocalIndex = <span class=\"keyword\">new</span> <span class=\"type\">OpenHashMap</span>[<span class=\"type\">ID</span>, <span class=\"type\">Int</span>](sortedDstIds.length)</span><br><span class=\"line\">        i = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (i &lt; sortedDstIds.length) &#123;</span><br><span class=\"line\">          dstIdToLocalIndex.update(sortedDstIds(i), i)</span><br><span class=\"line\">          i += <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        logDebug(</span><br><span class=\"line\">          <span class=\"string\">\"Converting to local indices took \"</span> + (<span class=\"type\">System</span>.nanoTime() - start) / <span class=\"number\">1e9</span> + <span class=\"string\">\" seconds.\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">val</span> dstLocalIndices = dstIds.map(dstIdToLocalIndex.apply)</span><br><span class=\"line\">        (srcBlockId, (dstBlockId, srcIds, dstLocalIndices, ratings))</span><br><span class=\"line\">    &#125;.groupByKey(<span class=\"keyword\">new</span> <span class=\"type\">ALSPartitioner</span>(srcPart.numPartitions)) <span class=\"comment\">//根据src block id进行聚合</span></span><br><span class=\"line\">      .mapValues &#123; iter =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> builder =</span><br><span class=\"line\">          <span class=\"keyword\">new</span> <span class=\"type\">UncompressedInBlockBuilder</span>[<span class=\"type\">ID</span>](<span class=\"keyword\">new</span> <span class=\"type\">LocalIndexEncoder</span>(dstPart.numPartitions))</span><br><span class=\"line\">        <span class=\"comment\">//将dstBlockId和dstLocalIndices编码，并汇总数据</span></span><br><span class=\"line\">        iter.foreach &#123; <span class=\"keyword\">case</span> (dstBlockId, srcIds, dstLocalIndices, ratings) =&gt;</span><br><span class=\"line\">          builder.add(dstBlockId, srcIds, dstLocalIndices, ratings)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//对结果进行压缩存储，结果格式为（uniqueSrcId数组, dstPtrs数组, dstEncodedIndices数组, ratings数组）</span></span><br><span class=\"line\">        builder.build().compress()</span><br><span class=\"line\">      &#125;.setName(prefix + <span class=\"string\">\"InBlocks\"</span>)</span><br><span class=\"line\">      .persist(storageLevel)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"comment\">//根据inBlocks计算outBlocks</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> outBlocks = inBlocks.mapValues &#123; <span class=\"keyword\">case</span> <span class=\"type\">InBlock</span>(srcIds, dstPtrs, dstEncodedIndices, _) =&gt;</span><br><span class=\"line\">      <span class=\"comment\">//构造编码器</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> encoder = <span class=\"keyword\">new</span> <span class=\"type\">LocalIndexEncoder</span>(dstPart.numPartitions)</span><br><span class=\"line\">      <span class=\"comment\">//定义ArrayBuilder数组，存储发往每个out block的 src id信息</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> activeIds = <span class=\"type\">Array</span>.fill(dstPart.numPartitions)(mutable.<span class=\"type\">ArrayBuilder</span>.make[<span class=\"type\">Int</span>])</span><br><span class=\"line\">      <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> seen = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Boolean</span>](dstPart.numPartitions)</span><br><span class=\"line\">      <span class=\"comment\">//依次计算当前src id是否发往每一个block id</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (i &lt; srcIds.length) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">var</span> j = dstPtrs(i)</span><br><span class=\"line\">        ju.<span class=\"type\">Arrays</span>.fill(seen, fALSe)</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (j &lt; dstPtrs(i + <span class=\"number\">1</span>)) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> dstBlockId = encoder.blockId(dstEncodedIndices(j))</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!seen(dstBlockId)) &#123;</span><br><span class=\"line\">            activeIds(dstBlockId) += i <span class=\"comment\">// add the local index in this out-block</span></span><br><span class=\"line\">            seen(dstBlockId) = <span class=\"literal\">true</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          j += <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        i += <span class=\"number\">1</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      activeIds.map &#123; x =&gt;</span><br><span class=\"line\">        x.result()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.setName(prefix + <span class=\"string\">\"OutBlocks\"</span>)</span><br><span class=\"line\">      .persist(storageLevel)</span><br><span class=\"line\">    (inBlocks, outBlocks)  <span class=\"comment\">//返回结果</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"inblock-compress\"><a href=\"#inblock-compress\" class=\"headerlink\" title=\"inblock compress\"></a>inblock compress</h4><p>  对inblock 中间结果压缩存储，返回结果格式为（uniqueSrcId数组, dstPtrs数组, dstEncodedIndices数组, ratings数组）</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compress</span></span>(): <span class=\"type\">InBlock</span>[<span class=\"type\">ID</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> sz = length</span><br><span class=\"line\">  assert(sz &gt; <span class=\"number\">0</span>, <span class=\"string\">\"Empty in-link block should not exist.\"</span>)</span><br><span class=\"line\">  sort()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> uniqueSrcIdsBuilder = mutable.<span class=\"type\">ArrayBuilder</span>.make[<span class=\"type\">ID</span>]</span><br><span class=\"line\">  <span class=\"keyword\">val</span> dstCountsBuilder = mutable.<span class=\"type\">ArrayBuilder</span>.make[<span class=\"type\">Int</span>]</span><br><span class=\"line\">  <span class=\"keyword\">var</span> preSrcId = srcIds(<span class=\"number\">0</span>)</span><br><span class=\"line\">  uniqueSrcIdsBuilder += preSrcId</span><br><span class=\"line\">  <span class=\"keyword\">var</span> curCount = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> i = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> j = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"comment\">//得到去重后的src id数组， 以及每个src id的数量</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (i &lt; sz) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> srcId = srcIds(i)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (srcId != preSrcId) &#123;</span><br><span class=\"line\">      uniqueSrcIdsBuilder += srcId</span><br><span class=\"line\">      dstCountsBuilder += curCount</span><br><span class=\"line\">      preSrcId = srcId</span><br><span class=\"line\">      j += <span class=\"number\">1</span></span><br><span class=\"line\">      curCount = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    curCount += <span class=\"number\">1</span></span><br><span class=\"line\">    i += <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  dstCountsBuilder += curCount</span><br><span class=\"line\">  <span class=\"keyword\">val</span> uniqueSrcIds = uniqueSrcIdsBuilder.result()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> numUniqueSrdIds = uniqueSrcIds.length</span><br><span class=\"line\">  <span class=\"keyword\">val</span> dstCounts = dstCountsBuilder.result()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> dstPtrs = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Int</span>](numUniqueSrdIds + <span class=\"number\">1</span>)</span><br><span class=\"line\">  <span class=\"keyword\">var</span> sum = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"comment\">//将src id和dst id关系通过dstPtrs进行压缩存储</span></span><br><span class=\"line\">  i = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (i &lt; numUniqueSrdIds) &#123;</span><br><span class=\"line\">    sum += dstCounts(i)</span><br><span class=\"line\">    i += <span class=\"number\">1</span></span><br><span class=\"line\">    dstPtrs(i) = sum</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"type\">InBlock</span>(uniqueSrcIds, dstPtrs, dstEncodedIndices, ratings)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"computeFactor\"><a href=\"#computeFactor\" class=\"headerlink\" title=\"computeFactor\"></a>computeFactor</h3><p>  根据srcFactorBlocks、srcOutBlocks、dstInBlocks, 计算dstFactorBlocks</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeFactors</span></span>[<span class=\"type\">ID</span>](</span><br><span class=\"line\">    srcFactorBlocks: <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">FactorBlock</span>)],</span><br><span class=\"line\">    srcOutBlocks: <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">OutBlock</span>)],</span><br><span class=\"line\">    dstInBlocks: <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">InBlock</span>[<span class=\"type\">ID</span>])],</span><br><span class=\"line\">    rank: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    regParam: <span class=\"type\">Double</span>,</span><br><span class=\"line\">    srcEncoder: <span class=\"type\">LocalIndexEncoder</span>,</span><br><span class=\"line\">    implicitPrefs: <span class=\"type\">Boolean</span> = fALSe,</span><br><span class=\"line\">    alpha: <span class=\"type\">Double</span> = <span class=\"number\">1.0</span>,</span><br><span class=\"line\">    solver: <span class=\"type\">LeastSquaresNESolver</span>): <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">FactorBlock</span>)] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> numSrcBlocks = srcFactorBlocks.partitions.length  <span class=\"comment\">//src block数量</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">YtY</span> = <span class=\"keyword\">if</span> (implicitPrefs) <span class=\"type\">Some</span>(computeYtY(srcFactorBlocks, rank)) <span class=\"keyword\">else</span> <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"comment\">//根据srcOut，得到每个dstBlock对应的srcBlockID 和srcFactor数组</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> srcOut = srcOutBlocks.join(srcFactorBlocks).flatMap &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> (srcBlockId, (srcOutBlock, srcFactors)) =&gt;</span><br><span class=\"line\">      </span><br><span class=\"line\">      srcOutBlock.view.zipWithIndex.map &#123; <span class=\"keyword\">case</span> (activeIndices, dstBlockId) =&gt;</span><br><span class=\"line\">        (dstBlockId, (srcBlockId, activeIndices.map(idx =&gt; srcFactors(idx))))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">//根据dstBlockId 对srcBlockID, array[srcFactor]进行聚合</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> merged = srcOut.groupByKey(<span class=\"keyword\">new</span> <span class=\"type\">ALSPartitioner</span>(dstInBlocks.partitions.length))</span><br><span class=\"line\">  <span class=\"comment\">//对每个dstBlockID, 计算其中每个dstID对应的隐语义向量</span></span><br><span class=\"line\">  dstInBlocks.join(merged).mapValues &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> (<span class=\"type\">InBlock</span>(dstIds, srcPtrs, srcEncodedIndices, ratings), srcFactors) =&gt;</span><br><span class=\"line\">      <span class=\"comment\">//得到每个block对应的src factor向量集合</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sortedSrcFactors = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">FactorBlock</span>](numSrcBlocks)</span><br><span class=\"line\">      srcFactors.foreach &#123; <span class=\"keyword\">case</span> (srcBlockId, factors) =&gt;</span><br><span class=\"line\">        sortedSrcFactors(srcBlockId) = factors</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"comment\">//对每个dstID, 获取对应的srcFactor及对应rating, 计算该dstID对应的隐语义向量</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> dstFactors = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Array</span>[<span class=\"type\">Float</span>]](dstIds.length)</span><br><span class=\"line\">      <span class=\"keyword\">var</span> j = <span class=\"number\">0</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> ls = <span class=\"keyword\">new</span> <span class=\"type\">NormalEquation</span>(rank)</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (j &lt; dstIds.length) &#123;</span><br><span class=\"line\">        ls.reset()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (implicitPrefs) &#123;</span><br><span class=\"line\">          ls.merge(<span class=\"type\">YtY</span>.get)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">var</span> i = srcPtrs(j)</span><br><span class=\"line\">        <span class=\"keyword\">var</span> numExplicits = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (i &lt; srcPtrs(j + <span class=\"number\">1</span>)) &#123; <span class=\"comment\">//依次得到每个srcFactor及rating值</span></span><br><span class=\"line\">          <span class=\"keyword\">val</span> encoded = srcEncodedIndices(i)</span><br><span class=\"line\">          <span class=\"keyword\">val</span> blockId = srcEncoder.blockId(encoded)</span><br><span class=\"line\">          <span class=\"keyword\">val</span> localIndex = srcEncoder.localIndex(encoded)</span><br><span class=\"line\">          <span class=\"comment\">//sortedSrcFactors通过blockId和localIndex进行索引，得到需要的factor向量。之前这里困惑挺久，一直感觉从srcOut传过来的factor向量只是一个子集，通过localIndex访问不正确，实际上这里的localIndex和srcOut那里存储的localindex是不需要对应的。因为同一个src id 本身的src local index不等于其它block对应的 dst localindex</span></span><br><span class=\"line\">          <span class=\"keyword\">val</span> srcFactor = sortedSrcFactors(blockId)(localIndex)</span><br><span class=\"line\">          <span class=\"keyword\">val</span> rating = ratings(i)</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (implicitPrefs) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Extension to the original paper to handle b &lt; 0. confidence is a function of |b|</span></span><br><span class=\"line\">            <span class=\"comment\">// instead so that it is never negative. c1 is confidence - 1.0.</span></span><br><span class=\"line\">            <span class=\"keyword\">val</span> c1 = alpha * math.abs(rating)</span><br><span class=\"line\">            <span class=\"comment\">// For rating &lt;= 0, the corresponding preference is 0. So the term below is only added</span></span><br><span class=\"line\">            <span class=\"comment\">// for rating &gt; 0. Because YtY is already added, we need to adjust the scaling here.</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (rating &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">              numExplicits += <span class=\"number\">1</span></span><br><span class=\"line\">              ls.add(srcFactor, (c1 + <span class=\"number\">1.0</span>) / c1, c1)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            ls.add(srcFactor, rating)</span><br><span class=\"line\">            numExplicits += <span class=\"number\">1</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          i += <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// Weight lambda by the number of explicit ratings based on the ALS-WR paper.</span></span><br><span class=\"line\">        dstFactors(j) = solver.solve(ls, numExplicits * regParam)</span><br><span class=\"line\">        j += <span class=\"number\">1</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      dstFactors</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"ALS-模型推荐\"><a href=\"#ALS-模型推荐\" class=\"headerlink\" title=\"ALS 模型推荐\"></a>ALS 模型推荐</h2><p><strong>模型参数</strong> </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val rank: Int,      //隐语义个数</span><br><span class=\"line\">val userFeatures: RDD[(Int, Array[Double])], //user factor数组, 存储user id 及对应的factor向量</span><br><span class=\"line\">val productFeatures: RDD[(Int, Array[Double])]) //item factor数组，存储item id及对应的factor向量</span><br></pre></td></tr></table></figure>\n<p><strong>对所有用户进行推荐</strong></p>\n<p>调用recommendForAll函数，首先对user向量和item向量分块并以矩阵形式存储，然后对二者做笛卡尔积，并计算每个user和每个item的得分，最终以user为key, 取topK个item及对应的得分，作为推荐结果. 计算topK时借助于小顶堆</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def recommendForAll(</span><br><span class=\"line\">    rank: Int,</span><br><span class=\"line\">    srcFeatures: RDD[(Int, Array[Double])],</span><br><span class=\"line\">    dstFeatures: RDD[(Int, Array[Double])],</span><br><span class=\"line\">    num: Int): RDD[(Int, Array[(Int, Double)])] = &#123;</span><br><span class=\"line\">  //对user向量和item向量分块并以矩阵形式存储</span><br><span class=\"line\">  val srcBlocks = blockify(rank, srcFeatures)</span><br><span class=\"line\">  val dstBlocks = blockify(rank, dstFeatures)</span><br><span class=\"line\">  //笛卡尔积，依次对每个组合计算user对item的偏好</span><br><span class=\"line\">  val ratings = srcBlocks.cartesian(dstBlocks).flatMap &#123;</span><br><span class=\"line\">    case ((srcIds, srcFactors), (dstIds, dstFactors)) =&gt;</span><br><span class=\"line\">      val m = srcIds.length</span><br><span class=\"line\">      val n = dstIds.length</span><br><span class=\"line\">      val ratings = srcFactors.transpose.multiply(dstFactors)</span><br><span class=\"line\">      val output = new Array[(Int, (Int, Double))](m * n)</span><br><span class=\"line\">      var k = 0</span><br><span class=\"line\">      ratings.foreachActive &#123; (i, j, r) =&gt;</span><br><span class=\"line\">        output(k) = (srcIds(i), (dstIds(j), r))</span><br><span class=\"line\">        k += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      output.toSeq</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  //根据user id作为key, 得到喜好分数最高的num个item</span><br><span class=\"line\">  ratings.topByKey(num)(Ordering.by(_._2))</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 对user向量和item向量分块并以矩阵形式存储, 结果的每个元组分别是对应的id数组和factor构成的矩阵</span><br><span class=\"line\">private def blockify(</span><br><span class=\"line\">    rank: Int,</span><br><span class=\"line\">    features: RDD[(Int, Array[Double])]): RDD[(Array[Int], DenseMatrix)] = &#123;</span><br><span class=\"line\">  val blockSize = 4096 // TODO: tune the block size</span><br><span class=\"line\">  val blockStorage = rank * blockSize</span><br><span class=\"line\">  features.mapPartitions &#123; iter =&gt;</span><br><span class=\"line\">    iter.grouped(blockSize).map &#123; grouped =&gt;</span><br><span class=\"line\">      val ids = mutable.ArrayBuilder.make[Int]</span><br><span class=\"line\">      ids.sizeHint(blockSize)</span><br><span class=\"line\">      val factors = mutable.ArrayBuilder.make[Double]</span><br><span class=\"line\">      factors.sizeHint(blockStorage)</span><br><span class=\"line\">      var i = 0</span><br><span class=\"line\">      grouped.foreach &#123; case (id, factor) =&gt;</span><br><span class=\"line\">        ids += id</span><br><span class=\"line\">        factors ++= factor</span><br><span class=\"line\">        i += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      (ids.result(), new DenseMatrix(rank, i, factors.result()))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"ALS推荐实践\"><a href=\"#ALS推荐实践\" class=\"headerlink\" title=\"ALS推荐实践\"></a>ALS推荐实践</h1><p>我们的平台是图片社交，每个用户都可以在平台上浏览图片，并进行点赞、评论等。推荐算法主要用于给用户推荐其最可能感兴趣的图片，最终提升用户体验。</p>\n<h2 id=\"离线实验\"><a href=\"#离线实验\" class=\"headerlink\" title=\"离线实验\"></a>离线实验</h2><p>我们平台暂时无法得到用户的显式评分数据，但是可以得到用户点击、点赞、评论等相关行为信息。因此，比较适合用隐反馈矩阵分解模型。</p>\n<h3 id=\"构造数据集\"><a href=\"#构造数据集\" class=\"headerlink\" title=\"构造数据集\"></a>构造数据集</h3><ul>\n<li><p>数据预处理</p>\n<p>从2周的用户行为数据中，过滤无行为用户数据，spam图片数据和spam用户数据。</p>\n</li>\n<li><p>构建rating元素</p>\n<p>对预处理之后的数据，根据用户每天的图片交互行为，分别对点击、点赞和评论等分别赋予不同的权值，得到rating矩阵. </p>\n</li>\n<li><p>生成训练集和测试集</p>\n<p>对于得到的rating数据，随机划分为两部分 $A:B = 7:3$，如果分别直接作为训练集和测试集是有问题的，因为$B$中的user或者item是有可能在$A$中没有出现过，这样会影响评估结果。 我们采用的方法是如果B数据中某个rating元素的user或item没有在A出现，则将该元素放到$A$中用作训练集。最终$A$和新加进来的元素共同构成训练集$A^1$， $B$留下的数据 $B^1$ 作为测试集。</p>\n</li>\n</ul>\n<h3 id=\"离线训练和评估\"><a href=\"#离线训练和评估\" class=\"headerlink\" title=\"离线训练和评估\"></a>离线训练和评估</h3><ul>\n<li>离线训练</li>\n</ul>\n<p>利用spark mllib库，对训练集构成的rating矩阵，建立隐反馈矩阵分解模型，并完成进行矩阵分解，生成user factor和item factor。</p>\n<ul>\n<li>评估</li>\n</ul>\n<p>调用模型的recommendForAll函数，对测试集所有user进行item推荐，并计算召回率和准确率。根据召回率和准确率，进行参数优化。</p>\n<ul>\n<li>评估指标</li>\n</ul>\n<p>假定$P_i$为用户$i$的预测结果，$P$为所有的预测结果，每个结果记录格式为（user, item）， $T$为测试集,每条记录格式为（user， item）。各种指标的的计算如下：</p>\n<p>召回率: $R= \\frac{|P \\bigcap T|}  {|T|}$</p>\n<p>准确率: $P= \\frac{|P \\bigcap T|}  {|P|}$</p>\n<p>F1:  $F= \\frac{2PR}  {P+R} $</p>\n<p>离散度：$\\frac{1}{N^2}\\sum_i\\sum_j\\frac{|P_i \\bigcap P_j|}{|P_i \\bigcup P_j|}$</p>\n<p>除了上述指标之外，我们还对用户连续多天推荐结果的差异性、用户覆盖率、图片覆盖率等指标进行评估。</p>\n<h2 id=\"在线ab测试\"><a href=\"#在线ab测试\" class=\"headerlink\" title=\"在线ab测试\"></a>在线ab测试</h2><p>abtest方案： 将als算法计算出的结果，定期写入到线上，作为线上的一种推荐来源。对实验组用户同时采用新策略和旧策略进行推荐，对照组用户只采用旧策略进行推荐。</p>\n<p>从2个维度进行评估：</p>\n<ul>\n<li>评估实验组和对照组用户在abtest上线前后点击率</li>\n<li>评估实验组用户在新旧两种策略推荐图片的点击率</li>\n</ul>\n<p>测试一定时间后，交换对照组和实验组用户，按照上述2个维度重新进行评估</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>【1】Y Koren，R Bell，C Volinsky, “Matrix Factorization Techniques for Recommender Systems”, 《Computer》, 2009.08; 42(8):30-37 </p>\n<p>【2】洪亮劼, “知人知面需知心——人工智能技术在推荐系统中的应用”, 2016.11, <a href=\"http://mp.weixin.qq.com/s/JuaM8d52-f8AzTjEPnCl7g\" target=\"_blank\" rel=\"noopener\">http://mp.weixin.qq.com/s/JuaM8d52-f8AzTjEPnCl7g</a></p>\n<p>【3】S. Funk, “Netflix Update: Try This at Home”, 2006.12, <a href=\"http://sifter.org/~simon/journal/20061211.html\" target=\"_blank\" rel=\"noopener\">http://sifter.org/~simon/journal/20061211.html</a></p>\n<p>【4】Y. Koren, “Factorization Meets the Neighborhood: A Mul-tifaceted Collaborative Filtering Model”, Proc. 14th ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining, ACM Press, 2008, pp.426-434</p>\n<p>【5】A. Paterek, “Improving Regularized Singular Value De-composition for Collaborative Filtering” Proc. KDD Cup and Workshop, ACM Press, 2007, pp.39-42</p>\n<p>【6】G. Takács et al., “Major Components of the Gravity Recom- mendation System”, SIGKDD Explorations, 2007.09, vol.9, pp.80-84</p>\n<p>【7】孟祥瑞, “ALS 在 Spark MLlib 中的实现”, 2015.05, <a href=\"http://www.csdn.net/article/2015-05-07/2824641\" target=\"_blank\" rel=\"noopener\">http://www.csdn.net/article/2015-05-07/2824641</a></p>\n<p>【8】Zhen-ming Yuan, et al., “A microblog recommendation algorithm based on social tagging and a temporal interest evolution model”, Frontiers of Information Technology &amp; Electronic Engineering, 2015.07,<br>Volume 16, Issue 7, pp 532–540 </p>\n<p>【9】Z Zhao, Z Cheng, L Hong, EH Chi, “Improving User Topic Interest Profiles by Behavior Factorization”, Proceedings of the 24th International Conference on World Wide Web, 2015.05, pp.1406-1416</p>\n<p>【10】阿里技术，”淘宝搜索/推荐系统背后深度强化学习与自适应在线学习的实践之路”, 2017.02, <a href=\"http://url.cn/451740J\" target=\"_blank\" rel=\"noopener\">http://url.cn/451740J</a></p>\n<p>【11】HT Cheng, L Koc, J Harmsen, T Shaked, “Wide &amp; Deep Learning for Recommender Systems”, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, 2016.09,  pp.7-10</p>\n<p>【12】黄安埠, “递归的艺术 - 深度递归网络在序列式推荐的应用”, 2016.10, <a href=\"http://mp.weixin.qq.com/s?__biz=MzA3MDQ4MzQzMg==&amp;mid=2665690422&amp;idx=1&amp;sn=9bd671983a85286149b51c908b686899&amp;chksm=842bb9b1b35c30a7eedb8d03e173aa8f43465db90e11075ac0c73b1784582f21eb93dcbd3e65&amp;scene=0%23wechat_redirect\" target=\"_blank\" rel=\"noopener\">http://mp.weixin.qq.com/s?__biz=MzA3MDQ4MzQzMg==&amp;mid=2665690422&amp;idx=1&amp;sn=9bd671983a85286149b51c908b686899&amp;chksm=842bb9b1b35c30a7eedb8d03e173aa8f43465db90e11075ac0c73b1784582f21eb93dcbd3e65&amp;scene=0%23wechat_redirect</a></p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<p>ALS（alternating least squares）是一种基础的推荐算法，相对于普通的协同过滤等方法，它不仅能通过降维增加模型的泛化能力，也方便加入其他建模因素（如数据偏差、时间、隐反馈等），大大提升了模型的灵活性。正因为此，ALS算法在Netflix推荐大赛中脱颖而出，在我们具体的工程实践中，也具有非常不错的表现。接下来，从如下几个方面和大家一起学习：ALS算法模型、spark ALS源码理解， ALS推荐实践。如描述有误，欢迎大家指正。</p>\n<h1 id=\"ALS算法模型\"><a href=\"#ALS算法模型\" class=\"headerlink\" title=\"ALS算法模型\"></a>ALS算法模型</h1><h2 id=\"为什么要用ALS模型\"><a href=\"#为什么要用ALS模型\" class=\"headerlink\" title=\"为什么要用ALS模型\"></a>为什么要用ALS模型</h2><p> 相对于其他模型，ALS模型优势如下：</p>\n<ul>\n<li><p><strong>相对于基于内容的推荐</strong>，ALS属于协同过滤大家族【1】【12】（也有人认为ALS 基于矩阵分解技术，不属于协同过滤范畴【2】），<strong>直接跟进用户行为信息进行建模，不需要获取user和item的内容信息</strong>（很多情况下这些内容信息并不是很好获取，但是相对基于内容的推荐，ALS存在冷启动问题）</p>\n</li>\n<li><p><strong>相对于传统的协同过滤推荐方法（user based、item based）</strong>， ALS算法属于factor model, 通过将数据从原始空间映射到更低维度空间，<strong>去除噪声信息，利用更主要的语义信息对问题建模，能获得更好的推荐效果</strong>。</p>\n</li>\n<li><p><strong>相对于svd分解模型而言</strong>， 两种模型都属于 factor model, 但<strong>svd分解模型更倾向于解决矩阵元素没有缺失的情况， 而通过一定的方式去填充矩阵不仅需要额外的填充成本，填充本身可能影响了数据的真实性</strong>。因此，直接对已知元素进行建模，是一种不错的思路。如【1，3-6】，直接对rating矩阵已知元素$r_{ui}$进行建模:</p>\n</li>\n</ul>\n<center><br>$\\sum_{u,i\\in\\mathbb K} (r_{ui} -<br>p_u^Tq_i)^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$ （1）<br></center>\n\n<ul>\n<li>针对所建模型1可以用SGD或ALS 两种算法求解。其中<strong>sgd方法相对比较简单，但是当我们要建模的矩阵中已知元素较多时（如隐反馈），采用sgd在每次迭代都要求解所有元素，其时间复杂度是非常大的</strong>。ALS算法在求解某个user （或item）向量时，不依赖其他任何user（item）向量，这个性质使得<strong>ALS算法在每次迭代过程中方便并行化求解，在解决大规模矩阵分解问题时非常具有优势</strong>。 </li>\n</ul>\n<h2 id=\"ALS模型有什么缺点\"><a href=\"#ALS模型有什么缺点\" class=\"headerlink\" title=\"ALS模型有什么缺点\"></a>ALS模型有什么缺点</h2><p> 相对于其它推荐算法，ALS模型具有非常明显的优势：不需要对user和item信息进行建模，能够更加灵活地对各种因素建模，方便大规模并行计算。但ALS模型在如下几方面，又有自己的局限性：</p>\n<ul>\n<li><p>冷启动问题</p>\n<p>包括user的冷启动和item的冷启动。由于rating矩阵的构建，依赖user的显式和隐式反馈信息，对于新的user和item，或者没有相关行为的user或item, 导致无法构建rating矩阵，或者rating矩阵构建不合理。</p>\n<p>基于内容的推荐能较好地解决冷启动相关问题。如【8】在解决用户的冷启动问题时，首先根据用户之间社交的亲密度，对用户进行聚类，利用相同群体的用户画像来建模自身兴趣。同时，对于item的冷启动问题，可利用item本身对一些关键词、类目、内容等相关信息进行建模。【9】为了提高用户兴趣的准确率和覆盖率，在对用户兴趣建模对时候，将用户兴趣进行更具体的分类（如消费兴趣、生产兴趣、具体的每个行为兴趣等），并针对具体的业务，采用线性回归的方法对各种兴趣利用线性回归的方式进行加权求和，提升用户兴趣准确率和覆盖率。</p>\n</li>\n<li><p>用户临时兴趣</p>\n<p>用户的兴趣是在不断变换的。对于相对较稳定的兴趣，ALS算法可以通过引入时间因素进行建模，如公式4。 但对于临时的兴趣变换，ALS算法是无法捕获的。</p>\n<p><strong>一种简单且有效的方法</strong></p>\n<p>将item划分为多个类别，每个类别对应一种兴趣。用户每次点击某个类别的item之后，认为该用户存在一种临时兴趣，通过动态增加相应类别的比例的item，迎合用户当前的消费需求。该方法的难点在于如何调整比例，才能让用户感到有很多自己喜欢的item, 同时又不会让用户感觉内容的单调。</p>\n<p><strong>淘宝的一些实践</strong></p>\n<p>为了有效获取用户的即时兴趣，给用户推荐最合适的产品，淘宝进行了比较多的实践【10】，分别如下所述。</p>\n<ul>\n<li><p>GBDT+FTRL模型</p>\n<p>由于GBDT模型比较擅长挖具有区分度的特征，其使用GBDT模型进行特征挖掘，将得到的特征输送给FTRL进行在线学习。输送给GBDT的特征包括两部分：一部分用户基础行为的次数、CTR等；另一部分是来自match粗选阶段的的特征，该部分特征来自不同的粗选模型输出.</p>\n</li>\n<li><p>Wide &amp; Deep Learning模型</p>\n<p>借鉴google论文思想【11】，利用wide模型 + deep模型 + LR，其中wide子结构通过特征交叉学习特征间的共现，deep子结构则输入具有泛化能力的离散特征和连续特征，wide模型和deep模型学习到的结果，再利用LR模型预测相应的得分。</p>\n</li>\n<li><p>Adaptive-Online-Learning</p>\n<p>保留每一时刻学习到的模型，根据业务指标，得到每个模型等权重信息，融合出最优的结果。该方法能够比较好地综合利用用户长期喝短期兴趣。</p>\n</li>\n<li><p>Reinforcement Learning</p>\n<p>该方法思想是通过定义每个步骤的奖励，当用户每次到来的时候，根据用户的累积奖励值，进行个性化推荐。</p>\n</li>\n</ul>\n<p><strong>腾讯的LSTM实践</strong></p>\n<p>  为解决音乐的推荐问题，腾讯采用的是LSTM深度学习方法【12】，将用户听的歌曲序列，抽取特征输入到LSTM网络进行训练。为防止有些用户对应的歌曲序列较短问题，其对这些数据的训练采用特殊处理，相关数据缺失的序列不进行状态更新。同时，为加快训练速度，将每次权值的训练过程通过矩阵的方式实现并发计算。另外，为降低soft max过程时间复杂度，采用Hierarchical softmax过程替代普通的softmax。</p>\n</li>\n</ul>\n<h2 id=\"ALS模型是什么\"><a href=\"#ALS模型是什么\" class=\"headerlink\" title=\"ALS模型是什么\"></a>ALS模型是什么</h2><h3 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h3><p>ALS模型属于隐语义模型，通过对用户行为矩阵R进行矩阵分解，得到user factor向量矩阵P、item factor向量矩阵Q. </p>\n<p>$R = P^T Q$ 。其中R、$P^T$、$Q^T$矩阵的定义如表1-表3所示。</p>\n<p>潜在语义空间对应的各个factor代表不同的属性信息，user向量描述了user对各种属性的喜好程度，item向量描述了item所具备的各种属性强度，二者在潜在语义空间的相似度描述了user对item的喜好程度,在进行推荐时，根据该喜好程度计算推荐结果。</p>\n<center>表1: rating矩阵R</center>\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th>item1</th>\n<th>item2</th>\n<th>item3</th>\n<th>item4</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>user1</td>\n<td>$r_{11}$</td>\n<td>$r_{12}$</td>\n<td>$r_{13}$</td>\n<td>$r_{14}$</td>\n</tr>\n<tr>\n<td>user2</td>\n<td>$r_{21}$</td>\n<td>$r_{22}$</td>\n<td>$r_{23}$</td>\n<td>$r_{24}$</td>\n</tr>\n<tr>\n<td>user3</td>\n<td>$r_{31}$</td>\n<td>$r_{32}$</td>\n<td>$r_{33}$</td>\n<td>$r_{34}$</td>\n</tr>\n<tr>\n<td>user4</td>\n<td>$r_{41}$</td>\n<td>$r_{42}$</td>\n<td>$r_{43}$</td>\n<td>$r_{44}$</td>\n</tr>\n<tr>\n<td>user5</td>\n<td>$r_{51}$</td>\n<td>$r_{52}$</td>\n<td>$r_{53}$</td>\n<td>$r_{54}$</td>\n</tr>\n</tbody>\n</table>\n<center>表2：user矩阵$P^T$</center>\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th>factor1</th>\n<th>factor2</th>\n<th>factor3 </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>user1</td>\n<td>$p_{11}$</td>\n<td>$p_{12}$</td>\n<td>$p_{13}$</td>\n</tr>\n<tr>\n<td>user2</td>\n<td>$p_{21}$</td>\n<td>$p_{22}$</td>\n<td>$p_{23}$</td>\n</tr>\n<tr>\n<td>user3</td>\n<td>$p_{31}$</td>\n<td>$p_{32}$</td>\n<td>$p_{33}$</td>\n</tr>\n<tr>\n<td>user4</td>\n<td>$p_{41}$</td>\n<td>$p_{42}$</td>\n<td>$p_{43}$</td>\n</tr>\n<tr>\n<td>user5</td>\n<td>$p_{51}$</td>\n<td>$p_{52}$</td>\n<td>$p_{53}$</td>\n</tr>\n</tbody>\n</table>\n<center>表3:item矩阵$Q^T$</center>\n\n<table>\n<thead>\n<tr>\n<th></th>\n<th>factor1</th>\n<th>factor2</th>\n<th>factor3 </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>item1</td>\n<td>$q_{11}$</td>\n<td>$q_{12}$</td>\n<td>$q_{13}$</td>\n</tr>\n<tr>\n<td>item2</td>\n<td>$q_{21}$</td>\n<td>$q_{22}$</td>\n<td>$q_{23}$</td>\n</tr>\n<tr>\n<td>item3</td>\n<td>$q_{31}$</td>\n<td>$q_{32}$</td>\n<td>$q_{33}$</td>\n</tr>\n<tr>\n<td>item4</td>\n<td>$q_{41}$</td>\n<td>$q_{42}$</td>\n<td>$q_{43}$</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"目标函数\"><a href=\"#目标函数\" class=\"headerlink\" title=\"目标函数\"></a>目标函数</h3><p>$MIN_{PQ} \\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$  (2)</p>\n<p>其中${(r_{ui} - p_u^Tq_i）}^2$ 目的在于最小化分解误差，$\\lambda(p_u^Tp_u+q_i^Tq_i)$ 为正则项。</p>\n<h3 id=\"目标函数求解\"><a href=\"#目标函数求解\" class=\"headerlink\" title=\"目标函数求解\"></a>目标函数求解</h3><p>由于目标函数中$p_u, q_i$都是未知变量，该问题是非凸的。当我们固定其中一个变量，解另外一个变量时，问题则变成凸问题，这是ALS求解的主要思想。在实际求解过程中分为如下几个步骤：</p>\n<ol>\n<li><p>随机初始化所有的变量$p_u, q_i$。</p>\n</li>\n<li><p>固定所有的$q_i$变量，求得$q_i$变量为当前值时$p_u$的最优值。</p>\n</li>\n<li><p>固定所有的$p_u$变量，求得$p_u$变量为当前值时$q_i$的最优值。</p>\n</li>\n<li><p>如果满足终止条件，则终止。否则，迭代执行2，3两步。</p>\n</li>\n</ol>\n<p>通过不断执行步骤2和步骤3，使得误差越来越小，直到收敛或达到指定次数而终止。通过可导函数性质我们知道，当对变量求导结果等于0当时候，函数可以取得极值。具体到公式2，固定一个变量，对另一变量求导结果等于0时，可以达到极小值。</p>\n<p>我们令$L = \\sum_{u,i\\in\\mathbb K} {(r_{ui} - p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>固定所有$q_i$, 对$p_u$求导</p>\n<p>$-\\frac{\\alpha L}{2\\alpha p_{uk}} = \\sum_{i} {q_{ik}(r_{ui} - p_u^Tq_i）} - \\lambda p_{uk} = 0$</p>\n<p>=&gt; $\\sum_{i} {q_{i}(r_{ui} - p_u^Tq_i）} - \\lambda p_{u} = 0$</p>\n<p>=&gt; $(\\sum_{i} {q_i q_i^T} + \\lambda E) p_u = \\sum_{i}q_i r_{ui}$</p>\n<p>=&gt; $p_u = (\\sum_{i} {q_i q_i^T} + \\lambda E)^{-1}\\sum_{i}q_i r_{ui}$</p>\n<p>=&gt; $ p_u = (Q_{u,i\\in\\mathbb K} Q_{u,i\\in\\mathbb K}^T + \\lambda E)^{-1}Q_{u,i\\in\\mathbb K}R_{u,i\\in\\mathbb K}^T$</p>\n<p>其中，$q_{u,i\\in\\mathbb K}$ 表示和user $u$有行为关联的item对应的向量矩阵，$r_{u,i\\in\\mathbb K}^T$表示和user $u$有行为关联的item对应rating元素构成的向量的转置。</p>\n<p><strong>更加灵活的ALS建模</strong></p>\n<p>相对于传统的协同协同过滤方法，ALS能更好的考虑其他因素，如数据偏差、时间等</p>\n<ol>\n<li><p>引入数据偏差</p>\n<p> user偏差：不同的用户，可能具有不同的评分标准。如用户在给item打分时，有的用户可能可能更倾向于给所有item打高分， 而有的挑剔用户会给所有item打分偏低</p>\n<p> item偏差：有的热门item可能所有用户都会倾向于打高分，而有的item可能本身大多数人会倾向于打低分</p>\n<p> 考虑use和item偏差的ALS建模：<br> $MIN_{PQB} \\sum_{u,i\\in\\mathbb K} {(r_{ui} - u - b_u - b_i-<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i+b_u^2+b_i^2)$  (3)</p>\n</li>\n<li><p>引入时间因素</p>\n<p> 用户偏好、rating矩阵，都可能随时间变化，item对应的属性不随时间变化，因此可进行如下建模<br>$MIN_{PQB} \\sum_{u,i\\in\\mathbb K} {(r_{ui}（t） - u - b_u(t) - b_i(t)-<br>p_u(t)^Tq_i）}^2 + \\lambda(p_u(t)^Tp_u(t)+q_i^Tq_i+b_u(t)^2+b_i(t)^2)$  (4)</p>\n</li>\n<li><p>引入隐反馈数据因素</p>\n<p> 很多时候，并没有用户对item明确的打分数据，此时可通过搜集用户隐反馈数据（浏览、点击、点赞等），进行隐反馈建模。有一点需要注意，此时不只是对$r_{ui}$大于0对用户行为建模，而是所有$r_{ui}$元素建模。模型如公式5所示：</p>\n<p> $MIN_{PQB} \\sum_{u,i} {c_{ui}(p_{ui} - u - b_u - b_i-<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i+b_u^2+b_i^2)$  (5)</p>\n<p> $p_{ui}$ 表示user u是否有相关行为表示喜欢item i, $c_{ui}$描述user u 对item i的喜欢程度，其定义如公式6和公式7所示</p>\n<p> $<br>p_{ui} =<br>\\begin{cases}<br>1,  &amp; r_{ui}&gt;0\\\\<br>0,  &amp; r_{ui}=0<br>\\end{cases}<br>$（6）</p>\n<p> $c_{ui} = 1 + \\alpha r_{ui}$（7）</p>\n</li>\n</ol>\n<h1 id=\"spark-ALS源码理解\"><a href=\"#spark-ALS源码理解\" class=\"headerlink\" title=\"spark ALS源码理解\"></a>spark ALS源码理解</h1><p>为加深对ALS算法的理解，该部分主要分析spark mllib中ALS源码的实现，大体上分为2部分：ALS模型训练、ALS模型推荐</p>\n<h2 id=\"ALS-模型训练\"><a href=\"#ALS-模型训练\" class=\"headerlink\" title=\"ALS 模型训练\"></a>ALS 模型训练</h2><h3 id=\"ALS-伴生类\"><a href=\"#ALS-伴生类\" class=\"headerlink\" title=\"ALS 伴生类\"></a>ALS 伴生类</h3><p>ALS 伴生对象提供外部调用 ALS模型训练的入口。通过传入相关参数， 返回训练好的模型对象MatrixFactorizationModel。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">ALS</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span></span>(</span><br><span class=\"line\">      ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>], <span class=\"comment\">//rating元素 （user, item, rate）</span></span><br><span class=\"line\">      rank: <span class=\"type\">Int</span>, <span class=\"comment\">//隐语义个数</span></span><br><span class=\"line\">      iterations: <span class=\"type\">Int</span>, <span class=\"comment\">//迭代次数</span></span><br><span class=\"line\">      lambda: <span class=\"type\">Double</span>, <span class=\"comment\">//正则惩罚项</span></span><br><span class=\"line\">      blocks: <span class=\"type\">Int</span>, <span class=\"comment\">//数据block个数</span></span><br><span class=\"line\">      seed: <span class=\"type\">Long</span> <span class=\"comment\">//随机数种子</span></span><br><span class=\"line\">    ): <span class=\"type\">MatrixFactorizationModel</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ALS</span>(blocks, blocks, rank, iterations, lambda, fALSe, <span class=\"number\">1.0</span>, seed).run(ratings)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trainImplicit</span></span>(</span><br><span class=\"line\">      ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>], <span class=\"comment\">// rating元素 （user, item, rate）</span></span><br><span class=\"line\">      rank: <span class=\"type\">Int</span>, <span class=\"comment\">//隐语义个数</span></span><br><span class=\"line\">      iterations: <span class=\"type\">Int</span>, <span class=\"comment\">//迭代次数</span></span><br><span class=\"line\">      lambda: <span class=\"type\">Double</span>, <span class=\"comment\">//正则惩罚项</span></span><br><span class=\"line\">      blocks: <span class=\"type\">Int</span>, <span class=\"comment\">//数据block个数</span></span><br><span class=\"line\">      alpha: <span class=\"type\">Double</span> <span class=\"comment\">//计算$c_ui$时用的alpha参数</span></span><br><span class=\"line\">    ): <span class=\"type\">MatrixFactorizationModel</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ALS</span>(blocks, blocks, rank, iterations, lambda, <span class=\"literal\">true</span>, alpha).run(ratings)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   <span class=\"comment\">//另外还有一些其他接口，因最终都通过调用上面2个函数，此处将其省略</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"ALS-私有类\"><a href=\"#ALS-私有类\" class=\"headerlink\" title=\"ALS 私有类\"></a>ALS 私有类</h3><p>定义了ALS类对应的各个参数，以及各个参数的设定方法。并定义了run方法供伴随类进行调用，该方法返回训练结果MatrixFactorizationModel给ALS伴随类。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ALS</span> <span class=\"title\">private</span> (<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var numUserBlocks: <span class=\"type\">Int</span>, //用户数据block个数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var numProductBlocks: <span class=\"type\">Int</span>, //item数据block个数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var rank: <span class=\"type\">Int</span>, //隐语义个数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var iterations: <span class=\"type\">Int</span>, //迭代次数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var lambda: <span class=\"type\">Double</span>, //正则惩罚项</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var implicitPrefs: <span class=\"type\">Boolean</span>, //是否使用隐反馈模型</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var alpha: <span class=\"type\">Double</span>, //计算$c_ui$时用的alpha参数</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    private var seed: <span class=\"type\">Long</span> = <span class=\"type\">System</span>.nanoTime(</span>) <span class=\"title\">//随机数种子</span>,<span class=\"title\">默认为当前时间戳</span></span></span><br><span class=\"line\"><span class=\"class\">  ) <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> <span class=\"keyword\">with</span> <span class=\"title\">Logging</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">//设置block个数</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">setBlocks</span></span>(numBlocks: <span class=\"type\">Int</span>): <span class=\"keyword\">this</span>.<span class=\"keyword\">type</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.numUserBlocks = numBlocks</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.numProductBlocks = numBlocks</span><br><span class=\"line\">    <span class=\"keyword\">this</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"comment\">// 另外对其他参数变量也有相关函数实现，因基本都是赋值操作，此处将其省略</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"comment\">//run方法，通过输入rating数据，完成训练兵返回结果MatrixFactorizationModel</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>(ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>]): <span class=\"type\">MatrixFactorizationModel</span> = &#123;</span><br><span class=\"line\">    require(!ratings.isEmpty(), <span class=\"string\">s\"No ratings available from <span class=\"subst\">$ratings</span>\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = ratings.context</span><br><span class=\"line\">    <span class=\"comment\">//设置user block个数</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> numUserBlocks = <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.numUserBlocks == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">      math.max(sc.defaultParallelism, ratings.partitions.length / <span class=\"number\">2</span>)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">this</span>.numUserBlocks</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//设置item block个数</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> numProductBlocks = <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.numProductBlocks == <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">      math.max(sc.defaultParallelism, ratings.partitions.length / <span class=\"number\">2</span>)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">this</span>.numProductBlocks</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//调用NewALS.train方法完成矩阵分解，生成user factor和item factor向量,该方法是整个ALS算法的核心实现</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> (floatUserFactors, floatProdFactors) = <span class=\"type\">NewALS</span>.train[<span class=\"type\">Int</span>](</span><br><span class=\"line\">      ratings = ratings.map(r =&gt; <span class=\"type\">NewALS</span>.<span class=\"type\">Rating</span>(r.user, r.product, r.rating.toFloat)),</span><br><span class=\"line\">      rank = rank,</span><br><span class=\"line\">      numUserBlocks = numUserBlocks,</span><br><span class=\"line\">      numItemBlocks = numProductBlocks,</span><br><span class=\"line\">      maxIter = iterations,</span><br><span class=\"line\">      regParam = lambda,</span><br><span class=\"line\">      implicitPrefs = implicitPrefs,</span><br><span class=\"line\">      alpha = alpha,</span><br><span class=\"line\">      nonnegative = nonnegative,</span><br><span class=\"line\">      intermediateRDDStorageLevel = intermediateRDDStorageLevel,</span><br><span class=\"line\">      finalRDDStorageLevel = <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>,</span><br><span class=\"line\">      checkpointInterval = checkpointInterval,</span><br><span class=\"line\">      seed = seed)</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">val</span> userFactors = floatUserFactors</span><br><span class=\"line\">      .mapValues(_.map(_.toDouble))</span><br><span class=\"line\">      .setName(<span class=\"string\">\"users\"</span>)</span><br><span class=\"line\">      .persist(finalRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> prodFactors = floatProdFactors</span><br><span class=\"line\">      .mapValues(_.map(_.toDouble))</span><br><span class=\"line\">      .setName(<span class=\"string\">\"products\"</span>)</span><br><span class=\"line\">      .persist(finalRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (finalRDDStorageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;</span><br><span class=\"line\">      userFactors.count()</span><br><span class=\"line\">      prodFactors.count()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//生成和返回ALS模型 MatrixFactorizationModel</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">MatrixFactorizationModel</span>(rank, userFactors, prodFactors)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"NewALS-train方法\"><a href=\"#NewALS-train方法\" class=\"headerlink\" title=\"NewALS.train方法\"></a>NewALS.train方法</h3><p>被ALS私有类的run方法调用，用于计算user factor和item factor向量。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span></span>[<span class=\"type\">ID</span>: <span class=\"type\">ClassTag</span>]( <span class=\"comment\">// scalastyle:ignore</span></span><br><span class=\"line\">      ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>[<span class=\"type\">ID</span>]],</span><br><span class=\"line\">      rank: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      numUserBlocks: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      numItemBlocks: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      maxIter: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      regParam: <span class=\"type\">Double</span> = <span class=\"number\">1.0</span>,</span><br><span class=\"line\">      implicitPrefs: <span class=\"type\">Boolean</span> = fALSe,</span><br><span class=\"line\">      alpha: <span class=\"type\">Double</span> = <span class=\"number\">1.0</span>,</span><br><span class=\"line\">      nonnegative: <span class=\"type\">Boolean</span> = fALSe,</span><br><span class=\"line\">      intermediateRDDStorageLevel: <span class=\"type\">StorageLevel</span> = <span class=\"type\">StorageLevel</span>.<span class=\"type\">MEMORY_AND_DISK</span>,</span><br><span class=\"line\">      finalRDDStorageLevel: <span class=\"type\">StorageLevel</span> = <span class=\"type\">StorageLevel</span>.<span class=\"type\">MEMORY_AND_DISK</span>,</span><br><span class=\"line\">      checkpointInterval: <span class=\"type\">Int</span> = <span class=\"number\">10</span>,</span><br><span class=\"line\">      seed: <span class=\"type\">Long</span> = <span class=\"number\">0</span>L)(</span><br><span class=\"line\">      <span class=\"keyword\">implicit</span> ord: <span class=\"type\">Ordering</span>[<span class=\"type\">ID</span>]): (<span class=\"type\">RDD</span>[(<span class=\"type\">ID</span>, <span class=\"type\">Array</span>[<span class=\"type\">Float</span>])], <span class=\"type\">RDD</span>[(<span class=\"type\">ID</span>, <span class=\"type\">Array</span>[<span class=\"type\">Float</span>])]) = &#123;</span><br><span class=\"line\">    require(!ratings.isEmpty(), <span class=\"string\">s\"No ratings available from <span class=\"subst\">$ratings</span>\"</span>)</span><br><span class=\"line\">    require(intermediateRDDStorageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>,</span><br><span class=\"line\">      <span class=\"string\">\"ALS is not designed to run without persisting intermediate RDDs.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = ratings.sparkContext</span><br><span class=\"line\">    <span class=\"comment\">//根据block个数，构建哈稀器。</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> userPart = <span class=\"keyword\">new</span> <span class=\"type\">ALSPartitioner</span>(numUserBlocks)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> itemPart = <span class=\"keyword\">new</span> <span class=\"type\">ALSPartitioner</span>(numItemBlocks)</span><br><span class=\"line\">    <span class=\"comment\">//构建索引编码器，根据block编号和block内索引进行编码，同时可将编码后结果快速解码为block编号和block内索引号。具体实现是通过block个数，确定block编码需要的二进制位数，以及block内索引位数，通过这些位数利用逻辑操作即可实现编码和解码</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> userLocalIndexEncoder = <span class=\"keyword\">new</span> <span class=\"type\">LocalIndexEncoder</span>(userPart.numPartitions)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> itemLocalIndexEncoder = <span class=\"keyword\">new</span> <span class=\"type\">LocalIndexEncoder</span>(itemPart.numPartitions)</span><br><span class=\"line\">    <span class=\"comment\">//构建求解器</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> solver = <span class=\"keyword\">if</span> (nonnegative) <span class=\"keyword\">new</span> <span class=\"type\">NNLSSolver</span> <span class=\"keyword\">else</span> <span class=\"keyword\">new</span> <span class=\"type\">CholeskySolver</span></span><br><span class=\"line\">    <span class=\"comment\">//对rating矩阵进行分块，得到((user_blockID, item_blockID),rating(user, item, rating))</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockRatings = partitionRatings(ratings, userPart, itemPart)</span><br><span class=\"line\">      .persist(intermediateRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"comment\">//构建user inblock和outblock数据，inblock数据记录每个user对应的所有item的地址，及对应rating信息。 outblock记录当前block的哪些user数据会被哪些block用上</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> (userInBlocks, userOutBlocks) =</span><br><span class=\"line\">      makeBlocks(<span class=\"string\">\"user\"</span>, blockRatings, userPart, itemPart, intermediateRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"comment\">// materialize blockRatings and user blocks</span></span><br><span class=\"line\">    userOutBlocks.count()</span><br><span class=\"line\">    <span class=\"comment\">//交换blockrating中的user, item数据，用于构造item的inblcok和outblock信息</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> swappedBlockRatings = blockRatings.map &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> ((userBlockId, itemBlockId), <span class=\"type\">RatingBlock</span>(userIds, itemIds, localRatings)) =&gt;</span><br><span class=\"line\">        ((itemBlockId, userBlockId), <span class=\"type\">RatingBlock</span>(itemIds, userIds, localRatings))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> (itemInBlocks, itemOutBlocks) =</span><br><span class=\"line\">      makeBlocks(<span class=\"string\">\"item\"</span>, swappedBlockRatings, itemPart, userPart, intermediateRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"comment\">// materialize item blocks</span></span><br><span class=\"line\">    itemOutBlocks.count()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> seedGen = <span class=\"keyword\">new</span> <span class=\"type\">XORShiftRandom</span>(seed)</span><br><span class=\"line\">    <span class=\"comment\">//随机初始化user factor和item factor</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> userFactors = initialize(userInBlocks, rank, seedGen.nextLong())</span><br><span class=\"line\">    <span class=\"keyword\">var</span> itemFactors = initialize(itemInBlocks, rank, seedGen.nextLong())</span><br><span class=\"line\">    <span class=\"keyword\">var</span> previousCheckpointFile: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> shouldCheckpoint: <span class=\"type\">Int</span> =&gt; <span class=\"type\">Boolean</span> = (iter) =&gt;</span><br><span class=\"line\">      sc.checkpointDir.isDefined &amp;&amp; checkpointInterval != <span class=\"number\">-1</span> &amp;&amp; (iter % checkpointInterval == <span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deletePreviousCheckpointFile: () =&gt; <span class=\"type\">Unit</span> = () =&gt;</span><br><span class=\"line\">      previousCheckpointFile.foreach &#123; file =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> checkpointFile = <span class=\"keyword\">new</span> <span class=\"type\">Path</span>(file)</span><br><span class=\"line\">          checkpointFile.getFileSystem(sc.hadoopConfiguration).delete(checkpointFile, <span class=\"literal\">true</span>)</span><br><span class=\"line\">        &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">          <span class=\"keyword\">case</span> e: <span class=\"type\">IOException</span> =&gt;</span><br><span class=\"line\">            logWarning(<span class=\"string\">s\"Cannot delete checkpoint file <span class=\"subst\">$file</span>:\"</span>, e)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    <span class=\"comment\">//针对隐反馈，迭代求解</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (implicitPrefs) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">for</span> (iter &lt;- <span class=\"number\">1</span> to maxIter) &#123;  <span class=\"comment\">//迭代总次数maxIter</span></span><br><span class=\"line\">        userFactors.setName(<span class=\"string\">s\"userFactors-<span class=\"subst\">$iter</span>\"</span>).persist(intermediateRDDStorageLevel)</span><br><span class=\"line\">        <span class=\"keyword\">val</span> previousItemFactors = itemFactors</span><br><span class=\"line\">        <span class=\"comment\">//固定user factor，优化item factor</span></span><br><span class=\"line\">        itemFactors = computeFactors(userFactors, userOutBlocks, itemInBlocks, rank, regParam,</span><br><span class=\"line\">          userLocalIndexEncoder, implicitPrefs, alpha, solver)</span><br><span class=\"line\">        previousItemFactors.unpersist()</span><br><span class=\"line\">        itemFactors.setName(<span class=\"string\">s\"itemFactors-<span class=\"subst\">$iter</span>\"</span>).persist(intermediateRDDStorageLevel)</span><br><span class=\"line\">        <span class=\"comment\">// <span class=\"doctag\">TODO:</span> Generalize PeriodicGraphCheckpointer and use it here.</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> deps = itemFactors.dependencies</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (shouldCheckpoint(iter)) &#123;</span><br><span class=\"line\">          itemFactors.checkpoint() <span class=\"comment\">// itemFactors gets materialized in computeFactors</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> previousUserFactors = userFactors</span><br><span class=\"line\">        <span class=\"comment\">//根据item factore, 优化user factor</span></span><br><span class=\"line\">        userFactors = computeFactors(itemFactors, itemOutBlocks, userInBlocks, rank, regParam,</span><br><span class=\"line\">          itemLocalIndexEncoder, implicitPrefs, alpha, solver)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (shouldCheckpoint(iter)) &#123;</span><br><span class=\"line\">          <span class=\"type\">ALS</span>.cleanShuffleDependencies(sc, deps)</span><br><span class=\"line\">          deletePreviousCheckpointFile()</span><br><span class=\"line\">          previousCheckpointFile = itemFactors.getCheckpointFile</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        previousUserFactors.unpersist()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">//针对显示反馈，迭代求解</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> (iter &lt;- <span class=\"number\">0</span> until maxIter) &#123; <span class=\"comment\">//迭代总次数maxIter</span></span><br><span class=\"line\">        <span class=\"comment\">//固定user factor，优化item factor</span></span><br><span class=\"line\">        itemFactors = computeFactors(userFactors, userOutBlocks, itemInBlocks, rank, regParam,</span><br><span class=\"line\">          userLocalIndexEncoder, solver = solver)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (shouldCheckpoint(iter)) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> deps = itemFactors.dependencies</span><br><span class=\"line\">          itemFactors.checkpoint()</span><br><span class=\"line\">          itemFactors.count() <span class=\"comment\">// checkpoint item factors and cut lineage</span></span><br><span class=\"line\">          <span class=\"type\">ALS</span>.cleanShuffleDependencies(sc, deps)</span><br><span class=\"line\">          deletePreviousCheckpointFile()</span><br><span class=\"line\">          previousCheckpointFile = itemFactors.getCheckpointFile</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//根据item factore, 优化user factor</span></span><br><span class=\"line\">        userFactors = computeFactors(itemFactors, itemOutBlocks, userInBlocks, rank, regParam,</span><br><span class=\"line\">          itemLocalIndexEncoder, solver = solver)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//将user id 和 factor拼接在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> userIdAndFactors = userInBlocks</span><br><span class=\"line\">      .mapValues(_.srcIds)</span><br><span class=\"line\">      .join(userFactors)</span><br><span class=\"line\">      .mapPartitions(&#123; items =&gt;</span><br><span class=\"line\">        items.flatMap &#123; <span class=\"keyword\">case</span> (_, (ids, factors)) =&gt;</span><br><span class=\"line\">          ids.view.zip(factors)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      <span class=\"comment\">// Preserve the partitioning because IDs are consistent with the partitioners in userInBlocks</span></span><br><span class=\"line\">      <span class=\"comment\">// and userFactors.</span></span><br><span class=\"line\">      &#125;, preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      .setName(<span class=\"string\">\"userFactors\"</span>)</span><br><span class=\"line\">      .persist(finalRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"comment\">//将item id 和 factor拼接在一起</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> itemIdAndFactors = itemInBlocks</span><br><span class=\"line\">      .mapValues(_.srcIds)</span><br><span class=\"line\">      .join(itemFactors)</span><br><span class=\"line\">      .mapPartitions(&#123; items =&gt;</span><br><span class=\"line\">        items.flatMap &#123; <span class=\"keyword\">case</span> (_, (ids, factors)) =&gt;</span><br><span class=\"line\">          ids.view.zip(factors)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;, preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      .setName(<span class=\"string\">\"itemFactors\"</span>)</span><br><span class=\"line\">      .persist(finalRDDStorageLevel)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (finalRDDStorageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;</span><br><span class=\"line\">      userIdAndFactors.count()</span><br><span class=\"line\">      itemFactors.unpersist()</span><br><span class=\"line\">      itemIdAndFactors.count()</span><br><span class=\"line\">      userInBlocks.unpersist()</span><br><span class=\"line\">      userOutBlocks.unpersist()</span><br><span class=\"line\">      itemInBlocks.unpersist()</span><br><span class=\"line\">      itemOutBlocks.unpersist()</span><br><span class=\"line\">      blockRatings.unpersist()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//返回user factor和item factor数据</span></span><br><span class=\"line\">    (userIdAndFactors, itemIdAndFactors)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"构建哈希器\"><a href=\"#构建哈希器\" class=\"headerlink\" title=\"构建哈希器\"></a>构建哈希器</h3><p>   构建哈希器，用于计算user或item id对应的block编号。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HashPartitioner</span>(<span class=\"params\">partitions: <span class=\"type\">Int</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span> </span>&#123;</span><br><span class=\"line\">  require(partitions &gt;= <span class=\"number\">0</span>, <span class=\"string\">s\"Number of partitions (<span class=\"subst\">$partitions</span>) cannot be negative.\"</span>)</span><br><span class=\"line\">  <span class=\"comment\">//block总数</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">numPartitions</span></span>: <span class=\"type\">Int</span> = partitions</span><br><span class=\"line\">  <span class=\"comment\">//通过求余计算block 编号</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartition</span></span>(key: <span class=\"type\">Any</span>): <span class=\"type\">Int</span> = key <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> <span class=\"literal\">null</span> =&gt; <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">case</span> _ =&gt; <span class=\"type\">Utils</span>.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">//判断2个哈希器是否相等</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">equALS</span></span>(other: <span class=\"type\">Any</span>): <span class=\"type\">Boolean</span> = other <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> h: <span class=\"type\">HashPartitioner</span> =&gt;</span><br><span class=\"line\">      h.numPartitions == numPartitions</span><br><span class=\"line\">    <span class=\"keyword\">case</span> _ =&gt;</span><br><span class=\"line\">      fALSe</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hashCode</span></span>: <span class=\"type\">Int</span> = numPartitions</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"构建地址编码解码器\"><a href=\"#构建地址编码解码器\" class=\"headerlink\" title=\"构建地址编码解码器\"></a>构建地址编码解码器</h3><p>构建地址编码解码器，根据block编号和block内索引对地址进行编码，同时可将编码后地址解码为block编号和block内索引号。具体实现是通过block个数确定block编码需要的二进制位数，以及block内索引位数，通过这些位数利用逻辑操作即可实现地址的编码和解码。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[recommendation] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LocalIndexEncoder</span>(<span class=\"params\">numBlocks: <span class=\"type\">Int</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    require(numBlocks &gt; <span class=\"number\">0</span>, <span class=\"string\">s\"numBlocks must be positive but found <span class=\"subst\">$numBlocks</span>.\"</span>)</span><br><span class=\"line\">    <span class=\"comment\">//block内部索引使用的二进制位数</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">final</span> <span class=\"keyword\">val</span> numLocalIndexBits =</span><br><span class=\"line\">      math.min(java.lang.<span class=\"type\">Integer</span>.numberOfLeadingZeros(numBlocks - <span class=\"number\">1</span>), <span class=\"number\">31</span>)</span><br><span class=\"line\">    <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">final</span> <span class=\"keyword\">val</span> localIndexMask = (<span class=\"number\">1</span> &lt;&lt; numLocalIndexBits) - <span class=\"number\">1</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//根据block编号和block内索引值，对地址编码</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">encode</span></span>(blockId: <span class=\"type\">Int</span>, localIndex: <span class=\"type\">Int</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">      require(blockId &lt; numBlocks)</span><br><span class=\"line\">      require((localIndex &amp; ~localIndexMask) == <span class=\"number\">0</span>)</span><br><span class=\"line\">      (blockId &lt;&lt; numLocalIndexBits) | localIndex</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//根据编码后地址，得到block编号</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">blockId</span></span>(encoded: <span class=\"type\">Int</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">      encoded &gt;&gt;&gt; numLocalIndexBits</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//根据编码地址，得到block内部索引</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">localIndex</span></span>(encoded: <span class=\"type\">Int</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">      encoded &amp; localIndexMask</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"partition-rating\"><a href=\"#partition-rating\" class=\"headerlink\" title=\"partition rating\"></a>partition rating</h3><p>格式化rating数据，将rating数据分块，根据user和product的id哈希后的结果，得到对应的块索引。最终返回（src_block_id, dst_block_id）(src_id数组，dst_id数组，rating数组)</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionRatings</span></span>[<span class=\"type\">ID</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">      ratings: <span class=\"type\">RDD</span>[<span class=\"type\">Rating</span>[<span class=\"type\">ID</span>]],</span><br><span class=\"line\">      srcPart: <span class=\"type\">Partitioner</span>,</span><br><span class=\"line\">      dstPart: <span class=\"type\">Partitioner</span>): <span class=\"type\">RDD</span>[((<span class=\"type\">Int</span>, <span class=\"type\">Int</span>), <span class=\"type\">RatingBlock</span>[<span class=\"type\">ID</span>])] = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//获得总block数</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> numPartitions = srcPart.numPartitions * dstPart.numPartitions</span><br><span class=\"line\">    <span class=\"comment\">//在rating的每个分区，计算每个rating元素对应的src_block_id和dst_block_id, 并放到对应的块索引中。然后，对所有分区的元素按照块索引进行聚合，并返回聚合结果</span></span><br><span class=\"line\">    ratings.mapPartitions &#123; iter =&gt;</span><br><span class=\"line\">      <span class=\"comment\">//生成numPartitions个一维数组，存储对应block的rating记录</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> builders = <span class=\"type\">Array</span>.fill(numPartitions)(<span class=\"keyword\">new</span> <span class=\"type\">RatingBlockBuilder</span>[<span class=\"type\">ID</span>])</span><br><span class=\"line\">      iter.flatMap &#123; r =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> srcBlockId = srcPart.getPartition(r.user) <span class=\"comment\">//user block id</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> dstBlockId = dstPart.getPartition(r.item) <span class=\"comment\">//item block id</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> idx = srcBlockId + srcPart.numPartitions * dstBlockId <span class=\"comment\">//数组索引计算</span></span><br><span class=\"line\">        <span class=\"comment\">//将对应的rating元素放在builders对应元素中</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> builder = builders(idx) </span><br><span class=\"line\">        builder.add(r) </span><br><span class=\"line\">        <span class=\"keyword\">if</span> (builder.size &gt;= <span class=\"number\">2048</span>) &#123; <span class=\"comment\">// 2048 * (3 * 4) = 24k</span></span><br><span class=\"line\">          <span class=\"comment\">//如果某个block内数据量较多，直接得到结果</span></span><br><span class=\"line\">          builders(idx) = <span class=\"keyword\">new</span> <span class=\"type\">RatingBlockBuilder</span></span><br><span class=\"line\">          <span class=\"type\">Iterator</span>.single(((srcBlockId, dstBlockId), builder.build()))</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">          <span class=\"type\">Iterator</span>.empty</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; ++ &#123;</span><br><span class=\"line\">        <span class=\"comment\">//对builders数组内元素，计算对应的src_block_id和dst_block_id,并将对应rating数据放在其中</span></span><br><span class=\"line\">        builders.view.zipWithIndex.filter(_._1.size &gt; <span class=\"number\">0</span>).map &#123; <span class=\"keyword\">case</span> (block, idx) =&gt;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> srcBlockId = idx % srcPart.numPartitions</span><br><span class=\"line\">          <span class=\"keyword\">val</span> dstBlockId = idx / srcPart.numPartitions</span><br><span class=\"line\">          ((srcBlockId, dstBlockId), block.build())</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.groupByKey().mapValues &#123; blocks =&gt;</span><br><span class=\"line\">      <span class=\"comment\">//对不同分区计算出的的rating元素进行聚合</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> builder = <span class=\"keyword\">new</span> <span class=\"type\">RatingBlockBuilder</span>[<span class=\"type\">ID</span>]</span><br><span class=\"line\">      blocks.foreach(builder.merge)</span><br><span class=\"line\">      builder.build() <span class=\"comment\">//value为 （src_id数组，dst_id数组，对应的rating数组）</span></span><br><span class=\"line\">    &#125;.setName(<span class=\"string\">\"ratingBlocks\"</span>)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"构造in-block-和out-block\"><a href=\"#构造in-block-和out-block\" class=\"headerlink\" title=\"构造in_block, 和out_block\"></a>构造in_block, 和out_block</h3><p>在分布式计算中，不同节点的通信是影响程序效率重要原因，通过合理的设计分区，使得不同节点交换数据尽量少，可以有效的提升运行效率。</p>\n<p>由上述章节中对目标函数求解推导，可以得知，每个用户向量的计算依赖于所有和它关联的item向量。如果不做任何优化，则每次优化user向量时，所有user向量的计算，都需要从其他节点得到对应item向量。如果节点A上有多个user和节点B上的某一item关联，则节点B需要向节点A传输多次item向量数据，实际上这是不必要的。优化的思路是，通过合理的分区，提前计算好所有节点需要从其它节点获取的item向量数据，将其缓存在本地，计算每个user向量时，直接从本地读取，可以大大减少需要传输的数据量，提升程序执行的效率。</p>\n<p>在源码中，通过out block缓存当前节点需要向其它节点传输的数据， in block用于缓存当前节点需要的数据索引。当其他节点信息传输到本地时，通过读取in block内索引信息，来从本地获取其它节点传过来的数据。更加详细的描述可参考【7】</p>\n<p>in block 结构： （block_id, Inblock(src_id数组, src_ptr, dst_id地址数组， rating数组）)<br>out block结构： （block_id， array[array[int]]） （二维数组存储发往每个block的src_id索引）</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">makeBlocks</span></span>[<span class=\"type\">ID</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">      prefix: <span class=\"type\">String</span>,</span><br><span class=\"line\">      ratingBlocks: <span class=\"type\">RDD</span>[((<span class=\"type\">Int</span>, <span class=\"type\">Int</span>), <span class=\"type\">RatingBlock</span>[<span class=\"type\">ID</span>])],</span><br><span class=\"line\">      srcPart: <span class=\"type\">Partitioner</span>,</span><br><span class=\"line\">      dstPart: <span class=\"type\">Partitioner</span>,</span><br><span class=\"line\">      storageLevel: <span class=\"type\">StorageLevel</span>)(</span><br><span class=\"line\">      <span class=\"keyword\">implicit</span> srcOrd: <span class=\"type\">Ordering</span>[<span class=\"type\">ID</span>]): (<span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">InBlock</span>[<span class=\"type\">ID</span>])], <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">OutBlock</span>)]) = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//根据ratingBlocks.map计算inBlocks</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> inBlocks = ratingBlocks.map &#123;</span><br><span class=\"line\">      <span class=\"keyword\">case</span> ((srcBlockId, dstBlockId), <span class=\"type\">RatingBlock</span>(srcIds, dstIds, ratings)) =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> start = <span class=\"type\">System</span>.nanoTime()</span><br><span class=\"line\">        <span class=\"comment\">//dst id去重复</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> dstIdSet = <span class=\"keyword\">new</span> <span class=\"type\">OpenHashSet</span>[<span class=\"type\">ID</span>](<span class=\"number\">1</span> &lt;&lt; <span class=\"number\">20</span>) </span><br><span class=\"line\">        dstIds.foreach(dstIdSet.add)  </span><br><span class=\"line\">        <span class=\"comment\">//dst id 去重结果进行排序</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> sortedDstIds = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">ID</span>](dstIdSet.size)</span><br><span class=\"line\">        <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">var</span> pos = dstIdSet.nextPos(<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (pos != <span class=\"number\">-1</span>) &#123;</span><br><span class=\"line\">          sortedDstIds(i) = dstIdSet.getValue(pos)</span><br><span class=\"line\">          pos = dstIdSet.nextPos(pos + <span class=\"number\">1</span>)</span><br><span class=\"line\">          i += <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        assert(i == dstIdSet.size)</span><br><span class=\"line\">        <span class=\"type\">Sorting</span>.quickSort(sortedDstIds)</span><br><span class=\"line\">        <span class=\"comment\">//得到dst id 对应的去重和排序后的索引值</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> dstIdToLocalIndex = <span class=\"keyword\">new</span> <span class=\"type\">OpenHashMap</span>[<span class=\"type\">ID</span>, <span class=\"type\">Int</span>](sortedDstIds.length)</span><br><span class=\"line\">        i = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (i &lt; sortedDstIds.length) &#123;</span><br><span class=\"line\">          dstIdToLocalIndex.update(sortedDstIds(i), i)</span><br><span class=\"line\">          i += <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        logDebug(</span><br><span class=\"line\">          <span class=\"string\">\"Converting to local indices took \"</span> + (<span class=\"type\">System</span>.nanoTime() - start) / <span class=\"number\">1e9</span> + <span class=\"string\">\" seconds.\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">val</span> dstLocalIndices = dstIds.map(dstIdToLocalIndex.apply)</span><br><span class=\"line\">        (srcBlockId, (dstBlockId, srcIds, dstLocalIndices, ratings))</span><br><span class=\"line\">    &#125;.groupByKey(<span class=\"keyword\">new</span> <span class=\"type\">ALSPartitioner</span>(srcPart.numPartitions)) <span class=\"comment\">//根据src block id进行聚合</span></span><br><span class=\"line\">      .mapValues &#123; iter =&gt;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> builder =</span><br><span class=\"line\">          <span class=\"keyword\">new</span> <span class=\"type\">UncompressedInBlockBuilder</span>[<span class=\"type\">ID</span>](<span class=\"keyword\">new</span> <span class=\"type\">LocalIndexEncoder</span>(dstPart.numPartitions))</span><br><span class=\"line\">        <span class=\"comment\">//将dstBlockId和dstLocalIndices编码，并汇总数据</span></span><br><span class=\"line\">        iter.foreach &#123; <span class=\"keyword\">case</span> (dstBlockId, srcIds, dstLocalIndices, ratings) =&gt;</span><br><span class=\"line\">          builder.add(dstBlockId, srcIds, dstLocalIndices, ratings)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//对结果进行压缩存储，结果格式为（uniqueSrcId数组, dstPtrs数组, dstEncodedIndices数组, ratings数组）</span></span><br><span class=\"line\">        builder.build().compress()</span><br><span class=\"line\">      &#125;.setName(prefix + <span class=\"string\">\"InBlocks\"</span>)</span><br><span class=\"line\">      .persist(storageLevel)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"comment\">//根据inBlocks计算outBlocks</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> outBlocks = inBlocks.mapValues &#123; <span class=\"keyword\">case</span> <span class=\"type\">InBlock</span>(srcIds, dstPtrs, dstEncodedIndices, _) =&gt;</span><br><span class=\"line\">      <span class=\"comment\">//构造编码器</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> encoder = <span class=\"keyword\">new</span> <span class=\"type\">LocalIndexEncoder</span>(dstPart.numPartitions)</span><br><span class=\"line\">      <span class=\"comment\">//定义ArrayBuilder数组，存储发往每个out block的 src id信息</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> activeIds = <span class=\"type\">Array</span>.fill(dstPart.numPartitions)(mutable.<span class=\"type\">ArrayBuilder</span>.make[<span class=\"type\">Int</span>])</span><br><span class=\"line\">      <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> seen = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Boolean</span>](dstPart.numPartitions)</span><br><span class=\"line\">      <span class=\"comment\">//依次计算当前src id是否发往每一个block id</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (i &lt; srcIds.length) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">var</span> j = dstPtrs(i)</span><br><span class=\"line\">        ju.<span class=\"type\">Arrays</span>.fill(seen, fALSe)</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (j &lt; dstPtrs(i + <span class=\"number\">1</span>)) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> dstBlockId = encoder.blockId(dstEncodedIndices(j))</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!seen(dstBlockId)) &#123;</span><br><span class=\"line\">            activeIds(dstBlockId) += i <span class=\"comment\">// add the local index in this out-block</span></span><br><span class=\"line\">            seen(dstBlockId) = <span class=\"literal\">true</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          j += <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        i += <span class=\"number\">1</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      activeIds.map &#123; x =&gt;</span><br><span class=\"line\">        x.result()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.setName(prefix + <span class=\"string\">\"OutBlocks\"</span>)</span><br><span class=\"line\">      .persist(storageLevel)</span><br><span class=\"line\">    (inBlocks, outBlocks)  <span class=\"comment\">//返回结果</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"inblock-compress\"><a href=\"#inblock-compress\" class=\"headerlink\" title=\"inblock compress\"></a>inblock compress</h4><p>  对inblock 中间结果压缩存储，返回结果格式为（uniqueSrcId数组, dstPtrs数组, dstEncodedIndices数组, ratings数组）</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compress</span></span>(): <span class=\"type\">InBlock</span>[<span class=\"type\">ID</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> sz = length</span><br><span class=\"line\">  assert(sz &gt; <span class=\"number\">0</span>, <span class=\"string\">\"Empty in-link block should not exist.\"</span>)</span><br><span class=\"line\">  sort()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> uniqueSrcIdsBuilder = mutable.<span class=\"type\">ArrayBuilder</span>.make[<span class=\"type\">ID</span>]</span><br><span class=\"line\">  <span class=\"keyword\">val</span> dstCountsBuilder = mutable.<span class=\"type\">ArrayBuilder</span>.make[<span class=\"type\">Int</span>]</span><br><span class=\"line\">  <span class=\"keyword\">var</span> preSrcId = srcIds(<span class=\"number\">0</span>)</span><br><span class=\"line\">  uniqueSrcIdsBuilder += preSrcId</span><br><span class=\"line\">  <span class=\"keyword\">var</span> curCount = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> i = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">var</span> j = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"comment\">//得到去重后的src id数组， 以及每个src id的数量</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (i &lt; sz) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> srcId = srcIds(i)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (srcId != preSrcId) &#123;</span><br><span class=\"line\">      uniqueSrcIdsBuilder += srcId</span><br><span class=\"line\">      dstCountsBuilder += curCount</span><br><span class=\"line\">      preSrcId = srcId</span><br><span class=\"line\">      j += <span class=\"number\">1</span></span><br><span class=\"line\">      curCount = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    curCount += <span class=\"number\">1</span></span><br><span class=\"line\">    i += <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  dstCountsBuilder += curCount</span><br><span class=\"line\">  <span class=\"keyword\">val</span> uniqueSrcIds = uniqueSrcIdsBuilder.result()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> numUniqueSrdIds = uniqueSrcIds.length</span><br><span class=\"line\">  <span class=\"keyword\">val</span> dstCounts = dstCountsBuilder.result()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> dstPtrs = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Int</span>](numUniqueSrdIds + <span class=\"number\">1</span>)</span><br><span class=\"line\">  <span class=\"keyword\">var</span> sum = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"comment\">//将src id和dst id关系通过dstPtrs进行压缩存储</span></span><br><span class=\"line\">  i = <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (i &lt; numUniqueSrdIds) &#123;</span><br><span class=\"line\">    sum += dstCounts(i)</span><br><span class=\"line\">    i += <span class=\"number\">1</span></span><br><span class=\"line\">    dstPtrs(i) = sum</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"type\">InBlock</span>(uniqueSrcIds, dstPtrs, dstEncodedIndices, ratings)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"computeFactor\"><a href=\"#computeFactor\" class=\"headerlink\" title=\"computeFactor\"></a>computeFactor</h3><p>  根据srcFactorBlocks、srcOutBlocks、dstInBlocks, 计算dstFactorBlocks</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeFactors</span></span>[<span class=\"type\">ID</span>](</span><br><span class=\"line\">    srcFactorBlocks: <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">FactorBlock</span>)],</span><br><span class=\"line\">    srcOutBlocks: <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">OutBlock</span>)],</span><br><span class=\"line\">    dstInBlocks: <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">InBlock</span>[<span class=\"type\">ID</span>])],</span><br><span class=\"line\">    rank: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    regParam: <span class=\"type\">Double</span>,</span><br><span class=\"line\">    srcEncoder: <span class=\"type\">LocalIndexEncoder</span>,</span><br><span class=\"line\">    implicitPrefs: <span class=\"type\">Boolean</span> = fALSe,</span><br><span class=\"line\">    alpha: <span class=\"type\">Double</span> = <span class=\"number\">1.0</span>,</span><br><span class=\"line\">    solver: <span class=\"type\">LeastSquaresNESolver</span>): <span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">FactorBlock</span>)] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> numSrcBlocks = srcFactorBlocks.partitions.length  <span class=\"comment\">//src block数量</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">YtY</span> = <span class=\"keyword\">if</span> (implicitPrefs) <span class=\"type\">Some</span>(computeYtY(srcFactorBlocks, rank)) <span class=\"keyword\">else</span> <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"comment\">//根据srcOut，得到每个dstBlock对应的srcBlockID 和srcFactor数组</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> srcOut = srcOutBlocks.join(srcFactorBlocks).flatMap &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> (srcBlockId, (srcOutBlock, srcFactors)) =&gt;</span><br><span class=\"line\">      </span><br><span class=\"line\">      srcOutBlock.view.zipWithIndex.map &#123; <span class=\"keyword\">case</span> (activeIndices, dstBlockId) =&gt;</span><br><span class=\"line\">        (dstBlockId, (srcBlockId, activeIndices.map(idx =&gt; srcFactors(idx))))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">//根据dstBlockId 对srcBlockID, array[srcFactor]进行聚合</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> merged = srcOut.groupByKey(<span class=\"keyword\">new</span> <span class=\"type\">ALSPartitioner</span>(dstInBlocks.partitions.length))</span><br><span class=\"line\">  <span class=\"comment\">//对每个dstBlockID, 计算其中每个dstID对应的隐语义向量</span></span><br><span class=\"line\">  dstInBlocks.join(merged).mapValues &#123;</span><br><span class=\"line\">    <span class=\"keyword\">case</span> (<span class=\"type\">InBlock</span>(dstIds, srcPtrs, srcEncodedIndices, ratings), srcFactors) =&gt;</span><br><span class=\"line\">      <span class=\"comment\">//得到每个block对应的src factor向量集合</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sortedSrcFactors = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">FactorBlock</span>](numSrcBlocks)</span><br><span class=\"line\">      srcFactors.foreach &#123; <span class=\"keyword\">case</span> (srcBlockId, factors) =&gt;</span><br><span class=\"line\">        sortedSrcFactors(srcBlockId) = factors</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"comment\">//对每个dstID, 获取对应的srcFactor及对应rating, 计算该dstID对应的隐语义向量</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> dstFactors = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Array</span>[<span class=\"type\">Float</span>]](dstIds.length)</span><br><span class=\"line\">      <span class=\"keyword\">var</span> j = <span class=\"number\">0</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> ls = <span class=\"keyword\">new</span> <span class=\"type\">NormalEquation</span>(rank)</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (j &lt; dstIds.length) &#123;</span><br><span class=\"line\">        ls.reset()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (implicitPrefs) &#123;</span><br><span class=\"line\">          ls.merge(<span class=\"type\">YtY</span>.get)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">var</span> i = srcPtrs(j)</span><br><span class=\"line\">        <span class=\"keyword\">var</span> numExplicits = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (i &lt; srcPtrs(j + <span class=\"number\">1</span>)) &#123; <span class=\"comment\">//依次得到每个srcFactor及rating值</span></span><br><span class=\"line\">          <span class=\"keyword\">val</span> encoded = srcEncodedIndices(i)</span><br><span class=\"line\">          <span class=\"keyword\">val</span> blockId = srcEncoder.blockId(encoded)</span><br><span class=\"line\">          <span class=\"keyword\">val</span> localIndex = srcEncoder.localIndex(encoded)</span><br><span class=\"line\">          <span class=\"comment\">//sortedSrcFactors通过blockId和localIndex进行索引，得到需要的factor向量。之前这里困惑挺久，一直感觉从srcOut传过来的factor向量只是一个子集，通过localIndex访问不正确，实际上这里的localIndex和srcOut那里存储的localindex是不需要对应的。因为同一个src id 本身的src local index不等于其它block对应的 dst localindex</span></span><br><span class=\"line\">          <span class=\"keyword\">val</span> srcFactor = sortedSrcFactors(blockId)(localIndex)</span><br><span class=\"line\">          <span class=\"keyword\">val</span> rating = ratings(i)</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (implicitPrefs) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Extension to the original paper to handle b &lt; 0. confidence is a function of |b|</span></span><br><span class=\"line\">            <span class=\"comment\">// instead so that it is never negative. c1 is confidence - 1.0.</span></span><br><span class=\"line\">            <span class=\"keyword\">val</span> c1 = alpha * math.abs(rating)</span><br><span class=\"line\">            <span class=\"comment\">// For rating &lt;= 0, the corresponding preference is 0. So the term below is only added</span></span><br><span class=\"line\">            <span class=\"comment\">// for rating &gt; 0. Because YtY is already added, we need to adjust the scaling here.</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (rating &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">              numExplicits += <span class=\"number\">1</span></span><br><span class=\"line\">              ls.add(srcFactor, (c1 + <span class=\"number\">1.0</span>) / c1, c1)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            ls.add(srcFactor, rating)</span><br><span class=\"line\">            numExplicits += <span class=\"number\">1</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          i += <span class=\"number\">1</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// Weight lambda by the number of explicit ratings based on the ALS-WR paper.</span></span><br><span class=\"line\">        dstFactors(j) = solver.solve(ls, numExplicits * regParam)</span><br><span class=\"line\">        j += <span class=\"number\">1</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      dstFactors</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"ALS-模型推荐\"><a href=\"#ALS-模型推荐\" class=\"headerlink\" title=\"ALS 模型推荐\"></a>ALS 模型推荐</h2><p><strong>模型参数</strong> </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val rank: Int,      //隐语义个数</span><br><span class=\"line\">val userFeatures: RDD[(Int, Array[Double])], //user factor数组, 存储user id 及对应的factor向量</span><br><span class=\"line\">val productFeatures: RDD[(Int, Array[Double])]) //item factor数组，存储item id及对应的factor向量</span><br></pre></td></tr></table></figure>\n<p><strong>对所有用户进行推荐</strong></p>\n<p>调用recommendForAll函数，首先对user向量和item向量分块并以矩阵形式存储，然后对二者做笛卡尔积，并计算每个user和每个item的得分，最终以user为key, 取topK个item及对应的得分，作为推荐结果. 计算topK时借助于小顶堆</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def recommendForAll(</span><br><span class=\"line\">    rank: Int,</span><br><span class=\"line\">    srcFeatures: RDD[(Int, Array[Double])],</span><br><span class=\"line\">    dstFeatures: RDD[(Int, Array[Double])],</span><br><span class=\"line\">    num: Int): RDD[(Int, Array[(Int, Double)])] = &#123;</span><br><span class=\"line\">  //对user向量和item向量分块并以矩阵形式存储</span><br><span class=\"line\">  val srcBlocks = blockify(rank, srcFeatures)</span><br><span class=\"line\">  val dstBlocks = blockify(rank, dstFeatures)</span><br><span class=\"line\">  //笛卡尔积，依次对每个组合计算user对item的偏好</span><br><span class=\"line\">  val ratings = srcBlocks.cartesian(dstBlocks).flatMap &#123;</span><br><span class=\"line\">    case ((srcIds, srcFactors), (dstIds, dstFactors)) =&gt;</span><br><span class=\"line\">      val m = srcIds.length</span><br><span class=\"line\">      val n = dstIds.length</span><br><span class=\"line\">      val ratings = srcFactors.transpose.multiply(dstFactors)</span><br><span class=\"line\">      val output = new Array[(Int, (Int, Double))](m * n)</span><br><span class=\"line\">      var k = 0</span><br><span class=\"line\">      ratings.foreachActive &#123; (i, j, r) =&gt;</span><br><span class=\"line\">        output(k) = (srcIds(i), (dstIds(j), r))</span><br><span class=\"line\">        k += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      output.toSeq</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  //根据user id作为key, 得到喜好分数最高的num个item</span><br><span class=\"line\">  ratings.topByKey(num)(Ordering.by(_._2))</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 对user向量和item向量分块并以矩阵形式存储, 结果的每个元组分别是对应的id数组和factor构成的矩阵</span><br><span class=\"line\">private def blockify(</span><br><span class=\"line\">    rank: Int,</span><br><span class=\"line\">    features: RDD[(Int, Array[Double])]): RDD[(Array[Int], DenseMatrix)] = &#123;</span><br><span class=\"line\">  val blockSize = 4096 // TODO: tune the block size</span><br><span class=\"line\">  val blockStorage = rank * blockSize</span><br><span class=\"line\">  features.mapPartitions &#123; iter =&gt;</span><br><span class=\"line\">    iter.grouped(blockSize).map &#123; grouped =&gt;</span><br><span class=\"line\">      val ids = mutable.ArrayBuilder.make[Int]</span><br><span class=\"line\">      ids.sizeHint(blockSize)</span><br><span class=\"line\">      val factors = mutable.ArrayBuilder.make[Double]</span><br><span class=\"line\">      factors.sizeHint(blockStorage)</span><br><span class=\"line\">      var i = 0</span><br><span class=\"line\">      grouped.foreach &#123; case (id, factor) =&gt;</span><br><span class=\"line\">        ids += id</span><br><span class=\"line\">        factors ++= factor</span><br><span class=\"line\">        i += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      (ids.result(), new DenseMatrix(rank, i, factors.result()))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"ALS推荐实践\"><a href=\"#ALS推荐实践\" class=\"headerlink\" title=\"ALS推荐实践\"></a>ALS推荐实践</h1><p>我们的平台是图片社交，每个用户都可以在平台上浏览图片，并进行点赞、评论等。推荐算法主要用于给用户推荐其最可能感兴趣的图片，最终提升用户体验。</p>\n<h2 id=\"离线实验\"><a href=\"#离线实验\" class=\"headerlink\" title=\"离线实验\"></a>离线实验</h2><p>我们平台暂时无法得到用户的显式评分数据，但是可以得到用户点击、点赞、评论等相关行为信息。因此，比较适合用隐反馈矩阵分解模型。</p>\n<h3 id=\"构造数据集\"><a href=\"#构造数据集\" class=\"headerlink\" title=\"构造数据集\"></a>构造数据集</h3><ul>\n<li><p>数据预处理</p>\n<p>从2周的用户行为数据中，过滤无行为用户数据，spam图片数据和spam用户数据。</p>\n</li>\n<li><p>构建rating元素</p>\n<p>对预处理之后的数据，根据用户每天的图片交互行为，分别对点击、点赞和评论等分别赋予不同的权值，得到rating矩阵. </p>\n</li>\n<li><p>生成训练集和测试集</p>\n<p>对于得到的rating数据，随机划分为两部分 $A:B = 7:3$，如果分别直接作为训练集和测试集是有问题的，因为$B$中的user或者item是有可能在$A$中没有出现过，这样会影响评估结果。 我们采用的方法是如果B数据中某个rating元素的user或item没有在A出现，则将该元素放到$A$中用作训练集。最终$A$和新加进来的元素共同构成训练集$A^1$， $B$留下的数据 $B^1$ 作为测试集。</p>\n</li>\n</ul>\n<h3 id=\"离线训练和评估\"><a href=\"#离线训练和评估\" class=\"headerlink\" title=\"离线训练和评估\"></a>离线训练和评估</h3><ul>\n<li>离线训练</li>\n</ul>\n<p>利用spark mllib库，对训练集构成的rating矩阵，建立隐反馈矩阵分解模型，并完成进行矩阵分解，生成user factor和item factor。</p>\n<ul>\n<li>评估</li>\n</ul>\n<p>调用模型的recommendForAll函数，对测试集所有user进行item推荐，并计算召回率和准确率。根据召回率和准确率，进行参数优化。</p>\n<ul>\n<li>评估指标</li>\n</ul>\n<p>假定$P_i$为用户$i$的预测结果，$P$为所有的预测结果，每个结果记录格式为（user, item）， $T$为测试集,每条记录格式为（user， item）。各种指标的的计算如下：</p>\n<p>召回率: $R= \\frac{|P \\bigcap T|}  {|T|}$</p>\n<p>准确率: $P= \\frac{|P \\bigcap T|}  {|P|}$</p>\n<p>F1:  $F= \\frac{2PR}  {P+R} $</p>\n<p>离散度：$\\frac{1}{N^2}\\sum_i\\sum_j\\frac{|P_i \\bigcap P_j|}{|P_i \\bigcup P_j|}$</p>\n<p>除了上述指标之外，我们还对用户连续多天推荐结果的差异性、用户覆盖率、图片覆盖率等指标进行评估。</p>\n<h2 id=\"在线ab测试\"><a href=\"#在线ab测试\" class=\"headerlink\" title=\"在线ab测试\"></a>在线ab测试</h2><p>abtest方案： 将als算法计算出的结果，定期写入到线上，作为线上的一种推荐来源。对实验组用户同时采用新策略和旧策略进行推荐，对照组用户只采用旧策略进行推荐。</p>\n<p>从2个维度进行评估：</p>\n<ul>\n<li>评估实验组和对照组用户在abtest上线前后点击率</li>\n<li>评估实验组用户在新旧两种策略推荐图片的点击率</li>\n</ul>\n<p>测试一定时间后，交换对照组和实验组用户，按照上述2个维度重新进行评估</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>【1】Y Koren，R Bell，C Volinsky, “Matrix Factorization Techniques for Recommender Systems”, 《Computer》, 2009.08; 42(8):30-37 </p>\n<p>【2】洪亮劼, “知人知面需知心——人工智能技术在推荐系统中的应用”, 2016.11, <a href=\"http://mp.weixin.qq.com/s/JuaM8d52-f8AzTjEPnCl7g\" target=\"_blank\" rel=\"noopener\">http://mp.weixin.qq.com/s/JuaM8d52-f8AzTjEPnCl7g</a></p>\n<p>【3】S. Funk, “Netflix Update: Try This at Home”, 2006.12, <a href=\"http://sifter.org/~simon/journal/20061211.html\" target=\"_blank\" rel=\"noopener\">http://sifter.org/~simon/journal/20061211.html</a></p>\n<p>【4】Y. Koren, “Factorization Meets the Neighborhood: A Mul-tifaceted Collaborative Filtering Model”, Proc. 14th ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining, ACM Press, 2008, pp.426-434</p>\n<p>【5】A. Paterek, “Improving Regularized Singular Value De-composition for Collaborative Filtering” Proc. KDD Cup and Workshop, ACM Press, 2007, pp.39-42</p>\n<p>【6】G. Takács et al., “Major Components of the Gravity Recom- mendation System”, SIGKDD Explorations, 2007.09, vol.9, pp.80-84</p>\n<p>【7】孟祥瑞, “ALS 在 Spark MLlib 中的实现”, 2015.05, <a href=\"http://www.csdn.net/article/2015-05-07/2824641\" target=\"_blank\" rel=\"noopener\">http://www.csdn.net/article/2015-05-07/2824641</a></p>\n<p>【8】Zhen-ming Yuan, et al., “A microblog recommendation algorithm based on social tagging and a temporal interest evolution model”, Frontiers of Information Technology &amp; Electronic Engineering, 2015.07,<br>Volume 16, Issue 7, pp 532–540 </p>\n<p>【9】Z Zhao, Z Cheng, L Hong, EH Chi, “Improving User Topic Interest Profiles by Behavior Factorization”, Proceedings of the 24th International Conference on World Wide Web, 2015.05, pp.1406-1416</p>\n<p>【10】阿里技术，”淘宝搜索/推荐系统背后深度强化学习与自适应在线学习的实践之路”, 2017.02, <a href=\"http://url.cn/451740J\" target=\"_blank\" rel=\"noopener\">http://url.cn/451740J</a></p>\n<p>【11】HT Cheng, L Koc, J Harmsen, T Shaked, “Wide &amp; Deep Learning for Recommender Systems”, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, 2016.09,  pp.7-10</p>\n<p>【12】黄安埠, “递归的艺术 - 深度递归网络在序列式推荐的应用”, 2016.10, <a href=\"http://mp.weixin.qq.com/s?__biz=MzA3MDQ4MzQzMg==&amp;mid=2665690422&amp;idx=1&amp;sn=9bd671983a85286149b51c908b686899&amp;chksm=842bb9b1b35c30a7eedb8d03e173aa8f43465db90e11075ac0c73b1784582f21eb93dcbd3e65&amp;scene=0%23wechat_redirect\" target=\"_blank\" rel=\"noopener\">http://mp.weixin.qq.com/s?__biz=MzA3MDQ4MzQzMg==&amp;mid=2665690422&amp;idx=1&amp;sn=9bd671983a85286149b51c908b686899&amp;chksm=842bb9b1b35c30a7eedb8d03e173aa8f43465db90e11075ac0c73b1784582f21eb93dcbd3e65&amp;scene=0%23wechat_redirect</a></p>\n"},{"title":"Deep Neural Networks for YouTube Recommendations论文学习","date":"2017-08-19T16:00:00.000Z","toc":true,"description":"论文来自YouTube团队使用DNN进行推荐方面的尝试，发表在16年9月的RecSys会议","mathjax":true,"_content":"\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n\n\n\nYouTube采用的推荐系统，是当前工业界最大最复杂的推荐系统之一，服务于上十亿的用户，目的是从不断增加的的海量的视频集合中，为每个用户推荐个性化的内容。\nYouTube的个性化推荐系统主要存在三方面的挑战：\n\n**数据规模:** 由于YouTube庞大的数据规模，很多在小数据集上非常有效的推荐算法无法使用，需要更加特殊的分布式学习算法和在线推荐服务系统。\n\n**新颖度：** 每一秒都有很多的视频上传到YouTube, 推荐系统需要及时考虑这些新上传的视频以及用户在视频上的新操作等，这就需要从 exploration/exploitation的角度去权衡。 \n\n**数据特点和噪声：** 由于用户行为的稀疏性以及一些不确定的额外因素干扰，通过用户的历史行为进行推荐是非常困难的；在YouTube上很少能获得用户的显示喜欢的内容，因此只能根据隐式反馈信号进行建模；视频内容的元数据不够结构化；推荐算法的设计需要对所有这些数据特点都是鲁棒的。\n\n为了保持和google内其他产品的一致，YouTube内部正在做的一件事情是将深度学习作为一种通用的解决方案，用于解决几乎所有的机器学习问题。该论文成果建立在google brain开源的tensorflow之上。tensorflow提供了可以大规模训练各种网络结构的实验框架，本篇论文实验使用了几千亿的样本，训练十亿的参数。\n\n\n# 系统概述\n\n如图1[1]所示，整个系统包括两个深度神经网络, 一个用于生成候选集，另一个用于排序。\n\n <center>![“推荐系统结构”](/dnn_for_youtube_recommandation/recommand_system.png) </center>\n\n图1：**推荐系统结构：包括触发和排序2个模块，触发模块用于从海量的数据中生成较少的候选集，排序模块对触发结果做更精细的选择**\n\n触发模块通过输入用户的历史行为，从全量的集合里面，检索出视一个较小的子集（几百个），该模块通过协同过滤的方式，提供了相对弱一些的的个性化。用户之间的相似性通过观看的视频id, 搜索query的分词结果，以及用户的人口属性信息等来衡量。\n\n排序模块针对触发结果列表，做更加精细的筛选。排序模块采用更加丰富的特征用于描述用户和视频，通过使用特定目标函数训练得到的模型，对视频进行打分。最后得分最高的一些视频推荐给当前的用户。\n\n通过结合触发和排序两个模块，具有两个明显的优势：（1）可以使得从很大的候选集合内，以较小的时间成本推荐个性化的内容给用户（2）可以融合多个算法的结果进行排序（具体使用时，可以将每个算法的结果合并，然后利用排序模块进行打分，取最高得分的视频推荐给用户，较好地实现多推荐算法融合）。\n  \n \n# 触发模块\n\n在触发阶段，会将海量的视频集合进行初步过滤，剩余几百个用户最可能喜欢的视频。\n\n## 用分类的思想对推荐建模\n\n我们把推荐问题建模为一个分类问题，预测在当前时刻$t$, 给定用户$u$和上下文$c$, 预测要观看的video的类别（有多少个候选video, 就有多少个类别）。如下式所示：\n\n$P(w\\_t=i|U,C)=\\frac{e^{v\\_i\\,u}}{\\sum\\_{j\\in V} \\; e^{v\\_j\\,u}}\\;\\;(式1)$ \n\n其中$u$表示的用户和上下文的隐语义向量，$v\\_j$表示第$j$个候选video的隐语义向量。隐语义向量的获取实际上是把一些稀疏的实体信息（如user, video等）映射为N维空间的稠密实向量，而DNN的作用就是从用户的浏览历史和上下文中学习隐语义向量$u$, $u$进一步可用于在$softmax$分类器中预测对应的video类别（即当前用户在上下文环境C时,最可能看的video）. \n\n尽管YouTube上存在显式反馈数据，但是论文中依然使用隐反馈数据，当用户看完一个视频则认为是正样本。这是由于用户的显示反馈行为是非常稀疏的，无法较好地进行模型的训练和预测，而用户存在大量的隐反馈行为，通过采用隐反馈数据，可以有效地以完成模型的训练和预测。\n\n**高效的多类别训练：**为了有效地完成数十亿类别的模型训练，论文采用了一种负采样的方法，首选根据负样本的分布采样负样本，然后通过重要性加权进行纠正，具体使用的方法可以参见文献【2】。对于每个样本，其真实的标签和负采样的类别标识都通过最小化交叉熵优化模型。实际上，每次都采样几千个样本，相对于传统的softmax, 其训练速度提升100倍以上。另外一种可行的高效训练方法是$hierarchical softmax$，但由于遍历每个节点等于在将无关的类别进行分类，这使得分类更加复杂，有损模型的效果。\n\n在线服务阶段，需要在几十毫秒内计算当前用户最可能喜欢的$N$个video, 这需要一个近似的得分方案，使得时间延迟对于类别数量的增长是低于线性的。论文的采用的是基于hash的方法【3】，从点积空间寻找最近的$N$个类别作为推荐的结果。\n\n## 模型架构\n\n对于每个video, 学习其在固定词典上的高维嵌入向量，并把这些向量作为神经网络的输入。每个用户的观看历史可用一个变长的video id序列来表示，每个video id可以通过嵌入的方式得到一个稠密的向量。这些向量通过聚合操作，得到单个向量（实验表示通过对所有向量求平均是最好的策略），作为神经网络的输入。这些video的嵌入向量，和其他的模型参数，都通过误差反向传播，并利用梯度下降方法进行学习。图2是触发模块的模型架构，其输入同时包括观看的video特征和和其他各种非video特征。\n\n<center>\n![“候选推荐”](/dnn_for_youtube_recommandation/recommand_matching.png) \n</center>\n\n图2：**触发模块结构图：嵌入的稀疏特征和和非稠密特征同时作为模型的输入，同一个用户的多个嵌入特征通过平均的方式，得到固定的尺寸，并输入到隐藏层，所有的隐藏层都是全连接。在模型训练阶段，通过最小化交叉熵，并采用梯度下降的方法对sampled softmax进行求解。在serving阶段，通过一个近似最近邻的查找，得到数百个候选推荐结果**\n\n## 特征多样性\n\n使用DNN作为矩阵分解方法的进一步泛化，可以带来的一个非常明显的优势：任何的连续特征和离散特征，都能比较容易地加入到模型中。\n\n**搜索历史：**和观看历史非常近似，每个搜索query都可以分词为unigrams和bigrams，每个分词可以嵌入为向量，将用户的所有token对应的嵌入向量进行平均，形成代表用户搜索历史的特征。\n\n**人口属性：**人口属性提供了非常重要的先验信息，有助于对新用户进行合理的推荐。地域和设备信息都通过嵌入的方式，拼接到整个模型的输入中。比较简单的离散或连续特征如用户性别、用户登录状态、用户年龄等，直接以归一化到0-1之间的实数形式输入到神经网络。\n\n**样本年龄：** 经过持续的观察得知，用户更加喜欢新内容，尽管不会以牺牲相关性为代价。如果我们简单地推荐新内容给用户，可能会使得推荐的内容不够相关。使用机器学习的方式进行推荐时，由于模型的训练，都来在历史样本，会使得历史的内容更容易得到推荐。论文的推荐系统产生的推荐结果，在训练窗口的几个星期内的流行度变化显示了用户在每个时间的平均喜好程度，同时表明video的流行度不是固定不变的，而是符合多项式分布的。为了去除不同时间因素的影响，我们把样本年龄特征加入模型进行训练，在server阶段，该特征置为零，表示当前时间在训练窗口的末尾。图3显示了该方法的效果：\n\n<center>\n![“样本年龄特征效果”](/dnn_for_youtube_recommandation/example_age_efficacy.png) \n</center>\n\n**图3：加入样本年龄特征后模型效果，使用样本年龄作为特征后，模型可以精确表示video的上传时间和独立于时间的属性。没有这个特征，模型预测的是整个训练时间窗口的平均喜好程度**\n\n\n\n## 标签和上下文选择\n     \n通过解决一个问题，然后将这个计算结果转换到特定的上下文环境中使用，是推荐业务经常使用的一种方式，这种方式对于线上的AB测试非常重要，但是离线实验对于在线的效果不是很好评估。\n\n**训练样本来自所有用户观看记录，而不只是推荐结果的观看记录。否则，很难推荐出新的video, 而只是在旧的内容进行exploitation；通过确保每个用户具有相同样本数，可以避免部分活跃用户的行为带来的训练误差，提升模型的在线效果；为了防止模型在原始问题上过拟合，对于分类器信息的使用需要非常谨慎。例如用户刚搜了一个query, 分类器在进行推荐时会选择搜索结果页中对应的视频，但是推荐用户刚搜过视频对于用户的体验是不好的，通过将原始的query分词后，抛弃序列信息，以bag of words的方式提供特征，分类器会不受原始搜索结果的影响**\n\n用户观看视频的顺序会对最终的观看概率有较大影响。因此，在训练的时候，用历史发生的行为+历史行为之后的视频观看结果作为样本进行训练，要好于用所有的行为+随机的视频观看结果进行训练。如图4所示：\n\n<center>\n![“训练数据和lable组合方式”](/dnn_for_youtube_recommandation/trainning_sequence_behavior_and_lable.png) \n</center>\n\n**图4：lable和输入上下文。 选择样本标签及对应的上下文对于样本准备更有挑战，但是对于在线的效果提升非常有帮助,4-b的效果要远远好于4-a.在4-b中，$t\\_{max}$表示训练窗口的最大观察时刻，$t\\_{max}-t\\_N$表示样本年龄特征**\n\n \n## 特征与DNN层数实验\n\n通过增加特征和DNN的层数，可以显著提升模型的效果，如图5所示。1M数量的video和搜索token嵌入分别嵌入到256维的向量，每个用户观看历史为最近的50个video和最近的50个query分词结果，softmax层输出1M个video的多项式分布概率。模型的结构是塔型，最底层的单元数最多，每向上一层单元数都减少一半。0层的网络等价于一个线性分解器，和YouTube早先的推荐系统类似。在进行网络调优时，网络的宽度和层数逐渐增加，直到带来的收益不再增加或者收敛变得困难。\n\n<center>\n![“模型效果与特征、DNN层数的关系”](/dnn_for_youtube_recommandation/model_performance_with_feature_depth.png) \n</center>\n\n **图5：模型效果与特征、DNN层数的关系。 Mean Average Precision (MAP) 随着特征、层数的增加而提升**\n\n \n# 排序模块\n\n在排序阶段，由于只对几百个候选样本进行打分，可以采用更多特征描述video, user和video的关系。排序不仅可以上述rank模型的结果进行精选，也可用于对多个来源的候选结果进行融合。论文采用和触发模块类似的模型结构，并利用LR模型对候选结果进行打分和排序，并返回得分最高的video作为排序模块的结果输出。排序模型最终以平均每次曝光的观看时间作为优化目标，而非点击率，通过点击率作为优化目标容易推荐带有欺骗性的视屏，这些视频虽然容易吸引用户点击，但用户点进去后会很快离开。排序模型的架构如图6所示：\n\n<center>\n![“排序模块架构”](/dnn_for_youtube_recommandation/recommand_ranking.png) \n</center>\n\n **图6：深度排序模型架构，输入包括univalent（如当前待评分的video id）和multivalent（如用户最近浏览过的多个video id）的离散特征的嵌入、连续特征的各个幂运算等，共输入几百个特征**\n\n## 特征表示\n\n排序模块包括离散特征（单值特征和多值特征）和连续特征，其中离散特征包括二值特征（如是否登录状态）和多个值的离散特征（如video id）, 多个离散值的特征包括univalent（如当前待评分的video id）和multivalent（如用户最近浏览过的多个video id），论文也使用了其他特征，如是否描述item(如曝光)，是否描述user/context(如query)\n\n### Feature Engineering\n\n尽管深度神经网络可以减少工程的难度，但是大多数特征还是不能直接输入到神经网络，需要花费一定的时间将用户、video等特征转化为可以直接输入的形式。\n\n最有用的特征是描述用户之前和item或者类似item的交互特征，如用户从对应的频道看的视频个数、用户最近看该主题video的时间，这类特征具有很强的泛化性能，候选集分数，候选集来源，历史的video曝光情况等非常重要。\n\n### Embedding Categorical Features\n\n同触发模块类似，将稀疏的离散特征嵌入到稠密的表示空间，用于神经网络的输入。每个独有的ID序列都会被映射到这样的一个空间，映射后特征的维度同ID值个数的对数成正比。在训练之前，通过一些扫描所有的ID建立查找表，在后续的训练过程中可以直接查找和更新这个表。对于ID值非常多的空间，根据点击的频率进行从高到低进行排序，对TOP N的ID进行嵌入，其他ID嵌入结果直接由0向量表示。multivalent离散特征的嵌入结果是多个univalent嵌入结果的均值。\n\n在同样的ID空间的不同特征，都共享该特征的嵌入结果。如曝光的video ID，上次观看的video ID等。不同的特征虽然共享嵌入的结果，但是分别输入到神经网络进行训练，可以在学习时利用到每个特征表示的具体信息。**嵌入结果的共享对于模型提升泛化性能、训练速度，减少内存消耗都是非常重要的。**\n\n### Normalizing Continuous Features\n\n通常神经网络对于特征的尺度和分布非常敏感，而基于树的组合方法对尺度和分布不敏感。因此对于论文模型，为连续特征选择合适的规范化方法非常重要。为了使得原始特征映射到0-1之间的均匀分布，采用公式2进行特征的转换，该转换类似于在训练之前，为特征值的分位数进行差值。\n\n$\\bar x = \\int\\_{-\\infty}^xd\\,f\\;\\;(式2)$\n\n除了规范化后的$\\bar x$, 同时增加$\\bar x^2$和$\\sqrt{\\bar x}$,同时捕获原始特征的超线性和次线性，赋予模型更加丰富的表达能力，实验证明通过这些特征的加入，可以提升模型的效果。\n\n\n## 期望浏览时间建模\n\n论文的目标是对期望浏览时间进行建模，其中正样本是有点击行为的样本，负样本是没有发生点击行为的样本。每个正样本都有对应的浏览时间，负样本的浏览时间为0. 为了预测期望浏览时间，采用加权逻辑回归的方法，正样本的权重是对应的浏览时间，负样本使用单位权重，采用交叉熵作为目标函数。这样模型学习的优势比odds为$\\frac{\\sum T\\_i}{N-k}$, 关于优势比的定义可参见【3】。其中$N$是训练样本数，$k$是正样本数，$T\\_i$对应第i个视频的浏览时间。由于实际正样本的数量比较少，优势比等价于$E\\[T\\](1+P)$, $P$是点击率，$E\\[T\\]$是期望浏览时间，由于$P$比较小,该目标进一步可以近似为平均观看时间。在预测阶段，使用指数函数$e^x$作为激活函数得到优势比odds, 作为预测的平均浏览时间的近似值。\n\n\n\n## 针对隐藏层的实验\n\n表1是不同隐藏层配置对应的对于next-day的预测结果，评估指标是**用户加权平均损失**，每个配置都是评估同一个页面的正负样本，如果正样本分数低于负样本，那么正样本的浏览时间是**预测错误浏览时长**， **用户加权平均损失**定义为**所有预测错误的浏览时间占比所有评估对的浏览时间之和**\n\n结果显示，增加神经网络的层数和每一层的宽度，都可以提升模型的效果。在CPU预算允许的前提下，论文采用了1024 ReLU =》 512 ReLU =》 256 ReLU的配置。不使用连续特征的幂特征，损失增加0.2%。正负样本权值设置相等，损失增加非常明显，达到4.1%。\n\n<center>\n![“隐藏层配置对预测结果的影响”](/dnn_for_youtube_recommandation/evaluation_of_diffrent_configure.png) \n**表1：不同ReLU单元隐藏层配置对next-day的预测结果影响，评估指标是用户加权平均损失**</center>\n\n\n# 总结\n\n论文描述了YouTube视频推荐的深度网络结构，主要分为触发模块和排序模块\n\n**触发模块：**论文采用的深度协同过滤，能够使用各种信号特征，并通过多层网络建模信号之间的交互，效果好于之前的矩阵分解模型；通过建模时考虑不对称的行为序列，能够更好地使用所有信息对未来进行预测，使得离线训练的模型能更好地用于在线预测；对于分类器的所有信号特征不直接使用，而是对其中一些特征经过仔细分析和加工后使用，能够获得更好的推荐效果；使用样本的年龄特征，能够避开时间因素的干扰，使得预测结果独立于时间特征。\n\n**排序模块：**模型在预测平均观看时间方面，好于之前的线性方法、组合树的方法。通过使用标签之前的用户行为，用户的效果有较大的提升；深度网络需要对离散特征做嵌入处理，对连续特征做规范化处理；多层网络能够有效地建模数百个特征之间的非线性交互；采用加权逻辑回归的方式，对于正负样本分别赋予不同全值，使得我们能够较好地学习优势比，并用优势比来预测浏览时间。在使用用户平均加权损失来评估效果时，该方法的效果要远远好于直接用点击率进行建模。\n\n# 参考文献\n\n【1】P Covington, J Adams, E Sargin. Deep Neural Networks for YouTube Recommendations, Acm Conference on Recommender Systems, 2016 :191-198\n【2】S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007, 2014.\n【3】T. Liu, A. W. Moore, A. Gray, and K. Yang. An\rinvestigation of practical approximate nearest\rneighbor algorithms. pages 825–832. MIT Press, 2004.\n【4】nside_Zhang, http://blog.csdn.net/lanchunhui/article/details/51037264, 2016","source":"_posts/dnn_for_youtube_recommandation.md","raw":"---\ntitle: Deep Neural Networks for YouTube Recommendations论文学习\ndate: 2017-08-20\ntoc: true\ncategories: 推荐系统\ntags: [个性化推荐,深度学习,DNN]\ndescription: 论文来自YouTube团队使用DNN进行推荐方面的尝试，发表在16年9月的RecSys会议\nmathjax: true\n---\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n\n\n\nYouTube采用的推荐系统，是当前工业界最大最复杂的推荐系统之一，服务于上十亿的用户，目的是从不断增加的的海量的视频集合中，为每个用户推荐个性化的内容。\nYouTube的个性化推荐系统主要存在三方面的挑战：\n\n**数据规模:** 由于YouTube庞大的数据规模，很多在小数据集上非常有效的推荐算法无法使用，需要更加特殊的分布式学习算法和在线推荐服务系统。\n\n**新颖度：** 每一秒都有很多的视频上传到YouTube, 推荐系统需要及时考虑这些新上传的视频以及用户在视频上的新操作等，这就需要从 exploration/exploitation的角度去权衡。 \n\n**数据特点和噪声：** 由于用户行为的稀疏性以及一些不确定的额外因素干扰，通过用户的历史行为进行推荐是非常困难的；在YouTube上很少能获得用户的显示喜欢的内容，因此只能根据隐式反馈信号进行建模；视频内容的元数据不够结构化；推荐算法的设计需要对所有这些数据特点都是鲁棒的。\n\n为了保持和google内其他产品的一致，YouTube内部正在做的一件事情是将深度学习作为一种通用的解决方案，用于解决几乎所有的机器学习问题。该论文成果建立在google brain开源的tensorflow之上。tensorflow提供了可以大规模训练各种网络结构的实验框架，本篇论文实验使用了几千亿的样本，训练十亿的参数。\n\n\n# 系统概述\n\n如图1[1]所示，整个系统包括两个深度神经网络, 一个用于生成候选集，另一个用于排序。\n\n <center>![“推荐系统结构”](/dnn_for_youtube_recommandation/recommand_system.png) </center>\n\n图1：**推荐系统结构：包括触发和排序2个模块，触发模块用于从海量的数据中生成较少的候选集，排序模块对触发结果做更精细的选择**\n\n触发模块通过输入用户的历史行为，从全量的集合里面，检索出视一个较小的子集（几百个），该模块通过协同过滤的方式，提供了相对弱一些的的个性化。用户之间的相似性通过观看的视频id, 搜索query的分词结果，以及用户的人口属性信息等来衡量。\n\n排序模块针对触发结果列表，做更加精细的筛选。排序模块采用更加丰富的特征用于描述用户和视频，通过使用特定目标函数训练得到的模型，对视频进行打分。最后得分最高的一些视频推荐给当前的用户。\n\n通过结合触发和排序两个模块，具有两个明显的优势：（1）可以使得从很大的候选集合内，以较小的时间成本推荐个性化的内容给用户（2）可以融合多个算法的结果进行排序（具体使用时，可以将每个算法的结果合并，然后利用排序模块进行打分，取最高得分的视频推荐给用户，较好地实现多推荐算法融合）。\n  \n \n# 触发模块\n\n在触发阶段，会将海量的视频集合进行初步过滤，剩余几百个用户最可能喜欢的视频。\n\n## 用分类的思想对推荐建模\n\n我们把推荐问题建模为一个分类问题，预测在当前时刻$t$, 给定用户$u$和上下文$c$, 预测要观看的video的类别（有多少个候选video, 就有多少个类别）。如下式所示：\n\n$P(w\\_t=i|U,C)=\\frac{e^{v\\_i\\,u}}{\\sum\\_{j\\in V} \\; e^{v\\_j\\,u}}\\;\\;(式1)$ \n\n其中$u$表示的用户和上下文的隐语义向量，$v\\_j$表示第$j$个候选video的隐语义向量。隐语义向量的获取实际上是把一些稀疏的实体信息（如user, video等）映射为N维空间的稠密实向量，而DNN的作用就是从用户的浏览历史和上下文中学习隐语义向量$u$, $u$进一步可用于在$softmax$分类器中预测对应的video类别（即当前用户在上下文环境C时,最可能看的video）. \n\n尽管YouTube上存在显式反馈数据，但是论文中依然使用隐反馈数据，当用户看完一个视频则认为是正样本。这是由于用户的显示反馈行为是非常稀疏的，无法较好地进行模型的训练和预测，而用户存在大量的隐反馈行为，通过采用隐反馈数据，可以有效地以完成模型的训练和预测。\n\n**高效的多类别训练：**为了有效地完成数十亿类别的模型训练，论文采用了一种负采样的方法，首选根据负样本的分布采样负样本，然后通过重要性加权进行纠正，具体使用的方法可以参见文献【2】。对于每个样本，其真实的标签和负采样的类别标识都通过最小化交叉熵优化模型。实际上，每次都采样几千个样本，相对于传统的softmax, 其训练速度提升100倍以上。另外一种可行的高效训练方法是$hierarchical softmax$，但由于遍历每个节点等于在将无关的类别进行分类，这使得分类更加复杂，有损模型的效果。\n\n在线服务阶段，需要在几十毫秒内计算当前用户最可能喜欢的$N$个video, 这需要一个近似的得分方案，使得时间延迟对于类别数量的增长是低于线性的。论文的采用的是基于hash的方法【3】，从点积空间寻找最近的$N$个类别作为推荐的结果。\n\n## 模型架构\n\n对于每个video, 学习其在固定词典上的高维嵌入向量，并把这些向量作为神经网络的输入。每个用户的观看历史可用一个变长的video id序列来表示，每个video id可以通过嵌入的方式得到一个稠密的向量。这些向量通过聚合操作，得到单个向量（实验表示通过对所有向量求平均是最好的策略），作为神经网络的输入。这些video的嵌入向量，和其他的模型参数，都通过误差反向传播，并利用梯度下降方法进行学习。图2是触发模块的模型架构，其输入同时包括观看的video特征和和其他各种非video特征。\n\n<center>\n![“候选推荐”](/dnn_for_youtube_recommandation/recommand_matching.png) \n</center>\n\n图2：**触发模块结构图：嵌入的稀疏特征和和非稠密特征同时作为模型的输入，同一个用户的多个嵌入特征通过平均的方式，得到固定的尺寸，并输入到隐藏层，所有的隐藏层都是全连接。在模型训练阶段，通过最小化交叉熵，并采用梯度下降的方法对sampled softmax进行求解。在serving阶段，通过一个近似最近邻的查找，得到数百个候选推荐结果**\n\n## 特征多样性\n\n使用DNN作为矩阵分解方法的进一步泛化，可以带来的一个非常明显的优势：任何的连续特征和离散特征，都能比较容易地加入到模型中。\n\n**搜索历史：**和观看历史非常近似，每个搜索query都可以分词为unigrams和bigrams，每个分词可以嵌入为向量，将用户的所有token对应的嵌入向量进行平均，形成代表用户搜索历史的特征。\n\n**人口属性：**人口属性提供了非常重要的先验信息，有助于对新用户进行合理的推荐。地域和设备信息都通过嵌入的方式，拼接到整个模型的输入中。比较简单的离散或连续特征如用户性别、用户登录状态、用户年龄等，直接以归一化到0-1之间的实数形式输入到神经网络。\n\n**样本年龄：** 经过持续的观察得知，用户更加喜欢新内容，尽管不会以牺牲相关性为代价。如果我们简单地推荐新内容给用户，可能会使得推荐的内容不够相关。使用机器学习的方式进行推荐时，由于模型的训练，都来在历史样本，会使得历史的内容更容易得到推荐。论文的推荐系统产生的推荐结果，在训练窗口的几个星期内的流行度变化显示了用户在每个时间的平均喜好程度，同时表明video的流行度不是固定不变的，而是符合多项式分布的。为了去除不同时间因素的影响，我们把样本年龄特征加入模型进行训练，在server阶段，该特征置为零，表示当前时间在训练窗口的末尾。图3显示了该方法的效果：\n\n<center>\n![“样本年龄特征效果”](/dnn_for_youtube_recommandation/example_age_efficacy.png) \n</center>\n\n**图3：加入样本年龄特征后模型效果，使用样本年龄作为特征后，模型可以精确表示video的上传时间和独立于时间的属性。没有这个特征，模型预测的是整个训练时间窗口的平均喜好程度**\n\n\n\n## 标签和上下文选择\n     \n通过解决一个问题，然后将这个计算结果转换到特定的上下文环境中使用，是推荐业务经常使用的一种方式，这种方式对于线上的AB测试非常重要，但是离线实验对于在线的效果不是很好评估。\n\n**训练样本来自所有用户观看记录，而不只是推荐结果的观看记录。否则，很难推荐出新的video, 而只是在旧的内容进行exploitation；通过确保每个用户具有相同样本数，可以避免部分活跃用户的行为带来的训练误差，提升模型的在线效果；为了防止模型在原始问题上过拟合，对于分类器信息的使用需要非常谨慎。例如用户刚搜了一个query, 分类器在进行推荐时会选择搜索结果页中对应的视频，但是推荐用户刚搜过视频对于用户的体验是不好的，通过将原始的query分词后，抛弃序列信息，以bag of words的方式提供特征，分类器会不受原始搜索结果的影响**\n\n用户观看视频的顺序会对最终的观看概率有较大影响。因此，在训练的时候，用历史发生的行为+历史行为之后的视频观看结果作为样本进行训练，要好于用所有的行为+随机的视频观看结果进行训练。如图4所示：\n\n<center>\n![“训练数据和lable组合方式”](/dnn_for_youtube_recommandation/trainning_sequence_behavior_and_lable.png) \n</center>\n\n**图4：lable和输入上下文。 选择样本标签及对应的上下文对于样本准备更有挑战，但是对于在线的效果提升非常有帮助,4-b的效果要远远好于4-a.在4-b中，$t\\_{max}$表示训练窗口的最大观察时刻，$t\\_{max}-t\\_N$表示样本年龄特征**\n\n \n## 特征与DNN层数实验\n\n通过增加特征和DNN的层数，可以显著提升模型的效果，如图5所示。1M数量的video和搜索token嵌入分别嵌入到256维的向量，每个用户观看历史为最近的50个video和最近的50个query分词结果，softmax层输出1M个video的多项式分布概率。模型的结构是塔型，最底层的单元数最多，每向上一层单元数都减少一半。0层的网络等价于一个线性分解器，和YouTube早先的推荐系统类似。在进行网络调优时，网络的宽度和层数逐渐增加，直到带来的收益不再增加或者收敛变得困难。\n\n<center>\n![“模型效果与特征、DNN层数的关系”](/dnn_for_youtube_recommandation/model_performance_with_feature_depth.png) \n</center>\n\n **图5：模型效果与特征、DNN层数的关系。 Mean Average Precision (MAP) 随着特征、层数的增加而提升**\n\n \n# 排序模块\n\n在排序阶段，由于只对几百个候选样本进行打分，可以采用更多特征描述video, user和video的关系。排序不仅可以上述rank模型的结果进行精选，也可用于对多个来源的候选结果进行融合。论文采用和触发模块类似的模型结构，并利用LR模型对候选结果进行打分和排序，并返回得分最高的video作为排序模块的结果输出。排序模型最终以平均每次曝光的观看时间作为优化目标，而非点击率，通过点击率作为优化目标容易推荐带有欺骗性的视屏，这些视频虽然容易吸引用户点击，但用户点进去后会很快离开。排序模型的架构如图6所示：\n\n<center>\n![“排序模块架构”](/dnn_for_youtube_recommandation/recommand_ranking.png) \n</center>\n\n **图6：深度排序模型架构，输入包括univalent（如当前待评分的video id）和multivalent（如用户最近浏览过的多个video id）的离散特征的嵌入、连续特征的各个幂运算等，共输入几百个特征**\n\n## 特征表示\n\n排序模块包括离散特征（单值特征和多值特征）和连续特征，其中离散特征包括二值特征（如是否登录状态）和多个值的离散特征（如video id）, 多个离散值的特征包括univalent（如当前待评分的video id）和multivalent（如用户最近浏览过的多个video id），论文也使用了其他特征，如是否描述item(如曝光)，是否描述user/context(如query)\n\n### Feature Engineering\n\n尽管深度神经网络可以减少工程的难度，但是大多数特征还是不能直接输入到神经网络，需要花费一定的时间将用户、video等特征转化为可以直接输入的形式。\n\n最有用的特征是描述用户之前和item或者类似item的交互特征，如用户从对应的频道看的视频个数、用户最近看该主题video的时间，这类特征具有很强的泛化性能，候选集分数，候选集来源，历史的video曝光情况等非常重要。\n\n### Embedding Categorical Features\n\n同触发模块类似，将稀疏的离散特征嵌入到稠密的表示空间，用于神经网络的输入。每个独有的ID序列都会被映射到这样的一个空间，映射后特征的维度同ID值个数的对数成正比。在训练之前，通过一些扫描所有的ID建立查找表，在后续的训练过程中可以直接查找和更新这个表。对于ID值非常多的空间，根据点击的频率进行从高到低进行排序，对TOP N的ID进行嵌入，其他ID嵌入结果直接由0向量表示。multivalent离散特征的嵌入结果是多个univalent嵌入结果的均值。\n\n在同样的ID空间的不同特征，都共享该特征的嵌入结果。如曝光的video ID，上次观看的video ID等。不同的特征虽然共享嵌入的结果，但是分别输入到神经网络进行训练，可以在学习时利用到每个特征表示的具体信息。**嵌入结果的共享对于模型提升泛化性能、训练速度，减少内存消耗都是非常重要的。**\n\n### Normalizing Continuous Features\n\n通常神经网络对于特征的尺度和分布非常敏感，而基于树的组合方法对尺度和分布不敏感。因此对于论文模型，为连续特征选择合适的规范化方法非常重要。为了使得原始特征映射到0-1之间的均匀分布，采用公式2进行特征的转换，该转换类似于在训练之前，为特征值的分位数进行差值。\n\n$\\bar x = \\int\\_{-\\infty}^xd\\,f\\;\\;(式2)$\n\n除了规范化后的$\\bar x$, 同时增加$\\bar x^2$和$\\sqrt{\\bar x}$,同时捕获原始特征的超线性和次线性，赋予模型更加丰富的表达能力，实验证明通过这些特征的加入，可以提升模型的效果。\n\n\n## 期望浏览时间建模\n\n论文的目标是对期望浏览时间进行建模，其中正样本是有点击行为的样本，负样本是没有发生点击行为的样本。每个正样本都有对应的浏览时间，负样本的浏览时间为0. 为了预测期望浏览时间，采用加权逻辑回归的方法，正样本的权重是对应的浏览时间，负样本使用单位权重，采用交叉熵作为目标函数。这样模型学习的优势比odds为$\\frac{\\sum T\\_i}{N-k}$, 关于优势比的定义可参见【3】。其中$N$是训练样本数，$k$是正样本数，$T\\_i$对应第i个视频的浏览时间。由于实际正样本的数量比较少，优势比等价于$E\\[T\\](1+P)$, $P$是点击率，$E\\[T\\]$是期望浏览时间，由于$P$比较小,该目标进一步可以近似为平均观看时间。在预测阶段，使用指数函数$e^x$作为激活函数得到优势比odds, 作为预测的平均浏览时间的近似值。\n\n\n\n## 针对隐藏层的实验\n\n表1是不同隐藏层配置对应的对于next-day的预测结果，评估指标是**用户加权平均损失**，每个配置都是评估同一个页面的正负样本，如果正样本分数低于负样本，那么正样本的浏览时间是**预测错误浏览时长**， **用户加权平均损失**定义为**所有预测错误的浏览时间占比所有评估对的浏览时间之和**\n\n结果显示，增加神经网络的层数和每一层的宽度，都可以提升模型的效果。在CPU预算允许的前提下，论文采用了1024 ReLU =》 512 ReLU =》 256 ReLU的配置。不使用连续特征的幂特征，损失增加0.2%。正负样本权值设置相等，损失增加非常明显，达到4.1%。\n\n<center>\n![“隐藏层配置对预测结果的影响”](/dnn_for_youtube_recommandation/evaluation_of_diffrent_configure.png) \n**表1：不同ReLU单元隐藏层配置对next-day的预测结果影响，评估指标是用户加权平均损失**</center>\n\n\n# 总结\n\n论文描述了YouTube视频推荐的深度网络结构，主要分为触发模块和排序模块\n\n**触发模块：**论文采用的深度协同过滤，能够使用各种信号特征，并通过多层网络建模信号之间的交互，效果好于之前的矩阵分解模型；通过建模时考虑不对称的行为序列，能够更好地使用所有信息对未来进行预测，使得离线训练的模型能更好地用于在线预测；对于分类器的所有信号特征不直接使用，而是对其中一些特征经过仔细分析和加工后使用，能够获得更好的推荐效果；使用样本的年龄特征，能够避开时间因素的干扰，使得预测结果独立于时间特征。\n\n**排序模块：**模型在预测平均观看时间方面，好于之前的线性方法、组合树的方法。通过使用标签之前的用户行为，用户的效果有较大的提升；深度网络需要对离散特征做嵌入处理，对连续特征做规范化处理；多层网络能够有效地建模数百个特征之间的非线性交互；采用加权逻辑回归的方式，对于正负样本分别赋予不同全值，使得我们能够较好地学习优势比，并用优势比来预测浏览时间。在使用用户平均加权损失来评估效果时，该方法的效果要远远好于直接用点击率进行建模。\n\n# 参考文献\n\n【1】P Covington, J Adams, E Sargin. Deep Neural Networks for YouTube Recommendations, Acm Conference on Recommender Systems, 2016 :191-198\n【2】S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007, 2014.\n【3】T. Liu, A. W. Moore, A. Gray, and K. Yang. An\rinvestigation of practical approximate nearest\rneighbor algorithms. pages 825–832. MIT Press, 2004.\n【4】nside_Zhang, http://blog.csdn.net/lanchunhui/article/details/51037264, 2016","slug":"dnn_for_youtube_recommandation","published":1,"updated":"2018-02-11T08:57:52.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjdikgud80005ga01idhc83co","content":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n\n\n\n<p>YouTube采用的推荐系统，是当前工业界最大最复杂的推荐系统之一，服务于上十亿的用户，目的是从不断增加的的海量的视频集合中，为每个用户推荐个性化的内容。<br>YouTube的个性化推荐系统主要存在三方面的挑战：</p>\n<p><strong>数据规模:</strong> 由于YouTube庞大的数据规模，很多在小数据集上非常有效的推荐算法无法使用，需要更加特殊的分布式学习算法和在线推荐服务系统。</p>\n<p><strong>新颖度：</strong> 每一秒都有很多的视频上传到YouTube, 推荐系统需要及时考虑这些新上传的视频以及用户在视频上的新操作等，这就需要从 exploration/exploitation的角度去权衡。 </p>\n<p><strong>数据特点和噪声：</strong> 由于用户行为的稀疏性以及一些不确定的额外因素干扰，通过用户的历史行为进行推荐是非常困难的；在YouTube上很少能获得用户的显示喜欢的内容，因此只能根据隐式反馈信号进行建模；视频内容的元数据不够结构化；推荐算法的设计需要对所有这些数据特点都是鲁棒的。</p>\n<p>为了保持和google内其他产品的一致，YouTube内部正在做的一件事情是将深度学习作为一种通用的解决方案，用于解决几乎所有的机器学习问题。该论文成果建立在google brain开源的tensorflow之上。tensorflow提供了可以大规模训练各种网络结构的实验框架，本篇论文实验使用了几千亿的样本，训练十亿的参数。</p>\n<h1 id=\"系统概述\"><a href=\"#系统概述\" class=\"headerlink\" title=\"系统概述\"></a>系统概述</h1><p>如图1[1]所示，整个系统包括两个深度神经网络, 一个用于生成候选集，另一个用于排序。</p>\n <center><img src=\"/dnn_for_youtube_recommandation/recommand_system.png\" alt=\"“推荐系统结构”\"> </center>\n\n<p>图1：<strong>推荐系统结构：包括触发和排序2个模块，触发模块用于从海量的数据中生成较少的候选集，排序模块对触发结果做更精细的选择</strong></p>\n<p>触发模块通过输入用户的历史行为，从全量的集合里面，检索出视一个较小的子集（几百个），该模块通过协同过滤的方式，提供了相对弱一些的的个性化。用户之间的相似性通过观看的视频id, 搜索query的分词结果，以及用户的人口属性信息等来衡量。</p>\n<p>排序模块针对触发结果列表，做更加精细的筛选。排序模块采用更加丰富的特征用于描述用户和视频，通过使用特定目标函数训练得到的模型，对视频进行打分。最后得分最高的一些视频推荐给当前的用户。</p>\n<p>通过结合触发和排序两个模块，具有两个明显的优势：（1）可以使得从很大的候选集合内，以较小的时间成本推荐个性化的内容给用户（2）可以融合多个算法的结果进行排序（具体使用时，可以将每个算法的结果合并，然后利用排序模块进行打分，取最高得分的视频推荐给用户，较好地实现多推荐算法融合）。</p>\n<h1 id=\"触发模块\"><a href=\"#触发模块\" class=\"headerlink\" title=\"触发模块\"></a>触发模块</h1><p>在触发阶段，会将海量的视频集合进行初步过滤，剩余几百个用户最可能喜欢的视频。</p>\n<h2 id=\"用分类的思想对推荐建模\"><a href=\"#用分类的思想对推荐建模\" class=\"headerlink\" title=\"用分类的思想对推荐建模\"></a>用分类的思想对推荐建模</h2><p>我们把推荐问题建模为一个分类问题，预测在当前时刻$t$, 给定用户$u$和上下文$c$, 预测要观看的video的类别（有多少个候选video, 就有多少个类别）。如下式所示：</p>\n<p>$P(w_t=i|U,C)=\\frac{e^{v_i\\,u}}{\\sum_{j\\in V} \\; e^{v_j\\,u}}\\;\\;(式1)$ </p>\n<p>其中$u$表示的用户和上下文的隐语义向量，$v_j$表示第$j$个候选video的隐语义向量。隐语义向量的获取实际上是把一些稀疏的实体信息（如user, video等）映射为N维空间的稠密实向量，而DNN的作用就是从用户的浏览历史和上下文中学习隐语义向量$u$, $u$进一步可用于在$softmax$分类器中预测对应的video类别（即当前用户在上下文环境C时,最可能看的video）. </p>\n<p>尽管YouTube上存在显式反馈数据，但是论文中依然使用隐反馈数据，当用户看完一个视频则认为是正样本。这是由于用户的显示反馈行为是非常稀疏的，无法较好地进行模型的训练和预测，而用户存在大量的隐反馈行为，通过采用隐反馈数据，可以有效地以完成模型的训练和预测。</p>\n<p><strong>高效的多类别训练：</strong>为了有效地完成数十亿类别的模型训练，论文采用了一种负采样的方法，首选根据负样本的分布采样负样本，然后通过重要性加权进行纠正，具体使用的方法可以参见文献【2】。对于每个样本，其真实的标签和负采样的类别标识都通过最小化交叉熵优化模型。实际上，每次都采样几千个样本，相对于传统的softmax, 其训练速度提升100倍以上。另外一种可行的高效训练方法是$hierarchical softmax$，但由于遍历每个节点等于在将无关的类别进行分类，这使得分类更加复杂，有损模型的效果。</p>\n<p>在线服务阶段，需要在几十毫秒内计算当前用户最可能喜欢的$N$个video, 这需要一个近似的得分方案，使得时间延迟对于类别数量的增长是低于线性的。论文的采用的是基于hash的方法【3】，从点积空间寻找最近的$N$个类别作为推荐的结果。</p>\n<h2 id=\"模型架构\"><a href=\"#模型架构\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h2><p>对于每个video, 学习其在固定词典上的高维嵌入向量，并把这些向量作为神经网络的输入。每个用户的观看历史可用一个变长的video id序列来表示，每个video id可以通过嵌入的方式得到一个稠密的向量。这些向量通过聚合操作，得到单个向量（实验表示通过对所有向量求平均是最好的策略），作为神经网络的输入。这些video的嵌入向量，和其他的模型参数，都通过误差反向传播，并利用梯度下降方法进行学习。图2是触发模块的模型架构，其输入同时包括观看的video特征和和其他各种非video特征。</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/recommand_matching.png\" alt=\"“候选推荐”\"><br></center>\n\n<p>图2：<strong>触发模块结构图：嵌入的稀疏特征和和非稠密特征同时作为模型的输入，同一个用户的多个嵌入特征通过平均的方式，得到固定的尺寸，并输入到隐藏层，所有的隐藏层都是全连接。在模型训练阶段，通过最小化交叉熵，并采用梯度下降的方法对sampled softmax进行求解。在serving阶段，通过一个近似最近邻的查找，得到数百个候选推荐结果</strong></p>\n<h2 id=\"特征多样性\"><a href=\"#特征多样性\" class=\"headerlink\" title=\"特征多样性\"></a>特征多样性</h2><p>使用DNN作为矩阵分解方法的进一步泛化，可以带来的一个非常明显的优势：任何的连续特征和离散特征，都能比较容易地加入到模型中。</p>\n<p><strong>搜索历史：</strong>和观看历史非常近似，每个搜索query都可以分词为unigrams和bigrams，每个分词可以嵌入为向量，将用户的所有token对应的嵌入向量进行平均，形成代表用户搜索历史的特征。</p>\n<p><strong>人口属性：</strong>人口属性提供了非常重要的先验信息，有助于对新用户进行合理的推荐。地域和设备信息都通过嵌入的方式，拼接到整个模型的输入中。比较简单的离散或连续特征如用户性别、用户登录状态、用户年龄等，直接以归一化到0-1之间的实数形式输入到神经网络。</p>\n<p><strong>样本年龄：</strong> 经过持续的观察得知，用户更加喜欢新内容，尽管不会以牺牲相关性为代价。如果我们简单地推荐新内容给用户，可能会使得推荐的内容不够相关。使用机器学习的方式进行推荐时，由于模型的训练，都来在历史样本，会使得历史的内容更容易得到推荐。论文的推荐系统产生的推荐结果，在训练窗口的几个星期内的流行度变化显示了用户在每个时间的平均喜好程度，同时表明video的流行度不是固定不变的，而是符合多项式分布的。为了去除不同时间因素的影响，我们把样本年龄特征加入模型进行训练，在server阶段，该特征置为零，表示当前时间在训练窗口的末尾。图3显示了该方法的效果：</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/example_age_efficacy.png\" alt=\"“样本年龄特征效果”\"><br></center>\n\n<p><strong>图3：加入样本年龄特征后模型效果，使用样本年龄作为特征后，模型可以精确表示video的上传时间和独立于时间的属性。没有这个特征，模型预测的是整个训练时间窗口的平均喜好程度</strong></p>\n<h2 id=\"标签和上下文选择\"><a href=\"#标签和上下文选择\" class=\"headerlink\" title=\"标签和上下文选择\"></a>标签和上下文选择</h2><p>通过解决一个问题，然后将这个计算结果转换到特定的上下文环境中使用，是推荐业务经常使用的一种方式，这种方式对于线上的AB测试非常重要，但是离线实验对于在线的效果不是很好评估。</p>\n<p><strong>训练样本来自所有用户观看记录，而不只是推荐结果的观看记录。否则，很难推荐出新的video, 而只是在旧的内容进行exploitation；通过确保每个用户具有相同样本数，可以避免部分活跃用户的行为带来的训练误差，提升模型的在线效果；为了防止模型在原始问题上过拟合，对于分类器信息的使用需要非常谨慎。例如用户刚搜了一个query, 分类器在进行推荐时会选择搜索结果页中对应的视频，但是推荐用户刚搜过视频对于用户的体验是不好的，通过将原始的query分词后，抛弃序列信息，以bag of words的方式提供特征，分类器会不受原始搜索结果的影响</strong></p>\n<p>用户观看视频的顺序会对最终的观看概率有较大影响。因此，在训练的时候，用历史发生的行为+历史行为之后的视频观看结果作为样本进行训练，要好于用所有的行为+随机的视频观看结果进行训练。如图4所示：</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/trainning_sequence_behavior_and_lable.png\" alt=\"“训练数据和lable组合方式”\"><br></center>\n\n<p><strong>图4：lable和输入上下文。 选择样本标签及对应的上下文对于样本准备更有挑战，但是对于在线的效果提升非常有帮助,4-b的效果要远远好于4-a.在4-b中，$t_{max}$表示训练窗口的最大观察时刻，$t_{max}-t_N$表示样本年龄特征</strong></p>\n<h2 id=\"特征与DNN层数实验\"><a href=\"#特征与DNN层数实验\" class=\"headerlink\" title=\"特征与DNN层数实验\"></a>特征与DNN层数实验</h2><p>通过增加特征和DNN的层数，可以显著提升模型的效果，如图5所示。1M数量的video和搜索token嵌入分别嵌入到256维的向量，每个用户观看历史为最近的50个video和最近的50个query分词结果，softmax层输出1M个video的多项式分布概率。模型的结构是塔型，最底层的单元数最多，每向上一层单元数都减少一半。0层的网络等价于一个线性分解器，和YouTube早先的推荐系统类似。在进行网络调优时，网络的宽度和层数逐渐增加，直到带来的收益不再增加或者收敛变得困难。</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/model_performance_with_feature_depth.png\" alt=\"“模型效果与特征、DNN层数的关系”\"><br></center>\n\n<p> <strong>图5：模型效果与特征、DNN层数的关系。 Mean Average Precision (MAP) 随着特征、层数的增加而提升</strong></p>\n<h1 id=\"排序模块\"><a href=\"#排序模块\" class=\"headerlink\" title=\"排序模块\"></a>排序模块</h1><p>在排序阶段，由于只对几百个候选样本进行打分，可以采用更多特征描述video, user和video的关系。排序不仅可以上述rank模型的结果进行精选，也可用于对多个来源的候选结果进行融合。论文采用和触发模块类似的模型结构，并利用LR模型对候选结果进行打分和排序，并返回得分最高的video作为排序模块的结果输出。排序模型最终以平均每次曝光的观看时间作为优化目标，而非点击率，通过点击率作为优化目标容易推荐带有欺骗性的视屏，这些视频虽然容易吸引用户点击，但用户点进去后会很快离开。排序模型的架构如图6所示：</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/recommand_ranking.png\" alt=\"“排序模块架构”\"><br></center>\n\n<p> <strong>图6：深度排序模型架构，输入包括univalent（如当前待评分的video id）和multivalent（如用户最近浏览过的多个video id）的离散特征的嵌入、连续特征的各个幂运算等，共输入几百个特征</strong></p>\n<h2 id=\"特征表示\"><a href=\"#特征表示\" class=\"headerlink\" title=\"特征表示\"></a>特征表示</h2><p>排序模块包括离散特征（单值特征和多值特征）和连续特征，其中离散特征包括二值特征（如是否登录状态）和多个值的离散特征（如video id）, 多个离散值的特征包括univalent（如当前待评分的video id）和multivalent（如用户最近浏览过的多个video id），论文也使用了其他特征，如是否描述item(如曝光)，是否描述user/context(如query)</p>\n<h3 id=\"Feature-Engineering\"><a href=\"#Feature-Engineering\" class=\"headerlink\" title=\"Feature Engineering\"></a>Feature Engineering</h3><p>尽管深度神经网络可以减少工程的难度，但是大多数特征还是不能直接输入到神经网络，需要花费一定的时间将用户、video等特征转化为可以直接输入的形式。</p>\n<p>最有用的特征是描述用户之前和item或者类似item的交互特征，如用户从对应的频道看的视频个数、用户最近看该主题video的时间，这类特征具有很强的泛化性能，候选集分数，候选集来源，历史的video曝光情况等非常重要。</p>\n<h3 id=\"Embedding-Categorical-Features\"><a href=\"#Embedding-Categorical-Features\" class=\"headerlink\" title=\"Embedding Categorical Features\"></a>Embedding Categorical Features</h3><p>同触发模块类似，将稀疏的离散特征嵌入到稠密的表示空间，用于神经网络的输入。每个独有的ID序列都会被映射到这样的一个空间，映射后特征的维度同ID值个数的对数成正比。在训练之前，通过一些扫描所有的ID建立查找表，在后续的训练过程中可以直接查找和更新这个表。对于ID值非常多的空间，根据点击的频率进行从高到低进行排序，对TOP N的ID进行嵌入，其他ID嵌入结果直接由0向量表示。multivalent离散特征的嵌入结果是多个univalent嵌入结果的均值。</p>\n<p>在同样的ID空间的不同特征，都共享该特征的嵌入结果。如曝光的video ID，上次观看的video ID等。不同的特征虽然共享嵌入的结果，但是分别输入到神经网络进行训练，可以在学习时利用到每个特征表示的具体信息。<strong>嵌入结果的共享对于模型提升泛化性能、训练速度，减少内存消耗都是非常重要的。</strong></p>\n<h3 id=\"Normalizing-Continuous-Features\"><a href=\"#Normalizing-Continuous-Features\" class=\"headerlink\" title=\"Normalizing Continuous Features\"></a>Normalizing Continuous Features</h3><p>通常神经网络对于特征的尺度和分布非常敏感，而基于树的组合方法对尺度和分布不敏感。因此对于论文模型，为连续特征选择合适的规范化方法非常重要。为了使得原始特征映射到0-1之间的均匀分布，采用公式2进行特征的转换，该转换类似于在训练之前，为特征值的分位数进行差值。</p>\n<p>$\\bar x = \\int_{-\\infty}^xd\\,f\\;\\;(式2)$</p>\n<p>除了规范化后的$\\bar x$, 同时增加$\\bar x^2$和$\\sqrt{\\bar x}$,同时捕获原始特征的超线性和次线性，赋予模型更加丰富的表达能力，实验证明通过这些特征的加入，可以提升模型的效果。</p>\n<h2 id=\"期望浏览时间建模\"><a href=\"#期望浏览时间建模\" class=\"headerlink\" title=\"期望浏览时间建模\"></a>期望浏览时间建模</h2><p>论文的目标是对期望浏览时间进行建模，其中正样本是有点击行为的样本，负样本是没有发生点击行为的样本。每个正样本都有对应的浏览时间，负样本的浏览时间为0. 为了预测期望浏览时间，采用加权逻辑回归的方法，正样本的权重是对应的浏览时间，负样本使用单位权重，采用交叉熵作为目标函数。这样模型学习的优势比odds为$\\frac{\\sum T_i}{N-k}$, 关于优势比的定义可参见【3】。其中$N$是训练样本数，$k$是正样本数，$T_i$对应第i个视频的浏览时间。由于实际正样本的数量比较少，优势比等价于$E[T](1+P)$, $P$是点击率，$E[T]$是期望浏览时间，由于$P$比较小,该目标进一步可以近似为平均观看时间。在预测阶段，使用指数函数$e^x$作为激活函数得到优势比odds, 作为预测的平均浏览时间的近似值。</p>\n<h2 id=\"针对隐藏层的实验\"><a href=\"#针对隐藏层的实验\" class=\"headerlink\" title=\"针对隐藏层的实验\"></a>针对隐藏层的实验</h2><p>表1是不同隐藏层配置对应的对于next-day的预测结果，评估指标是<strong>用户加权平均损失</strong>，每个配置都是评估同一个页面的正负样本，如果正样本分数低于负样本，那么正样本的浏览时间是<strong>预测错误浏览时长</strong>， <strong>用户加权平均损失</strong>定义为<strong>所有预测错误的浏览时间占比所有评估对的浏览时间之和</strong></p>\n<p>结果显示，增加神经网络的层数和每一层的宽度，都可以提升模型的效果。在CPU预算允许的前提下，论文采用了1024 ReLU =》 512 ReLU =》 256 ReLU的配置。不使用连续特征的幂特征，损失增加0.2%。正负样本权值设置相等，损失增加非常明显，达到4.1%。</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/evaluation_of_diffrent_configure.png\" alt=\"“隐藏层配置对预测结果的影响”\"><br><strong>表1：不同ReLU单元隐藏层配置对next-day的预测结果影响，评估指标是用户加权平均损失</strong></center>\n\n\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>论文描述了YouTube视频推荐的深度网络结构，主要分为触发模块和排序模块</p>\n<p><strong>触发模块：</strong>论文采用的深度协同过滤，能够使用各种信号特征，并通过多层网络建模信号之间的交互，效果好于之前的矩阵分解模型；通过建模时考虑不对称的行为序列，能够更好地使用所有信息对未来进行预测，使得离线训练的模型能更好地用于在线预测；对于分类器的所有信号特征不直接使用，而是对其中一些特征经过仔细分析和加工后使用，能够获得更好的推荐效果；使用样本的年龄特征，能够避开时间因素的干扰，使得预测结果独立于时间特征。</p>\n<p><strong>排序模块：</strong>模型在预测平均观看时间方面，好于之前的线性方法、组合树的方法。通过使用标签之前的用户行为，用户的效果有较大的提升；深度网络需要对离散特征做嵌入处理，对连续特征做规范化处理；多层网络能够有效地建模数百个特征之间的非线性交互；采用加权逻辑回归的方式，对于正负样本分别赋予不同全值，使得我们能够较好地学习优势比，并用优势比来预测浏览时间。在使用用户平均加权损失来评估效果时，该方法的效果要远远好于直接用点击率进行建模。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>【1】P Covington, J Adams, E Sargin. Deep Neural Networks for YouTube Recommendations, Acm Conference on Recommender Systems, 2016 :191-198<br>【2】S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007, 2014.<br>【3】T. Liu, A. W. Moore, A. Gray, and K. Yang. An<br>investigation of practical approximate nearest<br>neighbor algorithms. pages 825–832. MIT Press, 2004.<br>【4】nside_Zhang, <a href=\"http://blog.csdn.net/lanchunhui/article/details/51037264\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lanchunhui/article/details/51037264</a>, 2016</p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n\n\n\n<p>YouTube采用的推荐系统，是当前工业界最大最复杂的推荐系统之一，服务于上十亿的用户，目的是从不断增加的的海量的视频集合中，为每个用户推荐个性化的内容。<br>YouTube的个性化推荐系统主要存在三方面的挑战：</p>\n<p><strong>数据规模:</strong> 由于YouTube庞大的数据规模，很多在小数据集上非常有效的推荐算法无法使用，需要更加特殊的分布式学习算法和在线推荐服务系统。</p>\n<p><strong>新颖度：</strong> 每一秒都有很多的视频上传到YouTube, 推荐系统需要及时考虑这些新上传的视频以及用户在视频上的新操作等，这就需要从 exploration/exploitation的角度去权衡。 </p>\n<p><strong>数据特点和噪声：</strong> 由于用户行为的稀疏性以及一些不确定的额外因素干扰，通过用户的历史行为进行推荐是非常困难的；在YouTube上很少能获得用户的显示喜欢的内容，因此只能根据隐式反馈信号进行建模；视频内容的元数据不够结构化；推荐算法的设计需要对所有这些数据特点都是鲁棒的。</p>\n<p>为了保持和google内其他产品的一致，YouTube内部正在做的一件事情是将深度学习作为一种通用的解决方案，用于解决几乎所有的机器学习问题。该论文成果建立在google brain开源的tensorflow之上。tensorflow提供了可以大规模训练各种网络结构的实验框架，本篇论文实验使用了几千亿的样本，训练十亿的参数。</p>\n<h1 id=\"系统概述\"><a href=\"#系统概述\" class=\"headerlink\" title=\"系统概述\"></a>系统概述</h1><p>如图1[1]所示，整个系统包括两个深度神经网络, 一个用于生成候选集，另一个用于排序。</p>\n <center><img src=\"/dnn_for_youtube_recommandation/recommand_system.png\" alt=\"“推荐系统结构”\"> </center>\n\n<p>图1：<strong>推荐系统结构：包括触发和排序2个模块，触发模块用于从海量的数据中生成较少的候选集，排序模块对触发结果做更精细的选择</strong></p>\n<p>触发模块通过输入用户的历史行为，从全量的集合里面，检索出视一个较小的子集（几百个），该模块通过协同过滤的方式，提供了相对弱一些的的个性化。用户之间的相似性通过观看的视频id, 搜索query的分词结果，以及用户的人口属性信息等来衡量。</p>\n<p>排序模块针对触发结果列表，做更加精细的筛选。排序模块采用更加丰富的特征用于描述用户和视频，通过使用特定目标函数训练得到的模型，对视频进行打分。最后得分最高的一些视频推荐给当前的用户。</p>\n<p>通过结合触发和排序两个模块，具有两个明显的优势：（1）可以使得从很大的候选集合内，以较小的时间成本推荐个性化的内容给用户（2）可以融合多个算法的结果进行排序（具体使用时，可以将每个算法的结果合并，然后利用排序模块进行打分，取最高得分的视频推荐给用户，较好地实现多推荐算法融合）。</p>\n<h1 id=\"触发模块\"><a href=\"#触发模块\" class=\"headerlink\" title=\"触发模块\"></a>触发模块</h1><p>在触发阶段，会将海量的视频集合进行初步过滤，剩余几百个用户最可能喜欢的视频。</p>\n<h2 id=\"用分类的思想对推荐建模\"><a href=\"#用分类的思想对推荐建模\" class=\"headerlink\" title=\"用分类的思想对推荐建模\"></a>用分类的思想对推荐建模</h2><p>我们把推荐问题建模为一个分类问题，预测在当前时刻$t$, 给定用户$u$和上下文$c$, 预测要观看的video的类别（有多少个候选video, 就有多少个类别）。如下式所示：</p>\n<p>$P(w_t=i|U,C)=\\frac{e^{v_i\\,u}}{\\sum_{j\\in V} \\; e^{v_j\\,u}}\\;\\;(式1)$ </p>\n<p>其中$u$表示的用户和上下文的隐语义向量，$v_j$表示第$j$个候选video的隐语义向量。隐语义向量的获取实际上是把一些稀疏的实体信息（如user, video等）映射为N维空间的稠密实向量，而DNN的作用就是从用户的浏览历史和上下文中学习隐语义向量$u$, $u$进一步可用于在$softmax$分类器中预测对应的video类别（即当前用户在上下文环境C时,最可能看的video）. </p>\n<p>尽管YouTube上存在显式反馈数据，但是论文中依然使用隐反馈数据，当用户看完一个视频则认为是正样本。这是由于用户的显示反馈行为是非常稀疏的，无法较好地进行模型的训练和预测，而用户存在大量的隐反馈行为，通过采用隐反馈数据，可以有效地以完成模型的训练和预测。</p>\n<p><strong>高效的多类别训练：</strong>为了有效地完成数十亿类别的模型训练，论文采用了一种负采样的方法，首选根据负样本的分布采样负样本，然后通过重要性加权进行纠正，具体使用的方法可以参见文献【2】。对于每个样本，其真实的标签和负采样的类别标识都通过最小化交叉熵优化模型。实际上，每次都采样几千个样本，相对于传统的softmax, 其训练速度提升100倍以上。另外一种可行的高效训练方法是$hierarchical softmax$，但由于遍历每个节点等于在将无关的类别进行分类，这使得分类更加复杂，有损模型的效果。</p>\n<p>在线服务阶段，需要在几十毫秒内计算当前用户最可能喜欢的$N$个video, 这需要一个近似的得分方案，使得时间延迟对于类别数量的增长是低于线性的。论文的采用的是基于hash的方法【3】，从点积空间寻找最近的$N$个类别作为推荐的结果。</p>\n<h2 id=\"模型架构\"><a href=\"#模型架构\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h2><p>对于每个video, 学习其在固定词典上的高维嵌入向量，并把这些向量作为神经网络的输入。每个用户的观看历史可用一个变长的video id序列来表示，每个video id可以通过嵌入的方式得到一个稠密的向量。这些向量通过聚合操作，得到单个向量（实验表示通过对所有向量求平均是最好的策略），作为神经网络的输入。这些video的嵌入向量，和其他的模型参数，都通过误差反向传播，并利用梯度下降方法进行学习。图2是触发模块的模型架构，其输入同时包括观看的video特征和和其他各种非video特征。</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/recommand_matching.png\" alt=\"“候选推荐”\"><br></center>\n\n<p>图2：<strong>触发模块结构图：嵌入的稀疏特征和和非稠密特征同时作为模型的输入，同一个用户的多个嵌入特征通过平均的方式，得到固定的尺寸，并输入到隐藏层，所有的隐藏层都是全连接。在模型训练阶段，通过最小化交叉熵，并采用梯度下降的方法对sampled softmax进行求解。在serving阶段，通过一个近似最近邻的查找，得到数百个候选推荐结果</strong></p>\n<h2 id=\"特征多样性\"><a href=\"#特征多样性\" class=\"headerlink\" title=\"特征多样性\"></a>特征多样性</h2><p>使用DNN作为矩阵分解方法的进一步泛化，可以带来的一个非常明显的优势：任何的连续特征和离散特征，都能比较容易地加入到模型中。</p>\n<p><strong>搜索历史：</strong>和观看历史非常近似，每个搜索query都可以分词为unigrams和bigrams，每个分词可以嵌入为向量，将用户的所有token对应的嵌入向量进行平均，形成代表用户搜索历史的特征。</p>\n<p><strong>人口属性：</strong>人口属性提供了非常重要的先验信息，有助于对新用户进行合理的推荐。地域和设备信息都通过嵌入的方式，拼接到整个模型的输入中。比较简单的离散或连续特征如用户性别、用户登录状态、用户年龄等，直接以归一化到0-1之间的实数形式输入到神经网络。</p>\n<p><strong>样本年龄：</strong> 经过持续的观察得知，用户更加喜欢新内容，尽管不会以牺牲相关性为代价。如果我们简单地推荐新内容给用户，可能会使得推荐的内容不够相关。使用机器学习的方式进行推荐时，由于模型的训练，都来在历史样本，会使得历史的内容更容易得到推荐。论文的推荐系统产生的推荐结果，在训练窗口的几个星期内的流行度变化显示了用户在每个时间的平均喜好程度，同时表明video的流行度不是固定不变的，而是符合多项式分布的。为了去除不同时间因素的影响，我们把样本年龄特征加入模型进行训练，在server阶段，该特征置为零，表示当前时间在训练窗口的末尾。图3显示了该方法的效果：</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/example_age_efficacy.png\" alt=\"“样本年龄特征效果”\"><br></center>\n\n<p><strong>图3：加入样本年龄特征后模型效果，使用样本年龄作为特征后，模型可以精确表示video的上传时间和独立于时间的属性。没有这个特征，模型预测的是整个训练时间窗口的平均喜好程度</strong></p>\n<h2 id=\"标签和上下文选择\"><a href=\"#标签和上下文选择\" class=\"headerlink\" title=\"标签和上下文选择\"></a>标签和上下文选择</h2><p>通过解决一个问题，然后将这个计算结果转换到特定的上下文环境中使用，是推荐业务经常使用的一种方式，这种方式对于线上的AB测试非常重要，但是离线实验对于在线的效果不是很好评估。</p>\n<p><strong>训练样本来自所有用户观看记录，而不只是推荐结果的观看记录。否则，很难推荐出新的video, 而只是在旧的内容进行exploitation；通过确保每个用户具有相同样本数，可以避免部分活跃用户的行为带来的训练误差，提升模型的在线效果；为了防止模型在原始问题上过拟合，对于分类器信息的使用需要非常谨慎。例如用户刚搜了一个query, 分类器在进行推荐时会选择搜索结果页中对应的视频，但是推荐用户刚搜过视频对于用户的体验是不好的，通过将原始的query分词后，抛弃序列信息，以bag of words的方式提供特征，分类器会不受原始搜索结果的影响</strong></p>\n<p>用户观看视频的顺序会对最终的观看概率有较大影响。因此，在训练的时候，用历史发生的行为+历史行为之后的视频观看结果作为样本进行训练，要好于用所有的行为+随机的视频观看结果进行训练。如图4所示：</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/trainning_sequence_behavior_and_lable.png\" alt=\"“训练数据和lable组合方式”\"><br></center>\n\n<p><strong>图4：lable和输入上下文。 选择样本标签及对应的上下文对于样本准备更有挑战，但是对于在线的效果提升非常有帮助,4-b的效果要远远好于4-a.在4-b中，$t_{max}$表示训练窗口的最大观察时刻，$t_{max}-t_N$表示样本年龄特征</strong></p>\n<h2 id=\"特征与DNN层数实验\"><a href=\"#特征与DNN层数实验\" class=\"headerlink\" title=\"特征与DNN层数实验\"></a>特征与DNN层数实验</h2><p>通过增加特征和DNN的层数，可以显著提升模型的效果，如图5所示。1M数量的video和搜索token嵌入分别嵌入到256维的向量，每个用户观看历史为最近的50个video和最近的50个query分词结果，softmax层输出1M个video的多项式分布概率。模型的结构是塔型，最底层的单元数最多，每向上一层单元数都减少一半。0层的网络等价于一个线性分解器，和YouTube早先的推荐系统类似。在进行网络调优时，网络的宽度和层数逐渐增加，直到带来的收益不再增加或者收敛变得困难。</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/model_performance_with_feature_depth.png\" alt=\"“模型效果与特征、DNN层数的关系”\"><br></center>\n\n<p> <strong>图5：模型效果与特征、DNN层数的关系。 Mean Average Precision (MAP) 随着特征、层数的增加而提升</strong></p>\n<h1 id=\"排序模块\"><a href=\"#排序模块\" class=\"headerlink\" title=\"排序模块\"></a>排序模块</h1><p>在排序阶段，由于只对几百个候选样本进行打分，可以采用更多特征描述video, user和video的关系。排序不仅可以上述rank模型的结果进行精选，也可用于对多个来源的候选结果进行融合。论文采用和触发模块类似的模型结构，并利用LR模型对候选结果进行打分和排序，并返回得分最高的video作为排序模块的结果输出。排序模型最终以平均每次曝光的观看时间作为优化目标，而非点击率，通过点击率作为优化目标容易推荐带有欺骗性的视屏，这些视频虽然容易吸引用户点击，但用户点进去后会很快离开。排序模型的架构如图6所示：</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/recommand_ranking.png\" alt=\"“排序模块架构”\"><br></center>\n\n<p> <strong>图6：深度排序模型架构，输入包括univalent（如当前待评分的video id）和multivalent（如用户最近浏览过的多个video id）的离散特征的嵌入、连续特征的各个幂运算等，共输入几百个特征</strong></p>\n<h2 id=\"特征表示\"><a href=\"#特征表示\" class=\"headerlink\" title=\"特征表示\"></a>特征表示</h2><p>排序模块包括离散特征（单值特征和多值特征）和连续特征，其中离散特征包括二值特征（如是否登录状态）和多个值的离散特征（如video id）, 多个离散值的特征包括univalent（如当前待评分的video id）和multivalent（如用户最近浏览过的多个video id），论文也使用了其他特征，如是否描述item(如曝光)，是否描述user/context(如query)</p>\n<h3 id=\"Feature-Engineering\"><a href=\"#Feature-Engineering\" class=\"headerlink\" title=\"Feature Engineering\"></a>Feature Engineering</h3><p>尽管深度神经网络可以减少工程的难度，但是大多数特征还是不能直接输入到神经网络，需要花费一定的时间将用户、video等特征转化为可以直接输入的形式。</p>\n<p>最有用的特征是描述用户之前和item或者类似item的交互特征，如用户从对应的频道看的视频个数、用户最近看该主题video的时间，这类特征具有很强的泛化性能，候选集分数，候选集来源，历史的video曝光情况等非常重要。</p>\n<h3 id=\"Embedding-Categorical-Features\"><a href=\"#Embedding-Categorical-Features\" class=\"headerlink\" title=\"Embedding Categorical Features\"></a>Embedding Categorical Features</h3><p>同触发模块类似，将稀疏的离散特征嵌入到稠密的表示空间，用于神经网络的输入。每个独有的ID序列都会被映射到这样的一个空间，映射后特征的维度同ID值个数的对数成正比。在训练之前，通过一些扫描所有的ID建立查找表，在后续的训练过程中可以直接查找和更新这个表。对于ID值非常多的空间，根据点击的频率进行从高到低进行排序，对TOP N的ID进行嵌入，其他ID嵌入结果直接由0向量表示。multivalent离散特征的嵌入结果是多个univalent嵌入结果的均值。</p>\n<p>在同样的ID空间的不同特征，都共享该特征的嵌入结果。如曝光的video ID，上次观看的video ID等。不同的特征虽然共享嵌入的结果，但是分别输入到神经网络进行训练，可以在学习时利用到每个特征表示的具体信息。<strong>嵌入结果的共享对于模型提升泛化性能、训练速度，减少内存消耗都是非常重要的。</strong></p>\n<h3 id=\"Normalizing-Continuous-Features\"><a href=\"#Normalizing-Continuous-Features\" class=\"headerlink\" title=\"Normalizing Continuous Features\"></a>Normalizing Continuous Features</h3><p>通常神经网络对于特征的尺度和分布非常敏感，而基于树的组合方法对尺度和分布不敏感。因此对于论文模型，为连续特征选择合适的规范化方法非常重要。为了使得原始特征映射到0-1之间的均匀分布，采用公式2进行特征的转换，该转换类似于在训练之前，为特征值的分位数进行差值。</p>\n<p>$\\bar x = \\int_{-\\infty}^xd\\,f\\;\\;(式2)$</p>\n<p>除了规范化后的$\\bar x$, 同时增加$\\bar x^2$和$\\sqrt{\\bar x}$,同时捕获原始特征的超线性和次线性，赋予模型更加丰富的表达能力，实验证明通过这些特征的加入，可以提升模型的效果。</p>\n<h2 id=\"期望浏览时间建模\"><a href=\"#期望浏览时间建模\" class=\"headerlink\" title=\"期望浏览时间建模\"></a>期望浏览时间建模</h2><p>论文的目标是对期望浏览时间进行建模，其中正样本是有点击行为的样本，负样本是没有发生点击行为的样本。每个正样本都有对应的浏览时间，负样本的浏览时间为0. 为了预测期望浏览时间，采用加权逻辑回归的方法，正样本的权重是对应的浏览时间，负样本使用单位权重，采用交叉熵作为目标函数。这样模型学习的优势比odds为$\\frac{\\sum T_i}{N-k}$, 关于优势比的定义可参见【3】。其中$N$是训练样本数，$k$是正样本数，$T_i$对应第i个视频的浏览时间。由于实际正样本的数量比较少，优势比等价于$E[T](1+P)$, $P$是点击率，$E[T]$是期望浏览时间，由于$P$比较小,该目标进一步可以近似为平均观看时间。在预测阶段，使用指数函数$e^x$作为激活函数得到优势比odds, 作为预测的平均浏览时间的近似值。</p>\n<h2 id=\"针对隐藏层的实验\"><a href=\"#针对隐藏层的实验\" class=\"headerlink\" title=\"针对隐藏层的实验\"></a>针对隐藏层的实验</h2><p>表1是不同隐藏层配置对应的对于next-day的预测结果，评估指标是<strong>用户加权平均损失</strong>，每个配置都是评估同一个页面的正负样本，如果正样本分数低于负样本，那么正样本的浏览时间是<strong>预测错误浏览时长</strong>， <strong>用户加权平均损失</strong>定义为<strong>所有预测错误的浏览时间占比所有评估对的浏览时间之和</strong></p>\n<p>结果显示，增加神经网络的层数和每一层的宽度，都可以提升模型的效果。在CPU预算允许的前提下，论文采用了1024 ReLU =》 512 ReLU =》 256 ReLU的配置。不使用连续特征的幂特征，损失增加0.2%。正负样本权值设置相等，损失增加非常明显，达到4.1%。</p>\n<center><br><img src=\"/dnn_for_youtube_recommandation/evaluation_of_diffrent_configure.png\" alt=\"“隐藏层配置对预测结果的影响”\"><br><strong>表1：不同ReLU单元隐藏层配置对next-day的预测结果影响，评估指标是用户加权平均损失</strong></center>\n\n\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>论文描述了YouTube视频推荐的深度网络结构，主要分为触发模块和排序模块</p>\n<p><strong>触发模块：</strong>论文采用的深度协同过滤，能够使用各种信号特征，并通过多层网络建模信号之间的交互，效果好于之前的矩阵分解模型；通过建模时考虑不对称的行为序列，能够更好地使用所有信息对未来进行预测，使得离线训练的模型能更好地用于在线预测；对于分类器的所有信号特征不直接使用，而是对其中一些特征经过仔细分析和加工后使用，能够获得更好的推荐效果；使用样本的年龄特征，能够避开时间因素的干扰，使得预测结果独立于时间特征。</p>\n<p><strong>排序模块：</strong>模型在预测平均观看时间方面，好于之前的线性方法、组合树的方法。通过使用标签之前的用户行为，用户的效果有较大的提升；深度网络需要对离散特征做嵌入处理，对连续特征做规范化处理；多层网络能够有效地建模数百个特征之间的非线性交互；采用加权逻辑回归的方式，对于正负样本分别赋予不同全值，使得我们能够较好地学习优势比，并用优势比来预测浏览时间。在使用用户平均加权损失来评估效果时，该方法的效果要远远好于直接用点击率进行建模。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>【1】P Covington, J Adams, E Sargin. Deep Neural Networks for YouTube Recommendations, Acm Conference on Recommender Systems, 2016 :191-198<br>【2】S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007, 2014.<br>【3】T. Liu, A. W. Moore, A. Gray, and K. Yang. An<br>investigation of practical approximate nearest<br>neighbor algorithms. pages 825–832. MIT Press, 2004.<br>【4】nside_Zhang, <a href=\"http://blog.csdn.net/lanchunhui/article/details/51037264\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lanchunhui/article/details/51037264</a>, 2016</p>\n"},{"title":"矩阵分解模型的分布式求解","date":"2018-01-02T16:00:00.000Z","toc":true,"description":"矩阵分解模型分布式求解","mathjax":true,"_content":"\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n矩阵分解(mf)模型在推荐系统中有非常不错的表现，相对于传统的协同过滤方法，它不仅能通过降维增加模型的泛化能力，也方便加入其他因素（如数据偏差、时间、隐反馈等）对问题建模，从而产生更佳的推荐结果。本文主要介绍mf一些概念，基于sgd的mf分布式求解，基于als的mf分布式求解。\n该文涉及的所有分布式求解都是基于openmit[1]的ps框架，因此分布式求解都是在ps基础上进行实现的。相对于spark mllib的mf实现，在同样的资源情况下，该框架下的实现能支持更大规模的矩阵分解。\n\n# 矩阵分解相关概念\n我们接触到很多的矩阵分解相关的一些概念，svd,pca,mf推荐模型,als等，如下是对这些概念的一些解释。\n\n* **svd分解**\nsvd分解,是将一个矩阵A分解为三个矩阵，如下所示：\n$A\\_{m,n}=U\\_{m,m} I\\_{m,n} V\\_{n,n}^T  (1)$\n其中矩阵$I$对角线元素为奇异值，对应$AA^T$的特征值的平方根。$U$的列为$MM^T$的特征向量(正交基向量)，称为$M$的左奇异向量。$V$的列为$M^TM$的特征向量(正交基向量)，称为$M$的右奇异向量。\n为了减少存储空间，可以用前$k$大的奇异值来近似描述矩阵$I$,$U$和$V^T$用对应前k大奇异值的左奇异向量和右奇异向量来近似，如下所示：\n$A\\_{m,n} \\approx U\\_{m,k} I\\_{k,k} V\\_{k,n}^T  (2)$\n\n* **pca**\n主成分分析，对原始数据进行降维使用。pca可以通过svd分解来实现，具体可以对公式(2)两边同时乘$V\\_{n,k}$,如下所示：\n$A\\_{m,n} V\\_{n,k} \\approx U\\_{m,k} I\\_{k,k} V\\_{k,n}^T V\\_{n,k}$\n=> $A\\_{m,n} V\\_{n,k} \\approx U\\_{m,k} I\\_{k,k}$\n=> $A\\_{m,n} V\\_{n,k} \\approx A'\\_{m,k}(3)$\n经过公式3, 矩阵A由n列降为k列，如果要对行进行降维，其推导类似。\n\n* **mf推荐模型**\n在推荐领域，一般不直接使用svd进行矩阵分解，因为svd要求所有的矩阵元素不能缺失，而推荐所使用的的rating矩阵很难是完整的（互联网上的item经常海量的，一个user很难有机会接触所有的item, 导致user-item矩阵存在大量的元素缺失)。如果使用svd分解进行推荐，首先就需要对缺失的矩阵元素进行填充，不仅耗费大量的精力，而且填充的效果并不能保证准确。\n因此，对于个性化推荐，一般直接对已知的元素建立矩阵分解模型，如式4所示：\n$MIN\\_{PQ} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)（4）$\n对于(4)这样的建模，有些学者称为svd对已知元素建模(The goal of SVD, when restricted to the known ratings)[2].\n\n* **als**\nals（交替最小二乘）是一种矩阵分解优化算法。交替求解user向量和item向量，在求解user向量的时候固定item向量，在求解item向量的时候固定user向量，直到算法收敛或达到终止条件。\nals算法可用于求解矩阵分解模型模型如公式4, 也可用于更加灵活的矩阵分解模型，如隐反馈矩阵分解模型[3], 更加灵活地用于个性化推荐。\n\n* **非负矩阵分解[4]**\n非负矩阵分解，是指将非负的大矩阵分解成两个非负的小矩阵。其目标函数和约束如下：\n$MIN\\_{PQ} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 （5）$\n$subject \\; to \\; r\\_{ui} \\geq 0\\;\\; and \\;\\;p\\_{uk} \\geq 0 \\;\\; and \\;\\; q\\_{ik} \\geq 0 $\n相对于其他矩阵分解，非负矩阵分解的输入元素为非负，分解后矩阵的元素也非负。从计算上讲，虽然分解元素为负值是正确的，但是在很多情况下，在实际问题中是没有意义的。非负矩阵广泛应用于图像分析、文本聚类、语音处理、推荐系统等。\n\n# sgd求解in openmit\n## 目标函数及优化推导\n我们令$L=\\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n对于user $u$和item $i$(rating大于0), 目标函数：$MIN\\_{PQ} (L)={(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n令$L$对$p\\_{u,k}$,$q\\_{i,k}$求导，如下所示：\n\n$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p\\_{u,k}}=e\\_{u,i}q\\_{i,k}-\\lambda p\\_{u,k}\\;(6)$\n\n$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi q\\_{i,k}}=e\\_{u,i}p\\_{u,k}-\\lambda q\\_{i,k}\\;(7)$\n\n其中$e\\_{u,i}=r\\_{ui} - p\\_u^Tq\\_i$。\n\n利用梯度下降法迭代更新user向量p和item向量q, 如下所示：\n\n$p\\_{u,k} = p\\_{u,k}+\\alpha(e\\_{u,i}q\\_{i,k}-\\lambda p\\_{u,k})\\;(8)$\n\n$q\\_{i,k} = q\\_{i,k}+\\alpha(e\\_{u,i}p\\_{u,k}-\\lambda q\\_{i,k})\\;(9)$\n\n## 分布式实现in openmit\n\n在openmit中的矩阵存储模型如下图所示:\n\n![“矩阵存储模型”](/mf/data_model.png) \n<center/>图1：矩阵存储模型</center>\n我们假定user的数量远大于item数量，P矩阵代表user向量，Q矩阵代表item向量，R代表rating元素。此时我们将Q向量分布式存储在server集群，P向量分布式存储在worker集群，每个worker节点同时存储和该user相关联的rating元素R。\n\n每个worker节点在计算user向量的时候，由于只需要用到本地user向量、与本地user相关的item向量和rating元素,而user向量和相关的rating元素存储在本地，因此只需要从server端拉取对应的item向量，就可以根据式6和式7完成user和item的梯度计算。利用公式8更新user向量，并将item梯度向量push给server集群，server端根据当前item向量权重，及worker端push的item梯度信息，根据式9更新item向量。具体流程参见如下描述:\n\n**worker端流程**\n\n```c++\n//mf 分布式sgd求解woker端\nload matrix rating data, each user data is loaded by only one worker;\nfor each epoch:\n    for each batch:\n        get batch data B;\n        get user weights for users in B, (initialize user weights if not initialized)\n        pull item weights from server\n        for each user, item pair weith rating > 0:\n            update user gradient according to eq 6;\n            update item gradient according to eq 7;\n        update user weights according to eq 8\n        push item gradients to server\n```\n\n**server端流程**\n\n```c++\n//mf 分布式sgd求解server端\nwhile(true):\n    receive a requests from worker\n    if request type is 'pull':\n        if the item weights is not initialized:\n            initialize the item weights;\n        response the item weights to worker\n    if request type is 'push':\n        receive the item gradients\n        update item weights accoreding to eq 9\n```\n\n\n\n当user的数量远小于item数量的时候，为需要减少通讯开销，需要更改输入文件，实现将item向量Q及rating元素R存储worker端，user向量P存储在server端。这样在进行数据传输的时候，worker端将会拉取user权重信息，push user梯度信息。通过传输user而非item信息，有效减少数据的通讯开销。\n\n# als求解in openmit\n## 目标函数及优化推导\n### explicit als\nexplicit als只针对user-item矩阵中已知的rating元素进行建模，目标函数如式4所示，\n我们令$L=\\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n为求解user向量p, 固定所有item向量$q\\_i$, 令$L$对$p\\_u$求导等于0，\n\n$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p\\_{u,k}} = 0$\n\n=>$\\sum\\_{i} (r\\_{ui} - \np\\_u^Tq\\_i）q\\_{i,k}-\\lambda p\\_{u,k}=0\\;$\n\n=> $\\sum\\_{i} (r\\_{ui} - \np\\_u^Tq\\_i）q\\_{i}-\\lambda p\\_{u}=0\\;$\n\n=> $(\\sum\\_iq\\_iq\\_i^T+\\lambda I)p\\_u=\\sum\\_iq\\_ir\\_{ui}\\;(10)$\n\n同理，为求解item向量q, 固定所有user向量$p\\_u$, 令$L$对$q\\_i$求导等于0，可得\n$(\\sum\\_up\\_up\\_u^T+\\lambda I)q\\_i=\\sum\\_up\\_ur\\_{ui}\\;(11)$\n\n对于式10和式11,利用cholesky分解的方法求解对应的$p$和$q$向量。\n\n### implicit als\n\n对于所有的rating元素进行建模，通过$b\\_{ui}$建模user是否喜欢item, 通过$c\\_{ui}$建模user对item喜欢的程度，具体如下所示：\n\n目标函数：$MIN\\_{P,Q}\\sum\\_{u,i\\in\\mathbb K} c\\_{ui}{(b\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)\\;\\;(12)$\n\n其中$b\\_{ui} =  \\begin{cases} \n1,  & r\\_{ui}>0\\\\\\\\\n0,  & r\\_{ui}=0\n\\end{cases}\n$\n\n$c\\_{ui} = 1 + \\alpha r\\_{ui}$\n\n令$L=\\sum\\_{u,i\\in\\mathbb K} c\\_{ui}{(b\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n为求解user向量$p\\_u$, 固定所有item向量$q\\_i$, 令$L$对$p\\_u$求导等于0，\n同时，对每个用户，引入$n\\times n$矩阵$c^u$, $c^u\\_{ii}的值为c\\_{ui}$, 其余元素为0。\n\n$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p\\_{u,k}} = 0$\n\n=>$\\sum\\_{i} c\\_{ui}(b\\_{ui} - \np\\_u^Tq\\_i）q\\_{i,k}-\\lambda p\\_{u,k}=0\\;$\n\n=>$\\sum\\_{i} c\\_{ui}(b\\_{ui} - \np\\_u^Tq\\_i）q\\_{i}-\\lambda p\\_{u}=0\\;$\n\n=>$\\sum\\_{i} c^u\\_{ii}b\\_{ui}q\\_i-c^u\\_{ii}p^T\\_uq\\_iq\\_i = \\lambda p\\_u\\;\\;(13)$\n\n其中:\n$\\sum\\_{i} c^u\\_{ii}b\\_{ui}q\\_i=Q^TC^ub\\_u\\;(14)$\n\n$\\sum\\_{i} c^u\\_{ii}p^T\\_uq\\_iq\\_i = \\sum\\_{i} q\\_i c^u\\_{ii}q^T\\_ip\\_u=Q^TC^uQp\\_u \\;(15)$\n\n其中$Q$的每一行表示每个item向量。\n\n将式14和式15代入式13，得到：\n$(Q^TC^uQ+\\lambda I)p\\_u = Q^TC^ub\\_u\\;(16)$\n\n此时如果直接根据式16进行求解，假定item的个数为$n$, 每个item向量的维度为$f$, 对每个user向量的求解，仅$Q^TC^uQ$的计算就需要$O(f^2n)$.\n\n在论文[3]中，作者使用了一种有效的加速方式，$Q^TC^uQ=Q^TQ+Q^T(C^u-I)Q$, 其中$Q^TQ$不依赖具体的用户，可以在计算所有user向量之前计算好，$C^u-I$只有$n\\_u$个对角线元素非零。由于$n\\_u ≪ n$，$Q^TC^uQ$的计算效率会明显提高。同理，由于$b\\_u$也只有$n\\_u$个非零值，$Q^TC^ub\\_u$的计算效率也会非常高。假定cholesky的求解需要$O(f^3)$,则每个user向量计算的复杂度为$O(f^2n\\_u+f^3)$\n\n同理，为求解item向量$q\\_i$, 固定所有user向量$p\\_u$, 令$L$对$q\\_i$求导等于0, 可得：\n$(P^TC^iP+\\lambda I)q\\_i = P^TC^ib\\_i\\;(17)$\n\n\n## 分布式实现in openmit\nals的分布式实现和sgd的分布式实现流程基本相似，不同之处在于每个worker阶段计算的不是user和item的梯度，而是通过cholesky分解直接计算出user和item的权重。\n\n我们依然假设user的数量远远多于item的数量，worker端存储user权重和rating元素，server端存储item权重。worker端根据als计算出的user权重直接赋值给本地user向量，并将item权重push给server,由server直接赋值为新的item权重。\n\n具体流程如下伪代码所示:\n\n\n**worker端流程**\n\n```c++\n//mf 分布式als求解woker端\nload matrix rating data, each user data loaded by only one worker;\nfor each epoch:\n    for each batch:\n        get batch data B;\n        get user weights for users in B, (initialize user weights if not initialized)\n        pull item weights from server\n        pre compute Q'Q with item weight vector\n        for each user:\n            solve eq 16 by cholesky method to get user weight vector\n            assign new user weight vector\n        pre compute P'P with new user weight vector\n        for each item:\n            solve eq 17 by cholesky method to get item weight vector\n        push item weight to server\n```\n\n**server端流程**\n\n```c++\n//mf 分布式als求解server端\nwhile(true):\n    receive a requests from worker\n    if request type is 'pull':\n        if the item weights is not initialized:\n            initialize the item weights;\n        response the item weights to worker\n    if request type is 'push':\n        receive the item weights\n        assign new item weights vector\n```\n\n# 参考资料\n\n[1]https://github.com/openmit/openmit\n[2]Robert M. Bell, Yehuda Koren, \"Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights\", IEEE International Conference on Data Mining, 2007,pp.43-52\n[3]Yifan Hu, Yehuda Koren, Chris Volinsky, \"Collaborative Filtering for Implicit Feedback Datasets\", Eighth IEEE International Conference on Data Mining, 2009,pp.263-272\n[4]CJ Lin, \"Projected Gradient Methods for Nonnegative Matrix Factorization\",《Neural Computation》,2007;19(10):2756\n\n\n ","source":"_posts/mf.md","raw":"---\ntitle: 矩阵分解模型的分布式求解\ndate: 2018-01-03\ntoc: true\ncategories: 模型与算法\ntags: [矩阵分解,隐语义模型,推荐算法,协同过滤]\ndescription: 矩阵分解模型分布式求解\nmathjax: true\n---\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n矩阵分解(mf)模型在推荐系统中有非常不错的表现，相对于传统的协同过滤方法，它不仅能通过降维增加模型的泛化能力，也方便加入其他因素（如数据偏差、时间、隐反馈等）对问题建模，从而产生更佳的推荐结果。本文主要介绍mf一些概念，基于sgd的mf分布式求解，基于als的mf分布式求解。\n该文涉及的所有分布式求解都是基于openmit[1]的ps框架，因此分布式求解都是在ps基础上进行实现的。相对于spark mllib的mf实现，在同样的资源情况下，该框架下的实现能支持更大规模的矩阵分解。\n\n# 矩阵分解相关概念\n我们接触到很多的矩阵分解相关的一些概念，svd,pca,mf推荐模型,als等，如下是对这些概念的一些解释。\n\n* **svd分解**\nsvd分解,是将一个矩阵A分解为三个矩阵，如下所示：\n$A\\_{m,n}=U\\_{m,m} I\\_{m,n} V\\_{n,n}^T  (1)$\n其中矩阵$I$对角线元素为奇异值，对应$AA^T$的特征值的平方根。$U$的列为$MM^T$的特征向量(正交基向量)，称为$M$的左奇异向量。$V$的列为$M^TM$的特征向量(正交基向量)，称为$M$的右奇异向量。\n为了减少存储空间，可以用前$k$大的奇异值来近似描述矩阵$I$,$U$和$V^T$用对应前k大奇异值的左奇异向量和右奇异向量来近似，如下所示：\n$A\\_{m,n} \\approx U\\_{m,k} I\\_{k,k} V\\_{k,n}^T  (2)$\n\n* **pca**\n主成分分析，对原始数据进行降维使用。pca可以通过svd分解来实现，具体可以对公式(2)两边同时乘$V\\_{n,k}$,如下所示：\n$A\\_{m,n} V\\_{n,k} \\approx U\\_{m,k} I\\_{k,k} V\\_{k,n}^T V\\_{n,k}$\n=> $A\\_{m,n} V\\_{n,k} \\approx U\\_{m,k} I\\_{k,k}$\n=> $A\\_{m,n} V\\_{n,k} \\approx A'\\_{m,k}(3)$\n经过公式3, 矩阵A由n列降为k列，如果要对行进行降维，其推导类似。\n\n* **mf推荐模型**\n在推荐领域，一般不直接使用svd进行矩阵分解，因为svd要求所有的矩阵元素不能缺失，而推荐所使用的的rating矩阵很难是完整的（互联网上的item经常海量的，一个user很难有机会接触所有的item, 导致user-item矩阵存在大量的元素缺失)。如果使用svd分解进行推荐，首先就需要对缺失的矩阵元素进行填充，不仅耗费大量的精力，而且填充的效果并不能保证准确。\n因此，对于个性化推荐，一般直接对已知的元素建立矩阵分解模型，如式4所示：\n$MIN\\_{PQ} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)（4）$\n对于(4)这样的建模，有些学者称为svd对已知元素建模(The goal of SVD, when restricted to the known ratings)[2].\n\n* **als**\nals（交替最小二乘）是一种矩阵分解优化算法。交替求解user向量和item向量，在求解user向量的时候固定item向量，在求解item向量的时候固定user向量，直到算法收敛或达到终止条件。\nals算法可用于求解矩阵分解模型模型如公式4, 也可用于更加灵活的矩阵分解模型，如隐反馈矩阵分解模型[3], 更加灵活地用于个性化推荐。\n\n* **非负矩阵分解[4]**\n非负矩阵分解，是指将非负的大矩阵分解成两个非负的小矩阵。其目标函数和约束如下：\n$MIN\\_{PQ} \\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 （5）$\n$subject \\; to \\; r\\_{ui} \\geq 0\\;\\; and \\;\\;p\\_{uk} \\geq 0 \\;\\; and \\;\\; q\\_{ik} \\geq 0 $\n相对于其他矩阵分解，非负矩阵分解的输入元素为非负，分解后矩阵的元素也非负。从计算上讲，虽然分解元素为负值是正确的，但是在很多情况下，在实际问题中是没有意义的。非负矩阵广泛应用于图像分析、文本聚类、语音处理、推荐系统等。\n\n# sgd求解in openmit\n## 目标函数及优化推导\n我们令$L=\\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n对于user $u$和item $i$(rating大于0), 目标函数：$MIN\\_{PQ} (L)={(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n令$L$对$p\\_{u,k}$,$q\\_{i,k}$求导，如下所示：\n\n$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p\\_{u,k}}=e\\_{u,i}q\\_{i,k}-\\lambda p\\_{u,k}\\;(6)$\n\n$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi q\\_{i,k}}=e\\_{u,i}p\\_{u,k}-\\lambda q\\_{i,k}\\;(7)$\n\n其中$e\\_{u,i}=r\\_{ui} - p\\_u^Tq\\_i$。\n\n利用梯度下降法迭代更新user向量p和item向量q, 如下所示：\n\n$p\\_{u,k} = p\\_{u,k}+\\alpha(e\\_{u,i}q\\_{i,k}-\\lambda p\\_{u,k})\\;(8)$\n\n$q\\_{i,k} = q\\_{i,k}+\\alpha(e\\_{u,i}p\\_{u,k}-\\lambda q\\_{i,k})\\;(9)$\n\n## 分布式实现in openmit\n\n在openmit中的矩阵存储模型如下图所示:\n\n![“矩阵存储模型”](/mf/data_model.png) \n<center/>图1：矩阵存储模型</center>\n我们假定user的数量远大于item数量，P矩阵代表user向量，Q矩阵代表item向量，R代表rating元素。此时我们将Q向量分布式存储在server集群，P向量分布式存储在worker集群，每个worker节点同时存储和该user相关联的rating元素R。\n\n每个worker节点在计算user向量的时候，由于只需要用到本地user向量、与本地user相关的item向量和rating元素,而user向量和相关的rating元素存储在本地，因此只需要从server端拉取对应的item向量，就可以根据式6和式7完成user和item的梯度计算。利用公式8更新user向量，并将item梯度向量push给server集群，server端根据当前item向量权重，及worker端push的item梯度信息，根据式9更新item向量。具体流程参见如下描述:\n\n**worker端流程**\n\n```c++\n//mf 分布式sgd求解woker端\nload matrix rating data, each user data is loaded by only one worker;\nfor each epoch:\n    for each batch:\n        get batch data B;\n        get user weights for users in B, (initialize user weights if not initialized)\n        pull item weights from server\n        for each user, item pair weith rating > 0:\n            update user gradient according to eq 6;\n            update item gradient according to eq 7;\n        update user weights according to eq 8\n        push item gradients to server\n```\n\n**server端流程**\n\n```c++\n//mf 分布式sgd求解server端\nwhile(true):\n    receive a requests from worker\n    if request type is 'pull':\n        if the item weights is not initialized:\n            initialize the item weights;\n        response the item weights to worker\n    if request type is 'push':\n        receive the item gradients\n        update item weights accoreding to eq 9\n```\n\n\n\n当user的数量远小于item数量的时候，为需要减少通讯开销，需要更改输入文件，实现将item向量Q及rating元素R存储worker端，user向量P存储在server端。这样在进行数据传输的时候，worker端将会拉取user权重信息，push user梯度信息。通过传输user而非item信息，有效减少数据的通讯开销。\n\n# als求解in openmit\n## 目标函数及优化推导\n### explicit als\nexplicit als只针对user-item矩阵中已知的rating元素进行建模，目标函数如式4所示，\n我们令$L=\\sum\\_{u,i\\in\\mathbb K} {(r\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n为求解user向量p, 固定所有item向量$q\\_i$, 令$L$对$p\\_u$求导等于0，\n\n$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p\\_{u,k}} = 0$\n\n=>$\\sum\\_{i} (r\\_{ui} - \np\\_u^Tq\\_i）q\\_{i,k}-\\lambda p\\_{u,k}=0\\;$\n\n=> $\\sum\\_{i} (r\\_{ui} - \np\\_u^Tq\\_i）q\\_{i}-\\lambda p\\_{u}=0\\;$\n\n=> $(\\sum\\_iq\\_iq\\_i^T+\\lambda I)p\\_u=\\sum\\_iq\\_ir\\_{ui}\\;(10)$\n\n同理，为求解item向量q, 固定所有user向量$p\\_u$, 令$L$对$q\\_i$求导等于0，可得\n$(\\sum\\_up\\_up\\_u^T+\\lambda I)q\\_i=\\sum\\_up\\_ur\\_{ui}\\;(11)$\n\n对于式10和式11,利用cholesky分解的方法求解对应的$p$和$q$向量。\n\n### implicit als\n\n对于所有的rating元素进行建模，通过$b\\_{ui}$建模user是否喜欢item, 通过$c\\_{ui}$建模user对item喜欢的程度，具体如下所示：\n\n目标函数：$MIN\\_{P,Q}\\sum\\_{u,i\\in\\mathbb K} c\\_{ui}{(b\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)\\;\\;(12)$\n\n其中$b\\_{ui} =  \\begin{cases} \n1,  & r\\_{ui}>0\\\\\\\\\n0,  & r\\_{ui}=0\n\\end{cases}\n$\n\n$c\\_{ui} = 1 + \\alpha r\\_{ui}$\n\n令$L=\\sum\\_{u,i\\in\\mathbb K} c\\_{ui}{(b\\_{ui} - \np\\_u^Tq\\_i）}^2 + \\lambda(p\\_u^Tp\\_u+q\\_i^Tq\\_i)$\n\n为求解user向量$p\\_u$, 固定所有item向量$q\\_i$, 令$L$对$p\\_u$求导等于0，\n同时，对每个用户，引入$n\\times n$矩阵$c^u$, $c^u\\_{ii}的值为c\\_{ui}$, 其余元素为0。\n\n$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p\\_{u,k}} = 0$\n\n=>$\\sum\\_{i} c\\_{ui}(b\\_{ui} - \np\\_u^Tq\\_i）q\\_{i,k}-\\lambda p\\_{u,k}=0\\;$\n\n=>$\\sum\\_{i} c\\_{ui}(b\\_{ui} - \np\\_u^Tq\\_i）q\\_{i}-\\lambda p\\_{u}=0\\;$\n\n=>$\\sum\\_{i} c^u\\_{ii}b\\_{ui}q\\_i-c^u\\_{ii}p^T\\_uq\\_iq\\_i = \\lambda p\\_u\\;\\;(13)$\n\n其中:\n$\\sum\\_{i} c^u\\_{ii}b\\_{ui}q\\_i=Q^TC^ub\\_u\\;(14)$\n\n$\\sum\\_{i} c^u\\_{ii}p^T\\_uq\\_iq\\_i = \\sum\\_{i} q\\_i c^u\\_{ii}q^T\\_ip\\_u=Q^TC^uQp\\_u \\;(15)$\n\n其中$Q$的每一行表示每个item向量。\n\n将式14和式15代入式13，得到：\n$(Q^TC^uQ+\\lambda I)p\\_u = Q^TC^ub\\_u\\;(16)$\n\n此时如果直接根据式16进行求解，假定item的个数为$n$, 每个item向量的维度为$f$, 对每个user向量的求解，仅$Q^TC^uQ$的计算就需要$O(f^2n)$.\n\n在论文[3]中，作者使用了一种有效的加速方式，$Q^TC^uQ=Q^TQ+Q^T(C^u-I)Q$, 其中$Q^TQ$不依赖具体的用户，可以在计算所有user向量之前计算好，$C^u-I$只有$n\\_u$个对角线元素非零。由于$n\\_u ≪ n$，$Q^TC^uQ$的计算效率会明显提高。同理，由于$b\\_u$也只有$n\\_u$个非零值，$Q^TC^ub\\_u$的计算效率也会非常高。假定cholesky的求解需要$O(f^3)$,则每个user向量计算的复杂度为$O(f^2n\\_u+f^3)$\n\n同理，为求解item向量$q\\_i$, 固定所有user向量$p\\_u$, 令$L$对$q\\_i$求导等于0, 可得：\n$(P^TC^iP+\\lambda I)q\\_i = P^TC^ib\\_i\\;(17)$\n\n\n## 分布式实现in openmit\nals的分布式实现和sgd的分布式实现流程基本相似，不同之处在于每个worker阶段计算的不是user和item的梯度，而是通过cholesky分解直接计算出user和item的权重。\n\n我们依然假设user的数量远远多于item的数量，worker端存储user权重和rating元素，server端存储item权重。worker端根据als计算出的user权重直接赋值给本地user向量，并将item权重push给server,由server直接赋值为新的item权重。\n\n具体流程如下伪代码所示:\n\n\n**worker端流程**\n\n```c++\n//mf 分布式als求解woker端\nload matrix rating data, each user data loaded by only one worker;\nfor each epoch:\n    for each batch:\n        get batch data B;\n        get user weights for users in B, (initialize user weights if not initialized)\n        pull item weights from server\n        pre compute Q'Q with item weight vector\n        for each user:\n            solve eq 16 by cholesky method to get user weight vector\n            assign new user weight vector\n        pre compute P'P with new user weight vector\n        for each item:\n            solve eq 17 by cholesky method to get item weight vector\n        push item weight to server\n```\n\n**server端流程**\n\n```c++\n//mf 分布式als求解server端\nwhile(true):\n    receive a requests from worker\n    if request type is 'pull':\n        if the item weights is not initialized:\n            initialize the item weights;\n        response the item weights to worker\n    if request type is 'push':\n        receive the item weights\n        assign new item weights vector\n```\n\n# 参考资料\n\n[1]https://github.com/openmit/openmit\n[2]Robert M. Bell, Yehuda Koren, \"Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights\", IEEE International Conference on Data Mining, 2007,pp.43-52\n[3]Yifan Hu, Yehuda Koren, Chris Volinsky, \"Collaborative Filtering for Implicit Feedback Datasets\", Eighth IEEE International Conference on Data Mining, 2009,pp.263-272\n[4]CJ Lin, \"Projected Gradient Methods for Nonnegative Matrix Factorization\",《Neural Computation》,2007;19(10):2756\n\n\n ","slug":"mf","published":1,"updated":"2018-02-11T08:33:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjdikguda0006ga01h7pwpqbo","content":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<p>矩阵分解(mf)模型在推荐系统中有非常不错的表现，相对于传统的协同过滤方法，它不仅能通过降维增加模型的泛化能力，也方便加入其他因素（如数据偏差、时间、隐反馈等）对问题建模，从而产生更佳的推荐结果。本文主要介绍mf一些概念，基于sgd的mf分布式求解，基于als的mf分布式求解。<br>该文涉及的所有分布式求解都是基于openmit[1]的ps框架，因此分布式求解都是在ps基础上进行实现的。相对于spark mllib的mf实现，在同样的资源情况下，该框架下的实现能支持更大规模的矩阵分解。</p>\n<h1 id=\"矩阵分解相关概念\"><a href=\"#矩阵分解相关概念\" class=\"headerlink\" title=\"矩阵分解相关概念\"></a>矩阵分解相关概念</h1><p>我们接触到很多的矩阵分解相关的一些概念，svd,pca,mf推荐模型,als等，如下是对这些概念的一些解释。</p>\n<ul>\n<li><p><strong>svd分解</strong><br>svd分解,是将一个矩阵A分解为三个矩阵，如下所示：<br>$A_{m,n}=U_{m,m} I_{m,n} V_{n,n}^T  (1)$<br>其中矩阵$I$对角线元素为奇异值，对应$AA^T$的特征值的平方根。$U$的列为$MM^T$的特征向量(正交基向量)，称为$M$的左奇异向量。$V$的列为$M^TM$的特征向量(正交基向量)，称为$M$的右奇异向量。<br>为了减少存储空间，可以用前$k$大的奇异值来近似描述矩阵$I$,$U$和$V^T$用对应前k大奇异值的左奇异向量和右奇异向量来近似，如下所示：<br>$A_{m,n} \\approx U_{m,k} I_{k,k} V_{k,n}^T  (2)$</p>\n</li>\n<li><p><strong>pca</strong><br>主成分分析，对原始数据进行降维使用。pca可以通过svd分解来实现，具体可以对公式(2)两边同时乘$V_{n,k}$,如下所示：<br>$A_{m,n} V_{n,k} \\approx U_{m,k} I_{k,k} V_{k,n}^T V_{n,k}$<br>=&gt; $A_{m,n} V_{n,k} \\approx U_{m,k} I_{k,k}$<br>=&gt; $A_{m,n} V_{n,k} \\approx A’_{m,k}(3)$<br>经过公式3, 矩阵A由n列降为k列，如果要对行进行降维，其推导类似。</p>\n</li>\n<li><p><strong>mf推荐模型</strong><br>在推荐领域，一般不直接使用svd进行矩阵分解，因为svd要求所有的矩阵元素不能缺失，而推荐所使用的的rating矩阵很难是完整的（互联网上的item经常海量的，一个user很难有机会接触所有的item, 导致user-item矩阵存在大量的元素缺失)。如果使用svd分解进行推荐，首先就需要对缺失的矩阵元素进行填充，不仅耗费大量的精力，而且填充的效果并不能保证准确。<br>因此，对于个性化推荐，一般直接对已知的元素建立矩阵分解模型，如式4所示：<br>$MIN_{PQ} \\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)（4）$<br>对于(4)这样的建模，有些学者称为svd对已知元素建模(The goal of SVD, when restricted to the known ratings)[2].</p>\n</li>\n<li><p><strong>als</strong><br>als（交替最小二乘）是一种矩阵分解优化算法。交替求解user向量和item向量，在求解user向量的时候固定item向量，在求解item向量的时候固定user向量，直到算法收敛或达到终止条件。<br>als算法可用于求解矩阵分解模型模型如公式4, 也可用于更加灵活的矩阵分解模型，如隐反馈矩阵分解模型[3], 更加灵活地用于个性化推荐。</p>\n</li>\n<li><p><strong>非负矩阵分解[4]</strong><br>非负矩阵分解，是指将非负的大矩阵分解成两个非负的小矩阵。其目标函数和约束如下：<br>$MIN_{PQ} \\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 （5）$<br>$subject \\; to \\; r_{ui} \\geq 0\\;\\; and \\;\\;p_{uk} \\geq 0 \\;\\; and \\;\\; q_{ik} \\geq 0 $<br>相对于其他矩阵分解，非负矩阵分解的输入元素为非负，分解后矩阵的元素也非负。从计算上讲，虽然分解元素为负值是正确的，但是在很多情况下，在实际问题中是没有意义的。非负矩阵广泛应用于图像分析、文本聚类、语音处理、推荐系统等。</p>\n</li>\n</ul>\n<h1 id=\"sgd求解in-openmit\"><a href=\"#sgd求解in-openmit\" class=\"headerlink\" title=\"sgd求解in openmit\"></a>sgd求解in openmit</h1><h2 id=\"目标函数及优化推导\"><a href=\"#目标函数及优化推导\" class=\"headerlink\" title=\"目标函数及优化推导\"></a>目标函数及优化推导</h2><p>我们令$L=\\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>对于user $u$和item $i$(rating大于0), 目标函数：$MIN_{PQ} (L)={(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>令$L$对$p_{u,k}$,$q_{i,k}$求导，如下所示：</p>\n<p>$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p_{u,k}}=e_{u,i}q_{i,k}-\\lambda p_{u,k}\\;(6)$</p>\n<p>$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi q_{i,k}}=e_{u,i}p_{u,k}-\\lambda q_{i,k}\\;(7)$</p>\n<p>其中$e_{u,i}=r_{ui} - p_u^Tq_i$。</p>\n<p>利用梯度下降法迭代更新user向量p和item向量q, 如下所示：</p>\n<p>$p_{u,k} = p_{u,k}+\\alpha(e_{u,i}q_{i,k}-\\lambda p_{u,k})\\;(8)$</p>\n<p>$q_{i,k} = q_{i,k}+\\alpha(e_{u,i}p_{u,k}-\\lambda q_{i,k})\\;(9)$</p>\n<h2 id=\"分布式实现in-openmit\"><a href=\"#分布式实现in-openmit\" class=\"headerlink\" title=\"分布式实现in openmit\"></a>分布式实现in openmit</h2><p>在openmit中的矩阵存储模型如下图所示:</p>\n<p><img src=\"/mf/data_model.png\" alt=\"“矩阵存储模型”\"> </p>\n<p><center>图1：矩阵存储模型</center><br>我们假定user的数量远大于item数量，P矩阵代表user向量，Q矩阵代表item向量，R代表rating元素。此时我们将Q向量分布式存储在server集群，P向量分布式存储在worker集群，每个worker节点同时存储和该user相关联的rating元素R。</p>\n<p>每个worker节点在计算user向量的时候，由于只需要用到本地user向量、与本地user相关的item向量和rating元素,而user向量和相关的rating元素存储在本地，因此只需要从server端拉取对应的item向量，就可以根据式6和式7完成user和item的梯度计算。利用公式8更新user向量，并将item梯度向量push给server集群，server端根据当前item向量权重，及worker端push的item梯度信息，根据式9更新item向量。具体流程参见如下描述:</p>\n<p><strong>worker端流程</strong></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//mf 分布式sgd求解woker端</span></span><br><span class=\"line\">load matrix rating data, each user data is loaded by only one worker;</span><br><span class=\"line\"><span class=\"keyword\">for</span> each epoch:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> each batch:</span><br><span class=\"line\">        get batch data B;</span><br><span class=\"line\">        get user weights <span class=\"keyword\">for</span> users in B, (initialize user weights <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> initialized)</span><br><span class=\"line\">        pull item weights from server</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each user, item pair weith rating &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">            update user gradient according to eq <span class=\"number\">6</span>;</span><br><span class=\"line\">            update item gradient according to eq <span class=\"number\">7</span>;</span><br><span class=\"line\">        update user weights according to eq <span class=\"number\">8</span></span><br><span class=\"line\">        push item gradients to server</span><br></pre></td></tr></table></figure>\n<p><strong>server端流程</strong></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//mf 分布式sgd求解server端</span></span><br><span class=\"line\"><span class=\"keyword\">while</span>(<span class=\"literal\">true</span>):</span><br><span class=\"line\">    receive a requests from worker</span><br><span class=\"line\">    if request type is 'pull':</span><br><span class=\"line\">        <span class=\"keyword\">if</span> the item weights is <span class=\"keyword\">not</span> initialized:</span><br><span class=\"line\">            initialize the item weights;</span><br><span class=\"line\">        response the item weights to worker</span><br><span class=\"line\">    if request type is 'push':</span><br><span class=\"line\">        receive the item gradients</span><br><span class=\"line\">        update item weights accoreding to eq <span class=\"number\">9</span></span><br></pre></td></tr></table></figure>\n<p>当user的数量远小于item数量的时候，为需要减少通讯开销，需要更改输入文件，实现将item向量Q及rating元素R存储worker端，user向量P存储在server端。这样在进行数据传输的时候，worker端将会拉取user权重信息，push user梯度信息。通过传输user而非item信息，有效减少数据的通讯开销。</p>\n<h1 id=\"als求解in-openmit\"><a href=\"#als求解in-openmit\" class=\"headerlink\" title=\"als求解in openmit\"></a>als求解in openmit</h1><h2 id=\"目标函数及优化推导-1\"><a href=\"#目标函数及优化推导-1\" class=\"headerlink\" title=\"目标函数及优化推导\"></a>目标函数及优化推导</h2><h3 id=\"explicit-als\"><a href=\"#explicit-als\" class=\"headerlink\" title=\"explicit als\"></a>explicit als</h3><p>explicit als只针对user-item矩阵中已知的rating元素进行建模，目标函数如式4所示，<br>我们令$L=\\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>为求解user向量p, 固定所有item向量$q_i$, 令$L$对$p_u$求导等于0，</p>\n<p>$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p_{u,k}} = 0$</p>\n<p>=&gt;$\\sum_{i} (r_{ui} -<br>p_u^Tq_i）q_{i,k}-\\lambda p_{u,k}=0\\;$</p>\n<p>=&gt; $\\sum_{i} (r_{ui} -<br>p_u^Tq_i）q_{i}-\\lambda p_{u}=0\\;$</p>\n<p>=&gt; $(\\sum_iq_iq_i^T+\\lambda I)p_u=\\sum_iq_ir_{ui}\\;(10)$</p>\n<p>同理，为求解item向量q, 固定所有user向量$p_u$, 令$L$对$q_i$求导等于0，可得<br>$(\\sum_up_up_u^T+\\lambda I)q_i=\\sum_up_ur_{ui}\\;(11)$</p>\n<p>对于式10和式11,利用cholesky分解的方法求解对应的$p$和$q$向量。</p>\n<h3 id=\"implicit-als\"><a href=\"#implicit-als\" class=\"headerlink\" title=\"implicit als\"></a>implicit als</h3><p>对于所有的rating元素进行建模，通过$b_{ui}$建模user是否喜欢item, 通过$c_{ui}$建模user对item喜欢的程度，具体如下所示：</p>\n<p>目标函数：$MIN_{P,Q}\\sum_{u,i\\in\\mathbb K} c_{ui}{(b_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)\\;\\;(12)$</p>\n<p>其中$b_{ui} =  \\begin{cases}<br>1,  &amp; r_{ui}&gt;0\\\\<br>0,  &amp; r_{ui}=0<br>\\end{cases}<br>$</p>\n<p>$c_{ui} = 1 + \\alpha r_{ui}$</p>\n<p>令$L=\\sum_{u,i\\in\\mathbb K} c_{ui}{(b_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>为求解user向量$p_u$, 固定所有item向量$q_i$, 令$L$对$p_u$求导等于0，<br>同时，对每个用户，引入$n\\times n$矩阵$c^u$, $c^u_{ii}的值为c_{ui}$, 其余元素为0。</p>\n<p>$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p_{u,k}} = 0$</p>\n<p>=&gt;$\\sum_{i} c_{ui}(b_{ui} -<br>p_u^Tq_i）q_{i,k}-\\lambda p_{u,k}=0\\;$</p>\n<p>=&gt;$\\sum_{i} c_{ui}(b_{ui} -<br>p_u^Tq_i）q_{i}-\\lambda p_{u}=0\\;$</p>\n<p>=&gt;$\\sum_{i} c^u_{ii}b_{ui}q_i-c^u_{ii}p^T_uq_iq_i = \\lambda p_u\\;\\;(13)$</p>\n<p>其中:<br>$\\sum_{i} c^u_{ii}b_{ui}q_i=Q^TC^ub_u\\;(14)$</p>\n<p>$\\sum_{i} c^u_{ii}p^T_uq_iq_i = \\sum_{i} q_i c^u_{ii}q^T_ip_u=Q^TC^uQp_u \\;(15)$</p>\n<p>其中$Q$的每一行表示每个item向量。</p>\n<p>将式14和式15代入式13，得到：<br>$(Q^TC^uQ+\\lambda I)p_u = Q^TC^ub_u\\;(16)$</p>\n<p>此时如果直接根据式16进行求解，假定item的个数为$n$, 每个item向量的维度为$f$, 对每个user向量的求解，仅$Q^TC^uQ$的计算就需要$O(f^2n)$.</p>\n<p>在论文[3]中，作者使用了一种有效的加速方式，$Q^TC^uQ=Q^TQ+Q^T(C^u-I)Q$, 其中$Q^TQ$不依赖具体的用户，可以在计算所有user向量之前计算好，$C^u-I$只有$n_u$个对角线元素非零。由于$n_u ≪ n$，$Q^TC^uQ$的计算效率会明显提高。同理，由于$b_u$也只有$n_u$个非零值，$Q^TC^ub_u$的计算效率也会非常高。假定cholesky的求解需要$O(f^3)$,则每个user向量计算的复杂度为$O(f^2n_u+f^3)$</p>\n<p>同理，为求解item向量$q_i$, 固定所有user向量$p_u$, 令$L$对$q_i$求导等于0, 可得：<br>$(P^TC^iP+\\lambda I)q_i = P^TC^ib_i\\;(17)$</p>\n<h2 id=\"分布式实现in-openmit-1\"><a href=\"#分布式实现in-openmit-1\" class=\"headerlink\" title=\"分布式实现in openmit\"></a>分布式实现in openmit</h2><p>als的分布式实现和sgd的分布式实现流程基本相似，不同之处在于每个worker阶段计算的不是user和item的梯度，而是通过cholesky分解直接计算出user和item的权重。</p>\n<p>我们依然假设user的数量远远多于item的数量，worker端存储user权重和rating元素，server端存储item权重。worker端根据als计算出的user权重直接赋值给本地user向量，并将item权重push给server,由server直接赋值为新的item权重。</p>\n<p>具体流程如下伪代码所示:</p>\n<p><strong>worker端流程</strong></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//mf 分布式als求解woker端</span></span><br><span class=\"line\">load matrix rating data, each user data loaded by only one worker;</span><br><span class=\"line\"><span class=\"keyword\">for</span> each epoch:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> each batch:</span><br><span class=\"line\">        get batch data B;</span><br><span class=\"line\">        get user weights <span class=\"keyword\">for</span> users in B, (initialize user weights <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> initialized)</span><br><span class=\"line\">        pull item weights from server</span><br><span class=\"line\">        pre compute Q'Q with item weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> each user:</span><br><span class=\"line\">            solve eq <span class=\"number\">16</span> by cholesky method to get user weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">            assign <span class=\"keyword\">new</span> user weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">        pre compute P'P with <span class=\"keyword\">new</span> user weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> each item:</span><br><span class=\"line\">            solve eq <span class=\"number\">17</span> by cholesky method to get item weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">        push item weight to server</span><br></pre></td></tr></table></figure>\n<p><strong>server端流程</strong></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//mf 分布式als求解server端</span></span><br><span class=\"line\"><span class=\"keyword\">while</span>(<span class=\"literal\">true</span>):</span><br><span class=\"line\">    receive a requests from worker</span><br><span class=\"line\">    if request type is 'pull':</span><br><span class=\"line\">        <span class=\"keyword\">if</span> the item weights is <span class=\"keyword\">not</span> initialized:</span><br><span class=\"line\">            initialize the item weights;</span><br><span class=\"line\">        response the item weights to worker</span><br><span class=\"line\">    if request type is 'push':</span><br><span class=\"line\">        receive the item weights</span><br><span class=\"line\">        assign <span class=\"keyword\">new</span> item weights <span class=\"built_in\">vector</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>[1]<a href=\"https://github.com/openmit/openmit\" target=\"_blank\" rel=\"noopener\">https://github.com/openmit/openmit</a><br>[2]Robert M. Bell, Yehuda Koren, “Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights”, IEEE International Conference on Data Mining, 2007,pp.43-52<br>[3]Yifan Hu, Yehuda Koren, Chris Volinsky, “Collaborative Filtering for Implicit Feedback Datasets”, Eighth IEEE International Conference on Data Mining, 2009,pp.263-272<br>[4]CJ Lin, “Projected Gradient Methods for Nonnegative Matrix Factorization”,《Neural Computation》,2007;19(10):2756</p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<p>矩阵分解(mf)模型在推荐系统中有非常不错的表现，相对于传统的协同过滤方法，它不仅能通过降维增加模型的泛化能力，也方便加入其他因素（如数据偏差、时间、隐反馈等）对问题建模，从而产生更佳的推荐结果。本文主要介绍mf一些概念，基于sgd的mf分布式求解，基于als的mf分布式求解。<br>该文涉及的所有分布式求解都是基于openmit[1]的ps框架，因此分布式求解都是在ps基础上进行实现的。相对于spark mllib的mf实现，在同样的资源情况下，该框架下的实现能支持更大规模的矩阵分解。</p>\n<h1 id=\"矩阵分解相关概念\"><a href=\"#矩阵分解相关概念\" class=\"headerlink\" title=\"矩阵分解相关概念\"></a>矩阵分解相关概念</h1><p>我们接触到很多的矩阵分解相关的一些概念，svd,pca,mf推荐模型,als等，如下是对这些概念的一些解释。</p>\n<ul>\n<li><p><strong>svd分解</strong><br>svd分解,是将一个矩阵A分解为三个矩阵，如下所示：<br>$A_{m,n}=U_{m,m} I_{m,n} V_{n,n}^T  (1)$<br>其中矩阵$I$对角线元素为奇异值，对应$AA^T$的特征值的平方根。$U$的列为$MM^T$的特征向量(正交基向量)，称为$M$的左奇异向量。$V$的列为$M^TM$的特征向量(正交基向量)，称为$M$的右奇异向量。<br>为了减少存储空间，可以用前$k$大的奇异值来近似描述矩阵$I$,$U$和$V^T$用对应前k大奇异值的左奇异向量和右奇异向量来近似，如下所示：<br>$A_{m,n} \\approx U_{m,k} I_{k,k} V_{k,n}^T  (2)$</p>\n</li>\n<li><p><strong>pca</strong><br>主成分分析，对原始数据进行降维使用。pca可以通过svd分解来实现，具体可以对公式(2)两边同时乘$V_{n,k}$,如下所示：<br>$A_{m,n} V_{n,k} \\approx U_{m,k} I_{k,k} V_{k,n}^T V_{n,k}$<br>=&gt; $A_{m,n} V_{n,k} \\approx U_{m,k} I_{k,k}$<br>=&gt; $A_{m,n} V_{n,k} \\approx A’_{m,k}(3)$<br>经过公式3, 矩阵A由n列降为k列，如果要对行进行降维，其推导类似。</p>\n</li>\n<li><p><strong>mf推荐模型</strong><br>在推荐领域，一般不直接使用svd进行矩阵分解，因为svd要求所有的矩阵元素不能缺失，而推荐所使用的的rating矩阵很难是完整的（互联网上的item经常海量的，一个user很难有机会接触所有的item, 导致user-item矩阵存在大量的元素缺失)。如果使用svd分解进行推荐，首先就需要对缺失的矩阵元素进行填充，不仅耗费大量的精力，而且填充的效果并不能保证准确。<br>因此，对于个性化推荐，一般直接对已知的元素建立矩阵分解模型，如式4所示：<br>$MIN_{PQ} \\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)（4）$<br>对于(4)这样的建模，有些学者称为svd对已知元素建模(The goal of SVD, when restricted to the known ratings)[2].</p>\n</li>\n<li><p><strong>als</strong><br>als（交替最小二乘）是一种矩阵分解优化算法。交替求解user向量和item向量，在求解user向量的时候固定item向量，在求解item向量的时候固定user向量，直到算法收敛或达到终止条件。<br>als算法可用于求解矩阵分解模型模型如公式4, 也可用于更加灵活的矩阵分解模型，如隐反馈矩阵分解模型[3], 更加灵活地用于个性化推荐。</p>\n</li>\n<li><p><strong>非负矩阵分解[4]</strong><br>非负矩阵分解，是指将非负的大矩阵分解成两个非负的小矩阵。其目标函数和约束如下：<br>$MIN_{PQ} \\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 （5）$<br>$subject \\; to \\; r_{ui} \\geq 0\\;\\; and \\;\\;p_{uk} \\geq 0 \\;\\; and \\;\\; q_{ik} \\geq 0 $<br>相对于其他矩阵分解，非负矩阵分解的输入元素为非负，分解后矩阵的元素也非负。从计算上讲，虽然分解元素为负值是正确的，但是在很多情况下，在实际问题中是没有意义的。非负矩阵广泛应用于图像分析、文本聚类、语音处理、推荐系统等。</p>\n</li>\n</ul>\n<h1 id=\"sgd求解in-openmit\"><a href=\"#sgd求解in-openmit\" class=\"headerlink\" title=\"sgd求解in openmit\"></a>sgd求解in openmit</h1><h2 id=\"目标函数及优化推导\"><a href=\"#目标函数及优化推导\" class=\"headerlink\" title=\"目标函数及优化推导\"></a>目标函数及优化推导</h2><p>我们令$L=\\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>对于user $u$和item $i$(rating大于0), 目标函数：$MIN_{PQ} (L)={(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>令$L$对$p_{u,k}$,$q_{i,k}$求导，如下所示：</p>\n<p>$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p_{u,k}}=e_{u,i}q_{i,k}-\\lambda p_{u,k}\\;(6)$</p>\n<p>$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi q_{i,k}}=e_{u,i}p_{u,k}-\\lambda q_{i,k}\\;(7)$</p>\n<p>其中$e_{u,i}=r_{ui} - p_u^Tq_i$。</p>\n<p>利用梯度下降法迭代更新user向量p和item向量q, 如下所示：</p>\n<p>$p_{u,k} = p_{u,k}+\\alpha(e_{u,i}q_{i,k}-\\lambda p_{u,k})\\;(8)$</p>\n<p>$q_{i,k} = q_{i,k}+\\alpha(e_{u,i}p_{u,k}-\\lambda q_{i,k})\\;(9)$</p>\n<h2 id=\"分布式实现in-openmit\"><a href=\"#分布式实现in-openmit\" class=\"headerlink\" title=\"分布式实现in openmit\"></a>分布式实现in openmit</h2><p>在openmit中的矩阵存储模型如下图所示:</p>\n<p><img src=\"/mf/data_model.png\" alt=\"“矩阵存储模型”\"> </p>\n<p><center>图1：矩阵存储模型</center><br>我们假定user的数量远大于item数量，P矩阵代表user向量，Q矩阵代表item向量，R代表rating元素。此时我们将Q向量分布式存储在server集群，P向量分布式存储在worker集群，每个worker节点同时存储和该user相关联的rating元素R。</p>\n<p>每个worker节点在计算user向量的时候，由于只需要用到本地user向量、与本地user相关的item向量和rating元素,而user向量和相关的rating元素存储在本地，因此只需要从server端拉取对应的item向量，就可以根据式6和式7完成user和item的梯度计算。利用公式8更新user向量，并将item梯度向量push给server集群，server端根据当前item向量权重，及worker端push的item梯度信息，根据式9更新item向量。具体流程参见如下描述:</p>\n<p><strong>worker端流程</strong></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//mf 分布式sgd求解woker端</span></span><br><span class=\"line\">load matrix rating data, each user data is loaded by only one worker;</span><br><span class=\"line\"><span class=\"keyword\">for</span> each epoch:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> each batch:</span><br><span class=\"line\">        get batch data B;</span><br><span class=\"line\">        get user weights <span class=\"keyword\">for</span> users in B, (initialize user weights <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> initialized)</span><br><span class=\"line\">        pull item weights from server</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each user, item pair weith rating &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">            update user gradient according to eq <span class=\"number\">6</span>;</span><br><span class=\"line\">            update item gradient according to eq <span class=\"number\">7</span>;</span><br><span class=\"line\">        update user weights according to eq <span class=\"number\">8</span></span><br><span class=\"line\">        push item gradients to server</span><br></pre></td></tr></table></figure>\n<p><strong>server端流程</strong></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//mf 分布式sgd求解server端</span></span><br><span class=\"line\"><span class=\"keyword\">while</span>(<span class=\"literal\">true</span>):</span><br><span class=\"line\">    receive a requests from worker</span><br><span class=\"line\">    if request type is 'pull':</span><br><span class=\"line\">        <span class=\"keyword\">if</span> the item weights is <span class=\"keyword\">not</span> initialized:</span><br><span class=\"line\">            initialize the item weights;</span><br><span class=\"line\">        response the item weights to worker</span><br><span class=\"line\">    if request type is 'push':</span><br><span class=\"line\">        receive the item gradients</span><br><span class=\"line\">        update item weights accoreding to eq <span class=\"number\">9</span></span><br></pre></td></tr></table></figure>\n<p>当user的数量远小于item数量的时候，为需要减少通讯开销，需要更改输入文件，实现将item向量Q及rating元素R存储worker端，user向量P存储在server端。这样在进行数据传输的时候，worker端将会拉取user权重信息，push user梯度信息。通过传输user而非item信息，有效减少数据的通讯开销。</p>\n<h1 id=\"als求解in-openmit\"><a href=\"#als求解in-openmit\" class=\"headerlink\" title=\"als求解in openmit\"></a>als求解in openmit</h1><h2 id=\"目标函数及优化推导-1\"><a href=\"#目标函数及优化推导-1\" class=\"headerlink\" title=\"目标函数及优化推导\"></a>目标函数及优化推导</h2><h3 id=\"explicit-als\"><a href=\"#explicit-als\" class=\"headerlink\" title=\"explicit als\"></a>explicit als</h3><p>explicit als只针对user-item矩阵中已知的rating元素进行建模，目标函数如式4所示，<br>我们令$L=\\sum_{u,i\\in\\mathbb K} {(r_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>为求解user向量p, 固定所有item向量$q_i$, 令$L$对$p_u$求导等于0，</p>\n<p>$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p_{u,k}} = 0$</p>\n<p>=&gt;$\\sum_{i} (r_{ui} -<br>p_u^Tq_i）q_{i,k}-\\lambda p_{u,k}=0\\;$</p>\n<p>=&gt; $\\sum_{i} (r_{ui} -<br>p_u^Tq_i）q_{i}-\\lambda p_{u}=0\\;$</p>\n<p>=&gt; $(\\sum_iq_iq_i^T+\\lambda I)p_u=\\sum_iq_ir_{ui}\\;(10)$</p>\n<p>同理，为求解item向量q, 固定所有user向量$p_u$, 令$L$对$q_i$求导等于0，可得<br>$(\\sum_up_up_u^T+\\lambda I)q_i=\\sum_up_ur_{ui}\\;(11)$</p>\n<p>对于式10和式11,利用cholesky分解的方法求解对应的$p$和$q$向量。</p>\n<h3 id=\"implicit-als\"><a href=\"#implicit-als\" class=\"headerlink\" title=\"implicit als\"></a>implicit als</h3><p>对于所有的rating元素进行建模，通过$b_{ui}$建模user是否喜欢item, 通过$c_{ui}$建模user对item喜欢的程度，具体如下所示：</p>\n<p>目标函数：$MIN_{P,Q}\\sum_{u,i\\in\\mathbb K} c_{ui}{(b_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)\\;\\;(12)$</p>\n<p>其中$b_{ui} =  \\begin{cases}<br>1,  &amp; r_{ui}&gt;0\\\\<br>0,  &amp; r_{ui}=0<br>\\end{cases}<br>$</p>\n<p>$c_{ui} = 1 + \\alpha r_{ui}$</p>\n<p>令$L=\\sum_{u,i\\in\\mathbb K} c_{ui}{(b_{ui} -<br>p_u^Tq_i）}^2 + \\lambda(p_u^Tp_u+q_i^Tq_i)$</p>\n<p>为求解user向量$p_u$, 固定所有item向量$q_i$, 令$L$对$p_u$求导等于0，<br>同时，对每个用户，引入$n\\times n$矩阵$c^u$, $c^u_{ii}的值为c_{ui}$, 其余元素为0。</p>\n<p>$-\\frac{1}{2}\\frac{\\varphi L}{\\varphi p_{u,k}} = 0$</p>\n<p>=&gt;$\\sum_{i} c_{ui}(b_{ui} -<br>p_u^Tq_i）q_{i,k}-\\lambda p_{u,k}=0\\;$</p>\n<p>=&gt;$\\sum_{i} c_{ui}(b_{ui} -<br>p_u^Tq_i）q_{i}-\\lambda p_{u}=0\\;$</p>\n<p>=&gt;$\\sum_{i} c^u_{ii}b_{ui}q_i-c^u_{ii}p^T_uq_iq_i = \\lambda p_u\\;\\;(13)$</p>\n<p>其中:<br>$\\sum_{i} c^u_{ii}b_{ui}q_i=Q^TC^ub_u\\;(14)$</p>\n<p>$\\sum_{i} c^u_{ii}p^T_uq_iq_i = \\sum_{i} q_i c^u_{ii}q^T_ip_u=Q^TC^uQp_u \\;(15)$</p>\n<p>其中$Q$的每一行表示每个item向量。</p>\n<p>将式14和式15代入式13，得到：<br>$(Q^TC^uQ+\\lambda I)p_u = Q^TC^ub_u\\;(16)$</p>\n<p>此时如果直接根据式16进行求解，假定item的个数为$n$, 每个item向量的维度为$f$, 对每个user向量的求解，仅$Q^TC^uQ$的计算就需要$O(f^2n)$.</p>\n<p>在论文[3]中，作者使用了一种有效的加速方式，$Q^TC^uQ=Q^TQ+Q^T(C^u-I)Q$, 其中$Q^TQ$不依赖具体的用户，可以在计算所有user向量之前计算好，$C^u-I$只有$n_u$个对角线元素非零。由于$n_u ≪ n$，$Q^TC^uQ$的计算效率会明显提高。同理，由于$b_u$也只有$n_u$个非零值，$Q^TC^ub_u$的计算效率也会非常高。假定cholesky的求解需要$O(f^3)$,则每个user向量计算的复杂度为$O(f^2n_u+f^3)$</p>\n<p>同理，为求解item向量$q_i$, 固定所有user向量$p_u$, 令$L$对$q_i$求导等于0, 可得：<br>$(P^TC^iP+\\lambda I)q_i = P^TC^ib_i\\;(17)$</p>\n<h2 id=\"分布式实现in-openmit-1\"><a href=\"#分布式实现in-openmit-1\" class=\"headerlink\" title=\"分布式实现in openmit\"></a>分布式实现in openmit</h2><p>als的分布式实现和sgd的分布式实现流程基本相似，不同之处在于每个worker阶段计算的不是user和item的梯度，而是通过cholesky分解直接计算出user和item的权重。</p>\n<p>我们依然假设user的数量远远多于item的数量，worker端存储user权重和rating元素，server端存储item权重。worker端根据als计算出的user权重直接赋值给本地user向量，并将item权重push给server,由server直接赋值为新的item权重。</p>\n<p>具体流程如下伪代码所示:</p>\n<p><strong>worker端流程</strong></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//mf 分布式als求解woker端</span></span><br><span class=\"line\">load matrix rating data, each user data loaded by only one worker;</span><br><span class=\"line\"><span class=\"keyword\">for</span> each epoch:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> each batch:</span><br><span class=\"line\">        get batch data B;</span><br><span class=\"line\">        get user weights <span class=\"keyword\">for</span> users in B, (initialize user weights <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> initialized)</span><br><span class=\"line\">        pull item weights from server</span><br><span class=\"line\">        pre compute Q'Q with item weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> each user:</span><br><span class=\"line\">            solve eq <span class=\"number\">16</span> by cholesky method to get user weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">            assign <span class=\"keyword\">new</span> user weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">        pre compute P'P with <span class=\"keyword\">new</span> user weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> each item:</span><br><span class=\"line\">            solve eq <span class=\"number\">17</span> by cholesky method to get item weight <span class=\"built_in\">vector</span></span><br><span class=\"line\">        push item weight to server</span><br></pre></td></tr></table></figure>\n<p><strong>server端流程</strong></p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//mf 分布式als求解server端</span></span><br><span class=\"line\"><span class=\"keyword\">while</span>(<span class=\"literal\">true</span>):</span><br><span class=\"line\">    receive a requests from worker</span><br><span class=\"line\">    if request type is 'pull':</span><br><span class=\"line\">        <span class=\"keyword\">if</span> the item weights is <span class=\"keyword\">not</span> initialized:</span><br><span class=\"line\">            initialize the item weights;</span><br><span class=\"line\">        response the item weights to worker</span><br><span class=\"line\">    if request type is 'push':</span><br><span class=\"line\">        receive the item weights</span><br><span class=\"line\">        assign <span class=\"keyword\">new</span> item weights <span class=\"built_in\">vector</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>[1]<a href=\"https://github.com/openmit/openmit\" target=\"_blank\" rel=\"noopener\">https://github.com/openmit/openmit</a><br>[2]Robert M. Bell, Yehuda Koren, “Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights”, IEEE International Conference on Data Mining, 2007,pp.43-52<br>[3]Yifan Hu, Yehuda Koren, Chris Volinsky, “Collaborative Filtering for Implicit Feedback Datasets”, Eighth IEEE International Conference on Data Mining, 2009,pp.263-272<br>[4]CJ Lin, “Projected Gradient Methods for Nonnegative Matrix Factorization”,《Neural Computation》,2007;19(10):2756</p>\n"},{"title":"lbfgs算法与源码学习","date":"2018-01-12T16:00:00.000Z","toc":true,"description":"lbfgs算法具备牛顿法收敛速度快的优点，同时又不需要存储和计算完整的hessian矩阵，能够节省大量的存储和计算资源，非常适用于解决无约束的大规模的非线性优化问题。","mathjax":true,"_content":"\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\nLBFGS（limited-memory BFGS或limited-strorate BFGS）算法具备牛顿法收敛速度快的优点，同时又不需要存储和计算完整的hessian矩阵，能够节省大量的存储和计算资源，非常适用于解决无约束的大规模非线性优化问题。\n\n本文从牛顿法出发，先简要介绍牛顿法、拟牛顿法，然后从分别从原理和源码实现的角度介绍lbfgs优化算法。其源码主要来自chokkan等人贡献[1]。\n\n# 牛顿法\n**原始牛顿法:**\n\n目标函数: $min\\;\\;f(x)\\;(1)$\n\n函数$f(x)$在$x=x\\_k$附近进行二阶泰勒展开，如下所示：\n\n$f(x) \\approx f(x\\_k) + \\bigtriangledown f(x\\_k)(x-x\\_k) + \\frac{1}{2}(x-x\\_k)^T \\bigtriangledown^2 f''(x\\_k)(x-x\\_k)\\;(2)$\n\n为了求$f(x)$的极小值,令$f(x)$的导数为0，得到：\n\n$x = x\\_k - H\\_k^{-1}g\\_k\\;(3)$\n\n其中$g\\_k$为函数$f$在$x=x\\_k$的一阶导数， $H\\_k$为函数$f$在$x=x\\_k$的二阶导数(hessian矩阵)。\n因此，为求解下次迭代结果，可直接令:\n\n$x\\_{k+1}=x\\_k-H\\_k^{-1}g\\_k\\;(4)$\n\n算法在利用牛顿法求解时，从$x=x\\_0$出发，逐步迭代直到终止。终止的条件可以是梯度的二范数小于一定值，或者达到最大迭代次数等。\n\n**阻尼牛顿法:**\n\n原始牛顿法是固定步长迭代，对于非二次型目标函数，不能保证目标函数值稳定下降。严重情况下可能造成迭代点序列发散，使得计算失败。为消除该缺点，采用阻尼牛顿法，在更新迭代点时寻求最优步长$ \\lambda\\_k$。\n\n$\\lambda\\_k=argmin\\_{\\lambda}f(x\\_k+\\lambda H\\_k^{-1}g\\_k)\\;(5)$\n\n$x\\_{k+1}=x\\_k+\\lambda H\\_k^{-1}g\\_k\\;(6)$\n\n**牛顿法及阻尼牛顿法优点：**当目标函数$f$为二次函数，且hessian矩阵正定时，通过牛顿法一步就可以得到最优解。当目标函数$f$为非二次函数，但是其二次性较强或迭代点已进入极小点附近，其收敛速度也很快。\n\n**牛顿法及阻尼牛顿法缺点:**要求目标函数$f$需要具有连续的一、二阶导数，且hessian矩阵正定；当特征维度很高时，hessian矩阵存储需要很大空间，求逆计算量也很大，不适合用于大规模问题的优化。\n\n\n# 拟牛顿法\n\n拟牛顿法的核心思想是：直接构造hessian矩阵或hessian矩阵的逆，从而在构造的近似hessian矩阵基础上按照式4或式6进行迭代求解。\n\n## 拟牛顿条件\n\n函数$f(x)$在$x=x\\_{k+1}$附近进行二阶泰勒展开，如下所示：\n\n$f(x) \\approx f(x\\_{k+1}) + \\bigtriangledown f(x\\_{k+1})(x-x\\_{k+1}) + \\frac{1}{2}(x-x\\_{k+1})^T \\bigtriangledown^2 f(x\\_{k+1})(x-x\\_{k+1})\\;(7)$\n\n对上式两边对$x$求导，如下所示:\n\n$\\bigtriangledown f(x) \\approx \\bigtriangledown f(x\\_{k+1}) + \\bigtriangledown^2 f''(x\\_{k+1})(x-x\\_{k+1})\\;(7)$\n\n\n取$x=x\\_k$,则由式7可以得到：\n\n$g\\_{k+1}-g\\_k \\approx  H\\_{k+1}(x_{k+1}-x\\_k)\\;(8)$\n\n其中$g\\_k$为函数$f$在$x=x\\_k$的一阶导数， $H\\_{k+1}$为函数$f$在$x=x\\_{k+1}$的二阶导数(hessian矩阵)。令$y\\_k=g\\_{k+1}-g\\_k$, $s\\_k=x_{k+1}-x\\_k$, $G\\_{k+1}=H^{-1}\\_{k+1}$ 得：\n\n$y\\_k \\approx H\\_{k+1} s\\_k\\;(9)$\n\n$s\\_k \\approx G\\_{k+1}y\\_k\\;(10)$\n\n式9和式10是拟牛顿条件，在迭代过程中对hessian矩阵$H\\_{k+1}$做近似，或者对hessian矩阵的逆$G\\_{k+1}$做近似，而不是直接求解hessian矩阵，就是拟牛顿法。比较常用的拟牛顿法包括DFP算法和BFGS算法。\n\n## DFP算法\n\nDFP算法的核心是通过迭代对hessian矩阵的逆进行近似，迭代公式：\n$G\\_{k+1}=G\\_k + \\bigtriangleup G\\_k, \\; k = 0, 1, 2, ... \\;(11)$\n\n其中$G\\_k$可以通过单位矩阵构造，关键在于如何构造$\\bigtriangleup G\\_k$，其构造过程如下：\n为保证对称性，我们假定:\n\n$\\bigtriangleup G\\_k=\\alpha uu^T + \\beta vv^T\\;(12)$\n\n将式11和式12代入式10，可得：\n\n$s\\_k \\approx G\\_{k+1}y\\_k$\n\n$=>s\\_k = (G\\_k + \\bigtriangleup G\\_k)y\\_k=G\\_ky\\_k+\\alpha u^Ty\\_ku+\\beta v^Ty\\_kv$\n\n$=>s\\_k-G\\_ky\\_k=\\alpha u^Ty\\_ku+\\beta v^Ty\\_kv；（13）$\n\n为使得式12成立，直接使$\\alpha u^Ty\\_k=1$, $\\beta v^Ty\\_k=-1, u=s\\_k, v=G\\_ky\\_k$，得到$\\alpha=\\frac{1}{s^Ty\\_k}$, $\\beta = -\\frac{1}{y^T\\_kG\\_ky\\_k}$, 将$\\alpha,\\beta,u,v$代入式12，得：\n\n$\\bigtriangleup G\\_k=\\frac{s\\_ks^T\\_k}{s^T\\_ky\\_k}-\\frac{G\\_ky\\_ky^T\\_kG\\_k}{y^T\\_kG\\_ky\\_k} \\;(14)$\n\nDFP算法根据式11和式14，迭代求解hessian矩阵的逆$G\\_k$,其他步骤同牛顿法（或阻尼牛顿法）。\n\n## BFGS算法\nBFGS算法核心思想是通过迭代对hessian矩阵进行近似（和DFP算法不同之处在于，DFP算法是对hessian矩阵的逆进行近似）。相对于DFP算法，BFGS算法性能更佳，具有完善的局部收敛理论，在全局收敛性研究也取得重要进展[4]。\n\nBFGS算法和DFP算法推导类似，迭代公式：\n$H\\_{k+1}=H\\_k + \\bigtriangleup H\\_k, \\; k = 0, 1, 2, ... \\;(15)$\n\n其中H\\_0可以用单位矩阵进行构造，对于$\\bigtriangleup H\\_k$的构造如下：\n\n$\\bigtriangleup H\\_k= \\alpha uu^T + \\beta vv^T\\;(16)$\n\n将式15和式16代入式9，得：\n\n$y\\_k \\approx H\\_{k+1}s\\_k $\n\n$=> y\\_k= H\\_ks\\_k+\\alpha u^Ts\\_ku + \\beta v^Ts\\_kv$\n\n$=>y\\_k-H\\_ks\\_k=\\alpha u^Ts\\_ku + \\beta v^Ts\\_kv\\;(17)$\n\n为使式17成立，直接令$u=y\\_k$, $v=H\\_ks\\_k, \\alpha u^Ts\\_k=1, \\beta v^Ts\\_k=-1$,  将$\\alpha,\\beta,u,v$代入式15，得：\n\n$\\bigtriangleup H\\_k=\\frac{y\\_ky\\_k^T}{y\\_k^Ts\\_k}-\\frac{H\\_ks\\_ks\\_k^TH\\_k^T}{s\\_k^TH\\_ks\\_k}\\;(18)$\n\nBFGS算法通过式18更新hessian矩阵的求解过程，在求解搜索方向$d\\_k=H\\_k^{-1}g\\_k$时，通过求解线性方程组$H\\_kd\\_k=g\\_k$得到$d\\_k$的值。\n\n更一般的解法是通过sherman-morrison公式[6],直接得到$H\\_{k+1}^{-1}$和$H\\_k^{-1}$之间的关系如式19所示，并根据该关系迭代求解hessian矩阵的逆:\n$H\\_{k+1}^{-1}=(I-\\frac{s\\_ky_k^T}{y\\_k^Ts\\_k})H\\_k^{-1}(I-\\frac{y\\_ks\\_k^T}{y\\_k^Ts\\_k})+\\frac{s\\_ks\\_k^T}{y\\_k^Ts\\_k}\\;($\n\n$=> G\\_{k+1}=(I-\\frac{s\\_ky_k^T}{y\\_k^Ts\\_k})G\\_k(I-\\frac{y\\_ks\\_k^T}{y\\_k^Ts\\_k})+\\frac{s\\_ks\\_k^T}{y\\_k^Ts\\_k}\\;(19)$\n\n\n# LBFGS算法\n\nBFGS算法需要存储完整的$H\\_k^{-1}$矩阵。因此，当矩阵的维度比较大时，需要占用大量的存储空间(空间复杂度为$O(N^2)$)。LBFGS算法通过使用最近$m$次迭代过程中的$s$和$y$向量，使得其存储复杂度由$O(N^2)$下降到$O(m\\times N)$[2]。\n\n本章节首先介绍lbfgs算法和求解推导、然后介绍带有L1正则的LBFGS算法求解（OWLQN算法）、最后介绍lbfgs算法在liblbfgs库[1]中的实现。\n\n## LBFGS算法及求解\n对于式19，我们令$\\rho\\_k=\\frac{1}{y\\_k^Ts\\_k}$, $v\\_k=(I-\\rho\\_ky\\_ks\\_k^T)$, 得：\n\n$G\\_{k+1}=v\\_k^TG\\_kv\\_k+\\rho\\_ks\\_ks\\_k^T\\;(20)$\n\n假定$G\\_0$是正定矩阵，则：\n\n$G\\_1=v\\_0^TG\\_0v\\_0+\\rho\\_0s\\_0s\\_0^T$\n\n$G\\_2=v\\_1^TG\\_1v\\_1+\\rho\\_1s\\_1s\\_1^T=v\\_1^Tv\\_0^TG\\_0v\\_0v\\_1+v\\_1^T\\rho\\_0s\\_0s\\_0^Tv\\_1+\\rho\\_1s\\_1s\\_1^T$\n\n$G\\_3=v\\_2^Tv\\_1^TG\\_2v\\_1v\\_2+\\rho\\_2s\\_2s\\_2^T=v\\_2^Tv\\_1^Tv\\_0^TG\\_0v\\_0v\\_1v\\_2+v\\_2^Tv\\_1^T\\rho\\_0s\\_0s\\_0^Tv\\_1v\\_2+v\\_2^T\\rho\\_1s\\_1s\\_1^Tv\\_2+\\rho\\_2s\\_2s\\_2^T$\n\n通过递归式20，可得：\n\n$G\\_{k+1}=v\\_k^Tv\\_{k-1}^T...v\\_0^TG\\_0v\\_0...v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^Tv\\_{k-1}^T...v\\_1^T\\rho\\_0 s\\_0 s\\_0^Tv\\_1...v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ ...$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^Tv\\_{k-1}^T\\rho\\_{k-2} s\\_{k-2} s\\_{k-2}v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^T\\rho\\_{k-1} s\\_{k-1} s\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\rho\\_ks\\_ks\\_k^T\\;(21)$\n\n由式21可以得出，$G\\_{k+1}$的计算需要用到$G\\_0$,$s\\_i$, $y\\_i$,其中$i=0,1,2,...k$。而lbfgs算法最关键的点在于，通过使用距离当前迭代最近的$m$个$s$向量和$y$向量，近似求解$G\\_{k+1}$。当$k+1<=m$,则根据式21直接求解$G\\_{k+1}$, 当$k+1>m$时，只保留最近的$k$个$s$向量和$y$向量,具体计算如式22所示:\n\n$G\\_{k+1}=v\\_k^Tv\\_{k-1}^T...v\\_{k-m+1}^TG\\_0v\\_{k-m+1}...v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^Tv\\_{k-1}^T...v\\_{k-m+2}^T\\rho\\_{k-m+1} s\\_{k-m+1} s\\_{k-m+1}^Tv\\_{k-m+2}...v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ ...$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^Tv\\_{k-1}^T\\rho\\_{k-2} s\\_{k-2} s\\_{k-2}v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^T\\rho\\_{k-1} s\\_{k-1} s\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\rho\\_ks\\_ks\\_k^T\\;(22)$\n\n虽然式21和式22可用于在任何情况下，对hessian矩阵的逆进行迭代求近似解，进而用于lbfgs算法求解。然而，仅仅通过式21和式22，依然需要存储hessian矩阵的逆，并不能节省存储空间。实际上，我们只要能求出$G\\_kg\\_k$(或$H\\_k^{-1}g\\_k$)，就可以避开存储完整的$G\\_{k+1}$,将存储空间由$O(N^2)$下降至$O(m\\times N)$。[2]提供了有效计算$G\\_kg\\_k$的一个迭代算法，如下所示：\n\n**算法1:**\n\n1） $if\\;iter < M: incr = 0, bound= iter$\n\n$\\;\\;\\;\\; else \\; incr= iter - m, bound = m$\n   \n2) $q\\_{bound} = g\\_{iter}$\n\n3) $for \\;i = (bound-1), ... , 0$\n  \n$\\;\\;\\;\\;\\;\\;\\;\\;j = i + incr$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;\\alpha\\_i = \\rho\\_js\\_j^Tq\\_{i+1} (存储每个\\alpha\\_i)$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;q\\_i=q\\_{i+1}-\\alpha\\_iy\\_j$\n\n$\\;\\;\\;\\;r\\_0=G\\_0.q\\_0$\n\n$\\;\\;\\;\\;for\\;i=0, 1, ..., (bound - 1)$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;j=i+incr$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;\\beta\\_i = \\rho\\_jy\\_j^Tr\\_i$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;r\\_{i+1}=r\\_i+s\\_j(\\alpha\\_i-\\beta\\_i)$\n\n\n**算法1的证明：**\n\n$q\\_{bound}=g\\_{iter}$\n\n对于$0<i<bound$,\n\n$q\\_i=q\\_{i+1}-\\alpha\\_iy\\_i \\\\ $\n\n$=q\\_{i+1}-\\rho\\_jy\\_js\\_j^Tq\\_{i+1}$\n\n$=(I-\\rho\\_jy\\_js\\_j^T)q\\_{i+1}$\n\n$=v\\_j^Tq\\_{i+1}$\n\n$=v\\_{inc+i}^Tq\\_{i+1}$\n\n$=v\\_{inc+i}v\\_{inc+i+1}q\\_{i+2}$\n\n$=v\\_{inc+i}v\\_{inc+i+1}v\\_{inc+i+2}...v\\_{inc+bound-1}q\\_{bound}\\;(23)$\n\n\n\n$\\alpha\\_i=\\rho\\_js\\_j^Tq\\_{i+1}$\n\n$=\\rho\\_{inc+i}s\\_{inc+i}^Tv\\_{inc+i+1}v\\_{inc+i+2}...v\\_{inc+bound-1}q\\_{bound}\\;(24)$\n\n$r\\_0=G\\_0q\\_0=G\\_0v\\_{inc}v\\_{inc+1}...v\\_{inc+bound-1}q\\_{bound}(25)$\n\n$r\\_{i+1}=r\\_i+s\\_j(\\alpha\\_i-\\beta\\_i)$\n\n\n$=r\\_i+s\\_j\\alpha\\_j-s\\_j\\rho\\_jy\\_j^Tr\\_i=(I-s\\_j\\rho\\_jy\\_j^T)r\\_i+s\\_j\\alpha\\_i=v\\_{inc+i}^Tr\\_i+s\\_{inc+i}\\alpha\\_i(26)$\n\n由式26可得：\n$r\\_{bound}=s\\_{inc+bound-1}\\alpha\\_{bound-1}+v\\_{inc+bound-1}r\\_{bound-1}$\n\n$=s\\_{inc+bound-1}\\rho\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}+v\\_{inc+bound-1}r\\_{bound-1}$\n\n$=s\\_{inc+bound-1}\\rho\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T(s\\_{inc+bound-2}\\alpha\\_{bound-2}+v\\_{inc+bound-2}^Tr\\_{bound-2})$\n\n$=\\rho\\_{inc+bound-1}s\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T\\rho\\_{inc+bound-2}s\\_{inc+bound-2}s\\_{inc+bound-2}^Tv\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^Tr\\_{round-2}$\n\n$=\\rho\\_{inc+bound-1}s\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T\\rho\\_{inc+bound-2}s\\_{inc+bound-2}s\\_{inc+bound-2}^Tv\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\rho\\_{inc+bound-3}s\\_{inc+bound-3}s\\_{inc+bound-3}^Tv\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^Tv\\_{inc+bound-3}^Tr\\_{bound-3}$\n\n$=\\rho\\_{inc+bound-1}s\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T\\rho\\_{inc+bound-2}s\\_{inc+bound-2}s\\_{inc+bound-2}^Tv\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\rho\\_{inc+bound-3}s\\_{inc+bound-3}s\\_{inc+bound-3}^Tv\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;...$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\;...\\;v\\_{inc+1}^T\\rho\\_{inc}s\\_{inc}s\\_{inc}^Tv\\_{inc+1}\\;...\\;v\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\;...\\;v\\_{inc+1}^Tv\\_{inc}^Tr\\_0$\n\n$=\\rho\\_{inc+bound-1}s\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T\\rho\\_{inc+bound-2}s\\_{inc+bound-2}s\\_{inc+bound-2}^Tv\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\rho\\_{inc+bound-3}s\\_{inc+bound-3}s\\_{inc+bound-3}^Tv\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;...$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\;...\\;v\\_{inc+1}^T\\rho\\_{inc}s\\_{inc}s\\_{inc}^Tv\\_{inc+1}\\;...\\;v\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\;...\\;v\\_{inc+1}^Tv\\_{inc}^TG\\_0v\\_{inc}v\\_{inc+1}...v\\_{inc+bound-1}q\\_{bound}$\n\n$=G\\_{iter}g\\_{iter}$\n\n到此，lbfgs迭代求解证明完毕，其中[1]中实现的lbfgs求解，就是用的该迭代算法。\n\n## OWL-QN算法及求解 \n\n为了减少模型过拟合，我们在进行优化求解时，通常的方式是加入正则项。常见的正则因子包括$l1$正则和$l2$正则。相对于$l2$正则，$l1$正则的优势在于[3]:(1)当大多数特征之间不相关时，$l1$正则在理论和实践上都能够学习更好的模型;(2)$l1$正则能够学到更稀疏的参数空间，有更好的可解释型，在模型计算时能更高效的进行计算。\n\n由于$l1$正则的一阶导数是常数，迭代时使得每个变量尽量被更新为0（$l2$正则是一个比例值，使得每个变量逐渐接近0而不是直接更行为0）。由于$l1$正则在零点不可导，使得基于梯度的优化算法如lbfgs算法无法使用。 针对该问题，Galen Andrew等人提出了OWL-QN(Orthant-Wise Limited-memory Quasi-Newton)算法，用于求解带$l1$正则的log-linear model。\n\n### 相关定义\n\n为方便描述OWL-QN算法，我们做如下一些定义：\n\n$f(x)$对$x\\_i$的右导数：$\\partial\\_i^+=lim\\_\\{\\alpha->0}\\frac{f(x+\\alpha e\\_i)-f(x)}{\\alpha}$\n\n$f(x)$对$x\\_i$的左导数：$\\partial\\_i^-=lim\\_\\{\\alpha->0}\\frac{f(x)-f(x+\\alpha e\\_i)}{\\alpha}$\n\n其中$e\\_i$是第$i$个维度的基向量。\n\n$f(x)$对方向$d$的偏导数：$f′(x;d)=lim\\_\\{\\alpha->0}\\frac{f(x+\\alpha d)-f(x)}{\\alpha}$\n\n符号函数:$\\sigma(x\\_i) =  \\begin{cases} \n1,  & x\\_i>0\\\\\\\\\n-1,  & x\\_i<0\\\\\\\\\n0,  & x\\_i=0\n\\end{cases}\n$\n\n象限投影函数:$\\pi(x\\_i,y\\_i) =  \\begin{cases} \nx\\_i,  & \\sigma(x\\_i) = \\sigma(y\\_i)\\\\\\\\\n0,  & otherwise\n\\end{cases}\n$\n\n### OWL-QN算法\n\n**基于象限建模**\n\n考虑L1正则，要求解的目标函数为：\n\n$F(x)=f(x)+C ||x||\\_{1}\\;(27)$\n\n其中$f(x)$为原始损失，$C ||x||\\_{1}$为正则惩罚。\n\n对于包含$L1$正则目标函数，当数据点集合在某个特定的象限内部（所有维度的符号保持不变），它是可导的。$L1$正则部分是参数的线性函数，且目标函数的二阶导数只取决于原始损失(不包括正则)的部分。基于这点，对于目标函数，可构建包括当前点的某个象限二阶泰勒展开（固定该象限时梯度可以求解，hessian矩阵只根据原始损失部分求解即可），并限制搜索的点，使得迭代后参数对应象限对于当前的近似依然是合法的。\n\n对于向量$\\varepsilon \\in \\lbrace -1, 0 , 1 \\rbrace ^n$, 我们定义其对应象限区域为：\n\n$\\Omega\\_\\varepsilon=\\lbrace x \\in R^n: \\pi(x;\\varepsilon)=x\\rbrace$\n\n对于该象限内的任意点$x$，$F(x)=f(x)+C \\varepsilon^Tx\\;(28)$\n\n我们在式28基础上，扩展定义$F\\_\\varepsilon$为定义在$R^n$上函数，在每个象限具有和$R\\_\\varepsilon$空间类似的导数。通过损失函数的hessian矩阵的逆$H\\_k$，以及$F\\_\\varepsilon$的负梯度在$\\Omega\\_\\varepsilon$的投影$v^k$，可以近似$F\\_\\varepsilon$在$\\Omega\\_\\varepsilon$的投影。为迭代求$F\\_\\varepsilon$最小值，出于技术原因，限制搜索的方向和$v^k$所在象限一致。\n\n$p^k=\\pi(H\\_kv^k;v^k)$\n\n**选择投影象限：**\n\n为了选择投影的象限，我们定义伪梯度：\n\n$\\diamond\\_iF(x)=\\begin{cases} \n\\partial\\_i^{-}F(x),  & if\\;\\partial\\_i^{-}F(x)>0\\\\\\\\\n\\partial\\_i^{+}F(x),  & if\\;\\partial\\_i^{+}F(x)<0\\\\\\\\\n0,  & otherwise\n\\end{cases}\\;(29)\n$\n\n其中，$\\partial\\_i^{+/-}F(x)$定义如下：\n$\\partial\\_i^{+/-}F(x)=\\frac{\\partial}{\\partial x\\_i} f(x)+\\begin{cases}\nC \\sigma(x\\_i) & if\\;x\\_i\\neq 0\\\\\\\\\n+/-C & if\\;x\\_i=0\n\\end{cases}\\;(30)$\n\n由式30可得，$\\partial\\_i^{-}F(x)\\leq \\partial\\_i^{+}F(x)$，因此式29能够精确定义。伪梯度是对梯度信息的泛化，$x$是极小值的充要条件是$\\diamond\\_iF(x)=0$\n\n一个合理的象限选择可以定义如下：\n\n$\\varepsilon\\_i^k=\\begin{cases}\\sigma(x\\_i^k) &if(x\\_i^k\\neq0)\\\\\\\\\n\\sigma(-\\diamond\\_iF(x)) & if (x\\_i^k = 0)\n\\end{cases}\\;(31)$\n\n这样选择象限的理由是：-$\\diamond\\_iF(x)$和$F\\_\\varepsilon$的负梯度在$\\Omega\\_\\varepsilon$的投影$v^k$相等。因此，在利用owl-qn算法求解时，并不需要显示的计算$\\varepsilon\\_i$,直接计算$-\\diamond\\_iF(x)$, 就等价于按照式31设置$\\varepsilon$,并代入式28求解梯度的投影。\n\n**有约束的线性搜索**\n\n为了确保每次迭代没有离开合法的象限空间，owl-qns算法对搜索的点重新投影到象限$\\Omega\\_\\varepsilon$，对于符号发生变化的每个维度，均置为0.如式32所示。\n\n$x\\_{k+1}=\\pi(x^k+\\alpha p^k; \\varepsilon^k)\\;(32)$\n\n有很多的线性搜索方法，[3]采用的方法是：\n\n**算法1:有约束的线性搜索**\n\n(1) $设置\\;\\beta,\\gamma \\in (0,1)$\n\n(2) $for\\;\\;n = 0, 1, 2...$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha=\\beta^n$\n  \n$\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;if\\;\\;f(x^{k+1})\\leq f(x^k)-\\gamma v^T(x^{k+1}-x^k)$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;找到下个最优解 $\n    \n$\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;else\\;\\;continue$\n\n**owl-qn算法**\n\nowl-qn算法同普通的lbfgs算法基本相同，不同之处主要在于：（1）需要计算伪梯度；（2）搜索方向对$v^k$对应的象限做做投影；（3）搜索的点需要限制在上次迭代点对应的象限。（4）目标函数的非正则部分的梯度用于更新$y$向量集合,而不是用伪梯度去更新$y$向量集合。\n\n**算法2:owl-qn算法描述**\n\n$初始化x\\_0,s=\\lbrace\\rbrace,y=\\lbrace\\rbrace$\n\n$for\\;\\; k = 0 \\;to \\;MaxIters$\n\n$\\;\\;\\;\\;计算梯度v^k=-\\diamond f(x^k)$\n\n$\\;\\;\\;\\;通过s,y向量集合,计算d^k=H\\_kv^k$\n\n$\\;\\;\\;\\;p^k=\\pi(d^k;v^k)$\n\n$\\;\\;\\;\\;根据算法1求解x\\_{k+1}$\n\n$\\;\\;\\;\\;如果达到终止条件，则终止算法，否则更新s^k=x\\_{k+1}-x\\_{k},y\\_{k+1}=\\triangledown f(x^{k+1})-\\triangledown f(x^{k}) 向量集合$\n\n## LBFGS在liblbfgs开源库的实现\n\n本章节主要介绍LBFGS算法在liblbfgs开源库[1]的实现，[1]不仅实现了普通的lbfgs算法，也实现了上个章节介绍的owl-qn算法。\n\n**相关数据结构:**\n```c++\n//定义callback_data_t结构\nstruct tag_callback_data {\n    int n;  //变量个数\n    void *instance; //实例\n    lbfgs_evaluate_t proc_evaluate; //计算目标函数及梯度的回调函数\n    lbfgs_progress_t proc_progress; //接受优化过程进度的的回调函数\n};\ntypedef struct tag_callback_data callback_data_t;\n\n//定义iteration_data_t，存储lbfgs迭代需要的s,y向量\nstruct tag_iteration_data {\n    lbfgsfloatval_t alpha;  //算法1迭代需要的alpha变量\n    lbfgsfloatval_t *s;     //x(k+1) - x(k)\n    lbfgsfloatval_t *y;     //g(k+1) - g(k)\n    lbfgsfloatval_t ys;     //vecdot(y, s)\n};\ntypedef struct tag_iteration_data iteration_data_t;\n\n//定义lbfgs参数\nstatic const lbfgs_parameter_t _defparam = {\n    6, 1e-5, 0, 1e-5,\n    0, LBFGS_LINESEARCH_DEFAULT, 40,\n    1e-20, 1e20, 1e-4, 0.9, 0.9, 1.0e-16,\n    0.0, 0, -1,\n};\n\n```\n\n**lbfgs算法:**\n```c++\n//lbfgs算法求解核心过程，为描述lbfgs算法核心流程，此处只保留主要代码\nint lbfgs(\n    int n, //变量个数\n    lbfgsfloatval_t *x, //变量值\n    lbfgsfloatval_t *ptr_fx, // 函数值\n    lbfgs_evaluate_t proc_evaluate, //计算目标函数及梯度的回调函数\n    lbfgs_progress_t proc_progress, //接受优化过程进度的的回调函数\n    void *instance, //实例变量\n    lbfgs_parameter_t *_param //lbfgs优化永的的参数变量\n    )\n{\n    ... \n    //构建callback_data_t\n    callback_data_t cd;\n    cd.n = n; //参数的维度\n    cd.instance = instance; //实例变量\n    cd.proc_evaluate = proc_evaluate; //计算目标函数及梯度的回调函数\n    cd.proc_progress = proc_progress; //接受优化过程进度的的回调函数\n   ...\n    /* Allocate working space. */\n    xp = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));//上次迭代的变量值\n    g = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));//本次迭代对应的梯度值\n    gp = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));//上次迭代的梯度址\n    d = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));//迭代方向变量\n    w = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));\n    //对l1正则，分配OW-LQN算法伪梯度需要的存储空间 */\n    if (param.orthantwise_c != 0.) {\n        pg = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));\n        if (pg == NULL) {\n            ret = LBFGSERR_OUTOFMEMORY;\n            goto lbfgs_exit;\n        }\n    }    \n    //最近m次迭代相关向量的存储\n    lm = (iteration_data_t*)vecalloc(m * sizeof(iteration_data_t));\n    if (lm == NULL) {\n        ret = LBFGSERR_OUTOFMEMORY;\n        goto lbfgs_exit;\n    }    \n    //最近m次迭代相关向量的初始化\n    for (i = 0;i < m;++i) {\n        it = &lm[i];\n        it->alpha = 0;\n        it->ys = 0;\n        it->s = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));\n        it->y = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));\n        if (it->s == NULL || it->y == NULL) {\n            ret = LBFGSERR_OUTOFMEMORY;\n            goto lbfgs_exit;\n        }\n    }\n    //最近的m次迭代的目标函数值\n    if (0 < param.past) {\n        pf = (lbfgsfloatval_t*)vecalloc(param.past * sizeof(lbfgsfloatval_t));\n    }\n    //计算目标函数的值和梯度\n    fx = cd.proc_evaluate(cd.instance, x, g, cd.n, 0);\n    //如果有l1正则，计算带l1正则的目标函数值和伪梯度信息 \n    if (0. != param.orthantwise_c) {\n        //有l1正则，计算l1正则对应的norm\n        xnorm = owlqn_x1norm(x, param.orthantwise_start, param.orthantwise_end);\n        //将l1z正则对应的值加入目标函数\n        fx += xnorm * param.orthantwise_c;\n        //计算伪梯度信息\n        owlqn_pseudo_gradient(\n            pg, x, g, n,\n            param.orthantwise_c, param.orthantwise_start, param.orthantwise_end\n            );\n    }\n    //存储目标函数值到pf[0]\n    if (pf != NULL) {\n        pf[0] = fx;\n    }\n\n    //存储迭代方向到d变量, 假定原始hessian矩阵G0为单位矩阵，G0 g = g\n    if (param.orthantwise_c == 0.) {\n        vecncpy(d, g, n);\n    } else {\n        vecncpy(d, pg, n);\n    }\n    \n    //通过比较g_norm / max(1, x_norm)是否小于param.epsilon，确定是否已经达到极小值\n    vec2norm(&xnorm, x, n);\n    if (param.orthantwise_c == 0.) {\n        vec2norm(&gnorm, g, n);\n    } else {\n        vec2norm(&gnorm, pg, n);\n    }\n    if (xnorm < 1.0) xnorm = 1.0;\n    if (gnorm / xnorm <= param.epsilon) {\n        ret = LBFGS_ALREADY_MINIMIZED;\n        goto lbfgs_exit;\n    }\n    //初始化最优步长 step: 1.0 / sqrt(vecdot(d, d, n)) */\n    vec2norminv(&step, d, n);\n    k = 1;\n    end = 0;\n    for (;;) {\n        veccpy(xp, x, n);//存储变量值到xp\n        veccpy(gp, g, n);//存储梯度值到gp\n        /* Search for an optimal step. */\n        if (param.orthantwise_c == 0.) {//无l1正则，在d方向搜索最优解\n            ls = linesearch(n, x, &fx, g, d, &step, xp, gp, w, &cd, &param);\n        } else { //有l1正则，在d方向搜索最优解\n            ls = linesearch(n, x, &fx, g, d, &step, xp, pg, w, &cd, &param);\n            //计算伪梯度\n            owlqn_pseudo_gradient(\n                pg, x, g, n,\n                param.orthantwise_c, param.orthantwise_start, param.orthantwise_end\n                );\n        }\n        //达到终止条件\n        if (ls < 0) {\n            /* Revert to the previous point. */\n            veccpy(x, xp, n);\n            veccpy(g, gp, n);\n            ret = ls;\n            goto lbfgs_exit;\n        }\n\n        /* Compute x and g norms. */\n        //计算x范数，g范数\n        vec2norm(&xnorm, x, n);\n        if (param.orthantwise_c == 0.) {\n            vec2norm(&gnorm, g, n);\n        } else {\n            vec2norm(&gnorm, pg, n);\n        }\n\n        //输出进度信息\n        if (cd.proc_progress) {\n            if ((ret = cd.proc_progress(cd.instance, x, g, fx, xnorm, gnorm, step, cd.n, k, ls))) {\n                goto lbfgs_exit;\n            }\n        }\n\n        //收敛测试， |g(x)| / \\max(1, |x|) < \\epsil\n        if (xnorm < 1.0) xnorm = 1.0;\n        if (gnorm / xnorm <= param.epsilon) {\n            ret = LBFGS_SUCCESS;\n            break;\n        }\n\n        //以past为周期，根据当前函数值和1个周期之前的函数值判断是否停止迭代\n        //停止条件：(f(past_x) - f(x)) / f(x) < \\delta\n        if (pf != NULL) {\n            /* We don't test the stopping criterion while k < past. */\n            if (param.past <= k) {\n                /* Compute the relative improvement from the past. */\n                rate = (pf[k % param.past] - fx) / fx;\n                /* The stopping criterion. */\n                if (rate < param.delta) {\n                    ret = LBFGS_STOP;\n                    break;\n                }\n            }\n            /* Store the current value of the objective function. */\n            pf[k % param.past] = fx;\n        }\n        //达到最大迭代次数\n        if (param.max_iterations != 0 && param.max_iterations < k+1) {\n            /* Maximum number of iterations. */\n            ret = LBFGSERR_MAXIMUMITERATION;\n            break;\n        }\n\n        //更新向量s, y  s_{k+1} = x_{k+1} - x_{k}，y_{k+1} = g_{k+1} - g_{k}\n        it = &lm[end];\n        vecdiff(it->s, x, xp, n);\n        vecdiff(it->y, g, gp, n);\n\n        vecdot(&ys, it->y, it->s, n); //ys = y^t \\cdot s; 1 / \\rho\n        vecdot(&yy, it->y, it->y, n); //yy = y^t \\cdot y\n        it->ys = ys;// y^t \\cdot s\n\n        /*\n           Recursive formula to compute dir = -(H \\cdot g).\n               This is described in page 779 of:\n               Jorge Nocedal.\n               Updating Quasi-Newton Matrices with Limited Storage.\n               Mathematics of Computation, Vol. 35, No. 151,\n               pp. 773--782, 1980.\n        */\n        //根据文献[1]中算法（对应本文算法1），计算 -(G \\cdot g)\n        bound = (m <= k) ? m : k;\n        ++k;\n        end = (end + 1) % m;\n\n        /* Compute the steepest direction. */\n        if (param.orthantwise_c == 0.) {\n            /* Compute the negative of gradients. */\n            vecncpy(d, g, n);\n        } else {\n            vecncpy(d, pg, n);\n        }\n\n        j = end;\n        for (i = 0;i < bound;++i) {\n            j = (j + m - 1) % m;    /* if (--j == -1) j = m-1; */\n            it = &lm[j];\n            /* \\alpha_{j} = \\rho_{j} s^{t}_{j} \\cdot q_{k+1}. */\n            vecdot(&it->alpha, it->s, d, n);\n            it->alpha /= it->ys;\n            /* q_{i} = q_{i+1} - \\alpha_{i} y_{i}. */\n            vecadd(d, it->y, -it->alpha, n);\n        }\n        vecscale(d, ys / yy, n);\n\n        for (i = 0;i < bound;++i) {\n            it = &lm[j];\n            /* \\beta_{j} = \\rho_{j} y^t_{j} \\cdot \\gamma_{i}. */\n            vecdot(&beta, it->y, d, n);\n            beta /= it->ys;\n            /* \\gamma_{i+1} = \\gamma_{i} + (\\alpha_{j} - \\beta_{j}) s_{j}. */\n            vecadd(d, it->s, it->alpha - beta, n);\n            j = (j + 1) % m;        /* if (++j == m) j = 0; */\n        }        \n\n```\n\n\n# 参考资料\n[1] chokkan, https://github.com/chokkan/liblbfgs\n\n[2] Jorge Nocedal, Updating Quasi-Newton Matrices With Limited Storage\n\n[3] Galen Andrew, Jianfeng Gao, Scalable Training of L1-Regularized Log-Linear Models\n\n[4] 皮果提, http://blog.csdn.net/itplus/article/details/21896453\n\n[5] http://blog.sina.com.cn/s/blog_eb3aea990101gflj.html\n\n[6] https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula\n\n","source":"_posts/lbfgs.md","raw":"---\ntitle: lbfgs算法与源码学习\ndate: 2018-1-13\ntoc: true\ncategories: 模型与算法\ntags: [lbfgs, 拟牛顿算法, 非线性优化]\ndescription: lbfgs算法具备牛顿法收敛速度快的优点，同时又不需要存储和计算完整的hessian矩阵，能够节省大量的存储和计算资源，非常适用于解决无约束的大规模的非线性优化问题。\n\nmathjax: true\n---\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\nLBFGS（limited-memory BFGS或limited-strorate BFGS）算法具备牛顿法收敛速度快的优点，同时又不需要存储和计算完整的hessian矩阵，能够节省大量的存储和计算资源，非常适用于解决无约束的大规模非线性优化问题。\n\n本文从牛顿法出发，先简要介绍牛顿法、拟牛顿法，然后从分别从原理和源码实现的角度介绍lbfgs优化算法。其源码主要来自chokkan等人贡献[1]。\n\n# 牛顿法\n**原始牛顿法:**\n\n目标函数: $min\\;\\;f(x)\\;(1)$\n\n函数$f(x)$在$x=x\\_k$附近进行二阶泰勒展开，如下所示：\n\n$f(x) \\approx f(x\\_k) + \\bigtriangledown f(x\\_k)(x-x\\_k) + \\frac{1}{2}(x-x\\_k)^T \\bigtriangledown^2 f''(x\\_k)(x-x\\_k)\\;(2)$\n\n为了求$f(x)$的极小值,令$f(x)$的导数为0，得到：\n\n$x = x\\_k - H\\_k^{-1}g\\_k\\;(3)$\n\n其中$g\\_k$为函数$f$在$x=x\\_k$的一阶导数， $H\\_k$为函数$f$在$x=x\\_k$的二阶导数(hessian矩阵)。\n因此，为求解下次迭代结果，可直接令:\n\n$x\\_{k+1}=x\\_k-H\\_k^{-1}g\\_k\\;(4)$\n\n算法在利用牛顿法求解时，从$x=x\\_0$出发，逐步迭代直到终止。终止的条件可以是梯度的二范数小于一定值，或者达到最大迭代次数等。\n\n**阻尼牛顿法:**\n\n原始牛顿法是固定步长迭代，对于非二次型目标函数，不能保证目标函数值稳定下降。严重情况下可能造成迭代点序列发散，使得计算失败。为消除该缺点，采用阻尼牛顿法，在更新迭代点时寻求最优步长$ \\lambda\\_k$。\n\n$\\lambda\\_k=argmin\\_{\\lambda}f(x\\_k+\\lambda H\\_k^{-1}g\\_k)\\;(5)$\n\n$x\\_{k+1}=x\\_k+\\lambda H\\_k^{-1}g\\_k\\;(6)$\n\n**牛顿法及阻尼牛顿法优点：**当目标函数$f$为二次函数，且hessian矩阵正定时，通过牛顿法一步就可以得到最优解。当目标函数$f$为非二次函数，但是其二次性较强或迭代点已进入极小点附近，其收敛速度也很快。\n\n**牛顿法及阻尼牛顿法缺点:**要求目标函数$f$需要具有连续的一、二阶导数，且hessian矩阵正定；当特征维度很高时，hessian矩阵存储需要很大空间，求逆计算量也很大，不适合用于大规模问题的优化。\n\n\n# 拟牛顿法\n\n拟牛顿法的核心思想是：直接构造hessian矩阵或hessian矩阵的逆，从而在构造的近似hessian矩阵基础上按照式4或式6进行迭代求解。\n\n## 拟牛顿条件\n\n函数$f(x)$在$x=x\\_{k+1}$附近进行二阶泰勒展开，如下所示：\n\n$f(x) \\approx f(x\\_{k+1}) + \\bigtriangledown f(x\\_{k+1})(x-x\\_{k+1}) + \\frac{1}{2}(x-x\\_{k+1})^T \\bigtriangledown^2 f(x\\_{k+1})(x-x\\_{k+1})\\;(7)$\n\n对上式两边对$x$求导，如下所示:\n\n$\\bigtriangledown f(x) \\approx \\bigtriangledown f(x\\_{k+1}) + \\bigtriangledown^2 f''(x\\_{k+1})(x-x\\_{k+1})\\;(7)$\n\n\n取$x=x\\_k$,则由式7可以得到：\n\n$g\\_{k+1}-g\\_k \\approx  H\\_{k+1}(x_{k+1}-x\\_k)\\;(8)$\n\n其中$g\\_k$为函数$f$在$x=x\\_k$的一阶导数， $H\\_{k+1}$为函数$f$在$x=x\\_{k+1}$的二阶导数(hessian矩阵)。令$y\\_k=g\\_{k+1}-g\\_k$, $s\\_k=x_{k+1}-x\\_k$, $G\\_{k+1}=H^{-1}\\_{k+1}$ 得：\n\n$y\\_k \\approx H\\_{k+1} s\\_k\\;(9)$\n\n$s\\_k \\approx G\\_{k+1}y\\_k\\;(10)$\n\n式9和式10是拟牛顿条件，在迭代过程中对hessian矩阵$H\\_{k+1}$做近似，或者对hessian矩阵的逆$G\\_{k+1}$做近似，而不是直接求解hessian矩阵，就是拟牛顿法。比较常用的拟牛顿法包括DFP算法和BFGS算法。\n\n## DFP算法\n\nDFP算法的核心是通过迭代对hessian矩阵的逆进行近似，迭代公式：\n$G\\_{k+1}=G\\_k + \\bigtriangleup G\\_k, \\; k = 0, 1, 2, ... \\;(11)$\n\n其中$G\\_k$可以通过单位矩阵构造，关键在于如何构造$\\bigtriangleup G\\_k$，其构造过程如下：\n为保证对称性，我们假定:\n\n$\\bigtriangleup G\\_k=\\alpha uu^T + \\beta vv^T\\;(12)$\n\n将式11和式12代入式10，可得：\n\n$s\\_k \\approx G\\_{k+1}y\\_k$\n\n$=>s\\_k = (G\\_k + \\bigtriangleup G\\_k)y\\_k=G\\_ky\\_k+\\alpha u^Ty\\_ku+\\beta v^Ty\\_kv$\n\n$=>s\\_k-G\\_ky\\_k=\\alpha u^Ty\\_ku+\\beta v^Ty\\_kv；（13）$\n\n为使得式12成立，直接使$\\alpha u^Ty\\_k=1$, $\\beta v^Ty\\_k=-1, u=s\\_k, v=G\\_ky\\_k$，得到$\\alpha=\\frac{1}{s^Ty\\_k}$, $\\beta = -\\frac{1}{y^T\\_kG\\_ky\\_k}$, 将$\\alpha,\\beta,u,v$代入式12，得：\n\n$\\bigtriangleup G\\_k=\\frac{s\\_ks^T\\_k}{s^T\\_ky\\_k}-\\frac{G\\_ky\\_ky^T\\_kG\\_k}{y^T\\_kG\\_ky\\_k} \\;(14)$\n\nDFP算法根据式11和式14，迭代求解hessian矩阵的逆$G\\_k$,其他步骤同牛顿法（或阻尼牛顿法）。\n\n## BFGS算法\nBFGS算法核心思想是通过迭代对hessian矩阵进行近似（和DFP算法不同之处在于，DFP算法是对hessian矩阵的逆进行近似）。相对于DFP算法，BFGS算法性能更佳，具有完善的局部收敛理论，在全局收敛性研究也取得重要进展[4]。\n\nBFGS算法和DFP算法推导类似，迭代公式：\n$H\\_{k+1}=H\\_k + \\bigtriangleup H\\_k, \\; k = 0, 1, 2, ... \\;(15)$\n\n其中H\\_0可以用单位矩阵进行构造，对于$\\bigtriangleup H\\_k$的构造如下：\n\n$\\bigtriangleup H\\_k= \\alpha uu^T + \\beta vv^T\\;(16)$\n\n将式15和式16代入式9，得：\n\n$y\\_k \\approx H\\_{k+1}s\\_k $\n\n$=> y\\_k= H\\_ks\\_k+\\alpha u^Ts\\_ku + \\beta v^Ts\\_kv$\n\n$=>y\\_k-H\\_ks\\_k=\\alpha u^Ts\\_ku + \\beta v^Ts\\_kv\\;(17)$\n\n为使式17成立，直接令$u=y\\_k$, $v=H\\_ks\\_k, \\alpha u^Ts\\_k=1, \\beta v^Ts\\_k=-1$,  将$\\alpha,\\beta,u,v$代入式15，得：\n\n$\\bigtriangleup H\\_k=\\frac{y\\_ky\\_k^T}{y\\_k^Ts\\_k}-\\frac{H\\_ks\\_ks\\_k^TH\\_k^T}{s\\_k^TH\\_ks\\_k}\\;(18)$\n\nBFGS算法通过式18更新hessian矩阵的求解过程，在求解搜索方向$d\\_k=H\\_k^{-1}g\\_k$时，通过求解线性方程组$H\\_kd\\_k=g\\_k$得到$d\\_k$的值。\n\n更一般的解法是通过sherman-morrison公式[6],直接得到$H\\_{k+1}^{-1}$和$H\\_k^{-1}$之间的关系如式19所示，并根据该关系迭代求解hessian矩阵的逆:\n$H\\_{k+1}^{-1}=(I-\\frac{s\\_ky_k^T}{y\\_k^Ts\\_k})H\\_k^{-1}(I-\\frac{y\\_ks\\_k^T}{y\\_k^Ts\\_k})+\\frac{s\\_ks\\_k^T}{y\\_k^Ts\\_k}\\;($\n\n$=> G\\_{k+1}=(I-\\frac{s\\_ky_k^T}{y\\_k^Ts\\_k})G\\_k(I-\\frac{y\\_ks\\_k^T}{y\\_k^Ts\\_k})+\\frac{s\\_ks\\_k^T}{y\\_k^Ts\\_k}\\;(19)$\n\n\n# LBFGS算法\n\nBFGS算法需要存储完整的$H\\_k^{-1}$矩阵。因此，当矩阵的维度比较大时，需要占用大量的存储空间(空间复杂度为$O(N^2)$)。LBFGS算法通过使用最近$m$次迭代过程中的$s$和$y$向量，使得其存储复杂度由$O(N^2)$下降到$O(m\\times N)$[2]。\n\n本章节首先介绍lbfgs算法和求解推导、然后介绍带有L1正则的LBFGS算法求解（OWLQN算法）、最后介绍lbfgs算法在liblbfgs库[1]中的实现。\n\n## LBFGS算法及求解\n对于式19，我们令$\\rho\\_k=\\frac{1}{y\\_k^Ts\\_k}$, $v\\_k=(I-\\rho\\_ky\\_ks\\_k^T)$, 得：\n\n$G\\_{k+1}=v\\_k^TG\\_kv\\_k+\\rho\\_ks\\_ks\\_k^T\\;(20)$\n\n假定$G\\_0$是正定矩阵，则：\n\n$G\\_1=v\\_0^TG\\_0v\\_0+\\rho\\_0s\\_0s\\_0^T$\n\n$G\\_2=v\\_1^TG\\_1v\\_1+\\rho\\_1s\\_1s\\_1^T=v\\_1^Tv\\_0^TG\\_0v\\_0v\\_1+v\\_1^T\\rho\\_0s\\_0s\\_0^Tv\\_1+\\rho\\_1s\\_1s\\_1^T$\n\n$G\\_3=v\\_2^Tv\\_1^TG\\_2v\\_1v\\_2+\\rho\\_2s\\_2s\\_2^T=v\\_2^Tv\\_1^Tv\\_0^TG\\_0v\\_0v\\_1v\\_2+v\\_2^Tv\\_1^T\\rho\\_0s\\_0s\\_0^Tv\\_1v\\_2+v\\_2^T\\rho\\_1s\\_1s\\_1^Tv\\_2+\\rho\\_2s\\_2s\\_2^T$\n\n通过递归式20，可得：\n\n$G\\_{k+1}=v\\_k^Tv\\_{k-1}^T...v\\_0^TG\\_0v\\_0...v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^Tv\\_{k-1}^T...v\\_1^T\\rho\\_0 s\\_0 s\\_0^Tv\\_1...v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ ...$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^Tv\\_{k-1}^T\\rho\\_{k-2} s\\_{k-2} s\\_{k-2}v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^T\\rho\\_{k-1} s\\_{k-1} s\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\rho\\_ks\\_ks\\_k^T\\;(21)$\n\n由式21可以得出，$G\\_{k+1}$的计算需要用到$G\\_0$,$s\\_i$, $y\\_i$,其中$i=0,1,2,...k$。而lbfgs算法最关键的点在于，通过使用距离当前迭代最近的$m$个$s$向量和$y$向量，近似求解$G\\_{k+1}$。当$k+1<=m$,则根据式21直接求解$G\\_{k+1}$, 当$k+1>m$时，只保留最近的$k$个$s$向量和$y$向量,具体计算如式22所示:\n\n$G\\_{k+1}=v\\_k^Tv\\_{k-1}^T...v\\_{k-m+1}^TG\\_0v\\_{k-m+1}...v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^Tv\\_{k-1}^T...v\\_{k-m+2}^T\\rho\\_{k-m+1} s\\_{k-m+1} s\\_{k-m+1}^Tv\\_{k-m+2}...v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ ...$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^Tv\\_{k-1}^T\\rho\\_{k-2} s\\_{k-2} s\\_{k-2}v\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v\\_k^T\\rho\\_{k-1} s\\_{k-1} s\\_{k-1}v\\_k$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\rho\\_ks\\_ks\\_k^T\\;(22)$\n\n虽然式21和式22可用于在任何情况下，对hessian矩阵的逆进行迭代求近似解，进而用于lbfgs算法求解。然而，仅仅通过式21和式22，依然需要存储hessian矩阵的逆，并不能节省存储空间。实际上，我们只要能求出$G\\_kg\\_k$(或$H\\_k^{-1}g\\_k$)，就可以避开存储完整的$G\\_{k+1}$,将存储空间由$O(N^2)$下降至$O(m\\times N)$。[2]提供了有效计算$G\\_kg\\_k$的一个迭代算法，如下所示：\n\n**算法1:**\n\n1） $if\\;iter < M: incr = 0, bound= iter$\n\n$\\;\\;\\;\\; else \\; incr= iter - m, bound = m$\n   \n2) $q\\_{bound} = g\\_{iter}$\n\n3) $for \\;i = (bound-1), ... , 0$\n  \n$\\;\\;\\;\\;\\;\\;\\;\\;j = i + incr$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;\\alpha\\_i = \\rho\\_js\\_j^Tq\\_{i+1} (存储每个\\alpha\\_i)$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;q\\_i=q\\_{i+1}-\\alpha\\_iy\\_j$\n\n$\\;\\;\\;\\;r\\_0=G\\_0.q\\_0$\n\n$\\;\\;\\;\\;for\\;i=0, 1, ..., (bound - 1)$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;j=i+incr$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;\\beta\\_i = \\rho\\_jy\\_j^Tr\\_i$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;r\\_{i+1}=r\\_i+s\\_j(\\alpha\\_i-\\beta\\_i)$\n\n\n**算法1的证明：**\n\n$q\\_{bound}=g\\_{iter}$\n\n对于$0<i<bound$,\n\n$q\\_i=q\\_{i+1}-\\alpha\\_iy\\_i \\\\ $\n\n$=q\\_{i+1}-\\rho\\_jy\\_js\\_j^Tq\\_{i+1}$\n\n$=(I-\\rho\\_jy\\_js\\_j^T)q\\_{i+1}$\n\n$=v\\_j^Tq\\_{i+1}$\n\n$=v\\_{inc+i}^Tq\\_{i+1}$\n\n$=v\\_{inc+i}v\\_{inc+i+1}q\\_{i+2}$\n\n$=v\\_{inc+i}v\\_{inc+i+1}v\\_{inc+i+2}...v\\_{inc+bound-1}q\\_{bound}\\;(23)$\n\n\n\n$\\alpha\\_i=\\rho\\_js\\_j^Tq\\_{i+1}$\n\n$=\\rho\\_{inc+i}s\\_{inc+i}^Tv\\_{inc+i+1}v\\_{inc+i+2}...v\\_{inc+bound-1}q\\_{bound}\\;(24)$\n\n$r\\_0=G\\_0q\\_0=G\\_0v\\_{inc}v\\_{inc+1}...v\\_{inc+bound-1}q\\_{bound}(25)$\n\n$r\\_{i+1}=r\\_i+s\\_j(\\alpha\\_i-\\beta\\_i)$\n\n\n$=r\\_i+s\\_j\\alpha\\_j-s\\_j\\rho\\_jy\\_j^Tr\\_i=(I-s\\_j\\rho\\_jy\\_j^T)r\\_i+s\\_j\\alpha\\_i=v\\_{inc+i}^Tr\\_i+s\\_{inc+i}\\alpha\\_i(26)$\n\n由式26可得：\n$r\\_{bound}=s\\_{inc+bound-1}\\alpha\\_{bound-1}+v\\_{inc+bound-1}r\\_{bound-1}$\n\n$=s\\_{inc+bound-1}\\rho\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}+v\\_{inc+bound-1}r\\_{bound-1}$\n\n$=s\\_{inc+bound-1}\\rho\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T(s\\_{inc+bound-2}\\alpha\\_{bound-2}+v\\_{inc+bound-2}^Tr\\_{bound-2})$\n\n$=\\rho\\_{inc+bound-1}s\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T\\rho\\_{inc+bound-2}s\\_{inc+bound-2}s\\_{inc+bound-2}^Tv\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^Tr\\_{round-2}$\n\n$=\\rho\\_{inc+bound-1}s\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T\\rho\\_{inc+bound-2}s\\_{inc+bound-2}s\\_{inc+bound-2}^Tv\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\rho\\_{inc+bound-3}s\\_{inc+bound-3}s\\_{inc+bound-3}^Tv\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^Tv\\_{inc+bound-3}^Tr\\_{bound-3}$\n\n$=\\rho\\_{inc+bound-1}s\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T\\rho\\_{inc+bound-2}s\\_{inc+bound-2}s\\_{inc+bound-2}^Tv\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\rho\\_{inc+bound-3}s\\_{inc+bound-3}s\\_{inc+bound-3}^Tv\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;...$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\;...\\;v\\_{inc+1}^T\\rho\\_{inc}s\\_{inc}s\\_{inc}^Tv\\_{inc+1}\\;...\\;v\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\;...\\;v\\_{inc+1}^Tv\\_{inc}^Tr\\_0$\n\n$=\\rho\\_{inc+bound-1}s\\_{inc+bound-1}s\\_{inc+bound-1}^Tq\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^T\\rho\\_{inc+bound-2}s\\_{inc+bound-2}s\\_{inc+bound-2}^Tv\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\rho\\_{inc+bound-3}s\\_{inc+bound-3}s\\_{inc+bound-3}^Tv\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;...$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\;...\\;v\\_{inc+1}^T\\rho\\_{inc}s\\_{inc}s\\_{inc}^Tv\\_{inc+1}\\;...\\;v\\_{inc+bound-2}v\\_{inc+bound-1}q\\_{bound}$\n\n$\\;\\;\\;+v\\_{inc+bound-1}^Tv\\_{inc+bound-2}^T\\;...\\;v\\_{inc+1}^Tv\\_{inc}^TG\\_0v\\_{inc}v\\_{inc+1}...v\\_{inc+bound-1}q\\_{bound}$\n\n$=G\\_{iter}g\\_{iter}$\n\n到此，lbfgs迭代求解证明完毕，其中[1]中实现的lbfgs求解，就是用的该迭代算法。\n\n## OWL-QN算法及求解 \n\n为了减少模型过拟合，我们在进行优化求解时，通常的方式是加入正则项。常见的正则因子包括$l1$正则和$l2$正则。相对于$l2$正则，$l1$正则的优势在于[3]:(1)当大多数特征之间不相关时，$l1$正则在理论和实践上都能够学习更好的模型;(2)$l1$正则能够学到更稀疏的参数空间，有更好的可解释型，在模型计算时能更高效的进行计算。\n\n由于$l1$正则的一阶导数是常数，迭代时使得每个变量尽量被更新为0（$l2$正则是一个比例值，使得每个变量逐渐接近0而不是直接更行为0）。由于$l1$正则在零点不可导，使得基于梯度的优化算法如lbfgs算法无法使用。 针对该问题，Galen Andrew等人提出了OWL-QN(Orthant-Wise Limited-memory Quasi-Newton)算法，用于求解带$l1$正则的log-linear model。\n\n### 相关定义\n\n为方便描述OWL-QN算法，我们做如下一些定义：\n\n$f(x)$对$x\\_i$的右导数：$\\partial\\_i^+=lim\\_\\{\\alpha->0}\\frac{f(x+\\alpha e\\_i)-f(x)}{\\alpha}$\n\n$f(x)$对$x\\_i$的左导数：$\\partial\\_i^-=lim\\_\\{\\alpha->0}\\frac{f(x)-f(x+\\alpha e\\_i)}{\\alpha}$\n\n其中$e\\_i$是第$i$个维度的基向量。\n\n$f(x)$对方向$d$的偏导数：$f′(x;d)=lim\\_\\{\\alpha->0}\\frac{f(x+\\alpha d)-f(x)}{\\alpha}$\n\n符号函数:$\\sigma(x\\_i) =  \\begin{cases} \n1,  & x\\_i>0\\\\\\\\\n-1,  & x\\_i<0\\\\\\\\\n0,  & x\\_i=0\n\\end{cases}\n$\n\n象限投影函数:$\\pi(x\\_i,y\\_i) =  \\begin{cases} \nx\\_i,  & \\sigma(x\\_i) = \\sigma(y\\_i)\\\\\\\\\n0,  & otherwise\n\\end{cases}\n$\n\n### OWL-QN算法\n\n**基于象限建模**\n\n考虑L1正则，要求解的目标函数为：\n\n$F(x)=f(x)+C ||x||\\_{1}\\;(27)$\n\n其中$f(x)$为原始损失，$C ||x||\\_{1}$为正则惩罚。\n\n对于包含$L1$正则目标函数，当数据点集合在某个特定的象限内部（所有维度的符号保持不变），它是可导的。$L1$正则部分是参数的线性函数，且目标函数的二阶导数只取决于原始损失(不包括正则)的部分。基于这点，对于目标函数，可构建包括当前点的某个象限二阶泰勒展开（固定该象限时梯度可以求解，hessian矩阵只根据原始损失部分求解即可），并限制搜索的点，使得迭代后参数对应象限对于当前的近似依然是合法的。\n\n对于向量$\\varepsilon \\in \\lbrace -1, 0 , 1 \\rbrace ^n$, 我们定义其对应象限区域为：\n\n$\\Omega\\_\\varepsilon=\\lbrace x \\in R^n: \\pi(x;\\varepsilon)=x\\rbrace$\n\n对于该象限内的任意点$x$，$F(x)=f(x)+C \\varepsilon^Tx\\;(28)$\n\n我们在式28基础上，扩展定义$F\\_\\varepsilon$为定义在$R^n$上函数，在每个象限具有和$R\\_\\varepsilon$空间类似的导数。通过损失函数的hessian矩阵的逆$H\\_k$，以及$F\\_\\varepsilon$的负梯度在$\\Omega\\_\\varepsilon$的投影$v^k$，可以近似$F\\_\\varepsilon$在$\\Omega\\_\\varepsilon$的投影。为迭代求$F\\_\\varepsilon$最小值，出于技术原因，限制搜索的方向和$v^k$所在象限一致。\n\n$p^k=\\pi(H\\_kv^k;v^k)$\n\n**选择投影象限：**\n\n为了选择投影的象限，我们定义伪梯度：\n\n$\\diamond\\_iF(x)=\\begin{cases} \n\\partial\\_i^{-}F(x),  & if\\;\\partial\\_i^{-}F(x)>0\\\\\\\\\n\\partial\\_i^{+}F(x),  & if\\;\\partial\\_i^{+}F(x)<0\\\\\\\\\n0,  & otherwise\n\\end{cases}\\;(29)\n$\n\n其中，$\\partial\\_i^{+/-}F(x)$定义如下：\n$\\partial\\_i^{+/-}F(x)=\\frac{\\partial}{\\partial x\\_i} f(x)+\\begin{cases}\nC \\sigma(x\\_i) & if\\;x\\_i\\neq 0\\\\\\\\\n+/-C & if\\;x\\_i=0\n\\end{cases}\\;(30)$\n\n由式30可得，$\\partial\\_i^{-}F(x)\\leq \\partial\\_i^{+}F(x)$，因此式29能够精确定义。伪梯度是对梯度信息的泛化，$x$是极小值的充要条件是$\\diamond\\_iF(x)=0$\n\n一个合理的象限选择可以定义如下：\n\n$\\varepsilon\\_i^k=\\begin{cases}\\sigma(x\\_i^k) &if(x\\_i^k\\neq0)\\\\\\\\\n\\sigma(-\\diamond\\_iF(x)) & if (x\\_i^k = 0)\n\\end{cases}\\;(31)$\n\n这样选择象限的理由是：-$\\diamond\\_iF(x)$和$F\\_\\varepsilon$的负梯度在$\\Omega\\_\\varepsilon$的投影$v^k$相等。因此，在利用owl-qn算法求解时，并不需要显示的计算$\\varepsilon\\_i$,直接计算$-\\diamond\\_iF(x)$, 就等价于按照式31设置$\\varepsilon$,并代入式28求解梯度的投影。\n\n**有约束的线性搜索**\n\n为了确保每次迭代没有离开合法的象限空间，owl-qns算法对搜索的点重新投影到象限$\\Omega\\_\\varepsilon$，对于符号发生变化的每个维度，均置为0.如式32所示。\n\n$x\\_{k+1}=\\pi(x^k+\\alpha p^k; \\varepsilon^k)\\;(32)$\n\n有很多的线性搜索方法，[3]采用的方法是：\n\n**算法1:有约束的线性搜索**\n\n(1) $设置\\;\\beta,\\gamma \\in (0,1)$\n\n(2) $for\\;\\;n = 0, 1, 2...$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha=\\beta^n$\n  \n$\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;if\\;\\;f(x^{k+1})\\leq f(x^k)-\\gamma v^T(x^{k+1}-x^k)$\n\n$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;找到下个最优解 $\n    \n$\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;else\\;\\;continue$\n\n**owl-qn算法**\n\nowl-qn算法同普通的lbfgs算法基本相同，不同之处主要在于：（1）需要计算伪梯度；（2）搜索方向对$v^k$对应的象限做做投影；（3）搜索的点需要限制在上次迭代点对应的象限。（4）目标函数的非正则部分的梯度用于更新$y$向量集合,而不是用伪梯度去更新$y$向量集合。\n\n**算法2:owl-qn算法描述**\n\n$初始化x\\_0,s=\\lbrace\\rbrace,y=\\lbrace\\rbrace$\n\n$for\\;\\; k = 0 \\;to \\;MaxIters$\n\n$\\;\\;\\;\\;计算梯度v^k=-\\diamond f(x^k)$\n\n$\\;\\;\\;\\;通过s,y向量集合,计算d^k=H\\_kv^k$\n\n$\\;\\;\\;\\;p^k=\\pi(d^k;v^k)$\n\n$\\;\\;\\;\\;根据算法1求解x\\_{k+1}$\n\n$\\;\\;\\;\\;如果达到终止条件，则终止算法，否则更新s^k=x\\_{k+1}-x\\_{k},y\\_{k+1}=\\triangledown f(x^{k+1})-\\triangledown f(x^{k}) 向量集合$\n\n## LBFGS在liblbfgs开源库的实现\n\n本章节主要介绍LBFGS算法在liblbfgs开源库[1]的实现，[1]不仅实现了普通的lbfgs算法，也实现了上个章节介绍的owl-qn算法。\n\n**相关数据结构:**\n```c++\n//定义callback_data_t结构\nstruct tag_callback_data {\n    int n;  //变量个数\n    void *instance; //实例\n    lbfgs_evaluate_t proc_evaluate; //计算目标函数及梯度的回调函数\n    lbfgs_progress_t proc_progress; //接受优化过程进度的的回调函数\n};\ntypedef struct tag_callback_data callback_data_t;\n\n//定义iteration_data_t，存储lbfgs迭代需要的s,y向量\nstruct tag_iteration_data {\n    lbfgsfloatval_t alpha;  //算法1迭代需要的alpha变量\n    lbfgsfloatval_t *s;     //x(k+1) - x(k)\n    lbfgsfloatval_t *y;     //g(k+1) - g(k)\n    lbfgsfloatval_t ys;     //vecdot(y, s)\n};\ntypedef struct tag_iteration_data iteration_data_t;\n\n//定义lbfgs参数\nstatic const lbfgs_parameter_t _defparam = {\n    6, 1e-5, 0, 1e-5,\n    0, LBFGS_LINESEARCH_DEFAULT, 40,\n    1e-20, 1e20, 1e-4, 0.9, 0.9, 1.0e-16,\n    0.0, 0, -1,\n};\n\n```\n\n**lbfgs算法:**\n```c++\n//lbfgs算法求解核心过程，为描述lbfgs算法核心流程，此处只保留主要代码\nint lbfgs(\n    int n, //变量个数\n    lbfgsfloatval_t *x, //变量值\n    lbfgsfloatval_t *ptr_fx, // 函数值\n    lbfgs_evaluate_t proc_evaluate, //计算目标函数及梯度的回调函数\n    lbfgs_progress_t proc_progress, //接受优化过程进度的的回调函数\n    void *instance, //实例变量\n    lbfgs_parameter_t *_param //lbfgs优化永的的参数变量\n    )\n{\n    ... \n    //构建callback_data_t\n    callback_data_t cd;\n    cd.n = n; //参数的维度\n    cd.instance = instance; //实例变量\n    cd.proc_evaluate = proc_evaluate; //计算目标函数及梯度的回调函数\n    cd.proc_progress = proc_progress; //接受优化过程进度的的回调函数\n   ...\n    /* Allocate working space. */\n    xp = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));//上次迭代的变量值\n    g = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));//本次迭代对应的梯度值\n    gp = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));//上次迭代的梯度址\n    d = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));//迭代方向变量\n    w = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));\n    //对l1正则，分配OW-LQN算法伪梯度需要的存储空间 */\n    if (param.orthantwise_c != 0.) {\n        pg = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));\n        if (pg == NULL) {\n            ret = LBFGSERR_OUTOFMEMORY;\n            goto lbfgs_exit;\n        }\n    }    \n    //最近m次迭代相关向量的存储\n    lm = (iteration_data_t*)vecalloc(m * sizeof(iteration_data_t));\n    if (lm == NULL) {\n        ret = LBFGSERR_OUTOFMEMORY;\n        goto lbfgs_exit;\n    }    \n    //最近m次迭代相关向量的初始化\n    for (i = 0;i < m;++i) {\n        it = &lm[i];\n        it->alpha = 0;\n        it->ys = 0;\n        it->s = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));\n        it->y = (lbfgsfloatval_t*)vecalloc(n * sizeof(lbfgsfloatval_t));\n        if (it->s == NULL || it->y == NULL) {\n            ret = LBFGSERR_OUTOFMEMORY;\n            goto lbfgs_exit;\n        }\n    }\n    //最近的m次迭代的目标函数值\n    if (0 < param.past) {\n        pf = (lbfgsfloatval_t*)vecalloc(param.past * sizeof(lbfgsfloatval_t));\n    }\n    //计算目标函数的值和梯度\n    fx = cd.proc_evaluate(cd.instance, x, g, cd.n, 0);\n    //如果有l1正则，计算带l1正则的目标函数值和伪梯度信息 \n    if (0. != param.orthantwise_c) {\n        //有l1正则，计算l1正则对应的norm\n        xnorm = owlqn_x1norm(x, param.orthantwise_start, param.orthantwise_end);\n        //将l1z正则对应的值加入目标函数\n        fx += xnorm * param.orthantwise_c;\n        //计算伪梯度信息\n        owlqn_pseudo_gradient(\n            pg, x, g, n,\n            param.orthantwise_c, param.orthantwise_start, param.orthantwise_end\n            );\n    }\n    //存储目标函数值到pf[0]\n    if (pf != NULL) {\n        pf[0] = fx;\n    }\n\n    //存储迭代方向到d变量, 假定原始hessian矩阵G0为单位矩阵，G0 g = g\n    if (param.orthantwise_c == 0.) {\n        vecncpy(d, g, n);\n    } else {\n        vecncpy(d, pg, n);\n    }\n    \n    //通过比较g_norm / max(1, x_norm)是否小于param.epsilon，确定是否已经达到极小值\n    vec2norm(&xnorm, x, n);\n    if (param.orthantwise_c == 0.) {\n        vec2norm(&gnorm, g, n);\n    } else {\n        vec2norm(&gnorm, pg, n);\n    }\n    if (xnorm < 1.0) xnorm = 1.0;\n    if (gnorm / xnorm <= param.epsilon) {\n        ret = LBFGS_ALREADY_MINIMIZED;\n        goto lbfgs_exit;\n    }\n    //初始化最优步长 step: 1.0 / sqrt(vecdot(d, d, n)) */\n    vec2norminv(&step, d, n);\n    k = 1;\n    end = 0;\n    for (;;) {\n        veccpy(xp, x, n);//存储变量值到xp\n        veccpy(gp, g, n);//存储梯度值到gp\n        /* Search for an optimal step. */\n        if (param.orthantwise_c == 0.) {//无l1正则，在d方向搜索最优解\n            ls = linesearch(n, x, &fx, g, d, &step, xp, gp, w, &cd, &param);\n        } else { //有l1正则，在d方向搜索最优解\n            ls = linesearch(n, x, &fx, g, d, &step, xp, pg, w, &cd, &param);\n            //计算伪梯度\n            owlqn_pseudo_gradient(\n                pg, x, g, n,\n                param.orthantwise_c, param.orthantwise_start, param.orthantwise_end\n                );\n        }\n        //达到终止条件\n        if (ls < 0) {\n            /* Revert to the previous point. */\n            veccpy(x, xp, n);\n            veccpy(g, gp, n);\n            ret = ls;\n            goto lbfgs_exit;\n        }\n\n        /* Compute x and g norms. */\n        //计算x范数，g范数\n        vec2norm(&xnorm, x, n);\n        if (param.orthantwise_c == 0.) {\n            vec2norm(&gnorm, g, n);\n        } else {\n            vec2norm(&gnorm, pg, n);\n        }\n\n        //输出进度信息\n        if (cd.proc_progress) {\n            if ((ret = cd.proc_progress(cd.instance, x, g, fx, xnorm, gnorm, step, cd.n, k, ls))) {\n                goto lbfgs_exit;\n            }\n        }\n\n        //收敛测试， |g(x)| / \\max(1, |x|) < \\epsil\n        if (xnorm < 1.0) xnorm = 1.0;\n        if (gnorm / xnorm <= param.epsilon) {\n            ret = LBFGS_SUCCESS;\n            break;\n        }\n\n        //以past为周期，根据当前函数值和1个周期之前的函数值判断是否停止迭代\n        //停止条件：(f(past_x) - f(x)) / f(x) < \\delta\n        if (pf != NULL) {\n            /* We don't test the stopping criterion while k < past. */\n            if (param.past <= k) {\n                /* Compute the relative improvement from the past. */\n                rate = (pf[k % param.past] - fx) / fx;\n                /* The stopping criterion. */\n                if (rate < param.delta) {\n                    ret = LBFGS_STOP;\n                    break;\n                }\n            }\n            /* Store the current value of the objective function. */\n            pf[k % param.past] = fx;\n        }\n        //达到最大迭代次数\n        if (param.max_iterations != 0 && param.max_iterations < k+1) {\n            /* Maximum number of iterations. */\n            ret = LBFGSERR_MAXIMUMITERATION;\n            break;\n        }\n\n        //更新向量s, y  s_{k+1} = x_{k+1} - x_{k}，y_{k+1} = g_{k+1} - g_{k}\n        it = &lm[end];\n        vecdiff(it->s, x, xp, n);\n        vecdiff(it->y, g, gp, n);\n\n        vecdot(&ys, it->y, it->s, n); //ys = y^t \\cdot s; 1 / \\rho\n        vecdot(&yy, it->y, it->y, n); //yy = y^t \\cdot y\n        it->ys = ys;// y^t \\cdot s\n\n        /*\n           Recursive formula to compute dir = -(H \\cdot g).\n               This is described in page 779 of:\n               Jorge Nocedal.\n               Updating Quasi-Newton Matrices with Limited Storage.\n               Mathematics of Computation, Vol. 35, No. 151,\n               pp. 773--782, 1980.\n        */\n        //根据文献[1]中算法（对应本文算法1），计算 -(G \\cdot g)\n        bound = (m <= k) ? m : k;\n        ++k;\n        end = (end + 1) % m;\n\n        /* Compute the steepest direction. */\n        if (param.orthantwise_c == 0.) {\n            /* Compute the negative of gradients. */\n            vecncpy(d, g, n);\n        } else {\n            vecncpy(d, pg, n);\n        }\n\n        j = end;\n        for (i = 0;i < bound;++i) {\n            j = (j + m - 1) % m;    /* if (--j == -1) j = m-1; */\n            it = &lm[j];\n            /* \\alpha_{j} = \\rho_{j} s^{t}_{j} \\cdot q_{k+1}. */\n            vecdot(&it->alpha, it->s, d, n);\n            it->alpha /= it->ys;\n            /* q_{i} = q_{i+1} - \\alpha_{i} y_{i}. */\n            vecadd(d, it->y, -it->alpha, n);\n        }\n        vecscale(d, ys / yy, n);\n\n        for (i = 0;i < bound;++i) {\n            it = &lm[j];\n            /* \\beta_{j} = \\rho_{j} y^t_{j} \\cdot \\gamma_{i}. */\n            vecdot(&beta, it->y, d, n);\n            beta /= it->ys;\n            /* \\gamma_{i+1} = \\gamma_{i} + (\\alpha_{j} - \\beta_{j}) s_{j}. */\n            vecadd(d, it->s, it->alpha - beta, n);\n            j = (j + 1) % m;        /* if (++j == m) j = 0; */\n        }        \n\n```\n\n\n# 参考资料\n[1] chokkan, https://github.com/chokkan/liblbfgs\n\n[2] Jorge Nocedal, Updating Quasi-Newton Matrices With Limited Storage\n\n[3] Galen Andrew, Jianfeng Gao, Scalable Training of L1-Regularized Log-Linear Models\n\n[4] 皮果提, http://blog.csdn.net/itplus/article/details/21896453\n\n[5] http://blog.sina.com.cn/s/blog_eb3aea990101gflj.html\n\n[6] https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula\n\n","slug":"lbfgs","published":1,"updated":"2018-02-11T08:33:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjdikgudc0009ga01iqpxes74","content":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<p>LBFGS（limited-memory BFGS或limited-strorate BFGS）算法具备牛顿法收敛速度快的优点，同时又不需要存储和计算完整的hessian矩阵，能够节省大量的存储和计算资源，非常适用于解决无约束的大规模非线性优化问题。</p>\n<p>本文从牛顿法出发，先简要介绍牛顿法、拟牛顿法，然后从分别从原理和源码实现的角度介绍lbfgs优化算法。其源码主要来自chokkan等人贡献[1]。</p>\n<h1 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a>牛顿法</h1><p><strong>原始牛顿法:</strong></p>\n<p>目标函数: $min\\;\\;f(x)\\;(1)$</p>\n<p>函数$f(x)$在$x=x_k$附近进行二阶泰勒展开，如下所示：</p>\n<p>$f(x) \\approx f(x_k) + \\bigtriangledown f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\bigtriangledown^2 f’’(x_k)(x-x_k)\\;(2)$</p>\n<p>为了求$f(x)$的极小值,令$f(x)$的导数为0，得到：</p>\n<p>$x = x_k - H_k^{-1}g_k\\;(3)$</p>\n<p>其中$g_k$为函数$f$在$x=x_k$的一阶导数， $H_k$为函数$f$在$x=x_k$的二阶导数(hessian矩阵)。<br>因此，为求解下次迭代结果，可直接令:</p>\n<p>$x_{k+1}=x_k-H_k^{-1}g_k\\;(4)$</p>\n<p>算法在利用牛顿法求解时，从$x=x_0$出发，逐步迭代直到终止。终止的条件可以是梯度的二范数小于一定值，或者达到最大迭代次数等。</p>\n<p><strong>阻尼牛顿法:</strong></p>\n<p>原始牛顿法是固定步长迭代，对于非二次型目标函数，不能保证目标函数值稳定下降。严重情况下可能造成迭代点序列发散，使得计算失败。为消除该缺点，采用阻尼牛顿法，在更新迭代点时寻求最优步长$ \\lambda_k$。</p>\n<p>$\\lambda_k=argmin_{\\lambda}f(x_k+\\lambda H_k^{-1}g_k)\\;(5)$</p>\n<p>$x_{k+1}=x_k+\\lambda H_k^{-1}g_k\\;(6)$</p>\n<p><strong>牛顿法及阻尼牛顿法优点：</strong>当目标函数$f$为二次函数，且hessian矩阵正定时，通过牛顿法一步就可以得到最优解。当目标函数$f$为非二次函数，但是其二次性较强或迭代点已进入极小点附近，其收敛速度也很快。</p>\n<p><strong>牛顿法及阻尼牛顿法缺点:</strong>要求目标函数$f$需要具有连续的一、二阶导数，且hessian矩阵正定；当特征维度很高时，hessian矩阵存储需要很大空间，求逆计算量也很大，不适合用于大规模问题的优化。</p>\n<h1 id=\"拟牛顿法\"><a href=\"#拟牛顿法\" class=\"headerlink\" title=\"拟牛顿法\"></a>拟牛顿法</h1><p>拟牛顿法的核心思想是：直接构造hessian矩阵或hessian矩阵的逆，从而在构造的近似hessian矩阵基础上按照式4或式6进行迭代求解。</p>\n<h2 id=\"拟牛顿条件\"><a href=\"#拟牛顿条件\" class=\"headerlink\" title=\"拟牛顿条件\"></a>拟牛顿条件</h2><p>函数$f(x)$在$x=x_{k+1}$附近进行二阶泰勒展开，如下所示：</p>\n<p>$f(x) \\approx f(x_{k+1}) + \\bigtriangledown f(x_{k+1})(x-x_{k+1}) + \\frac{1}{2}(x-x_{k+1})^T \\bigtriangledown^2 f(x_{k+1})(x-x_{k+1})\\;(7)$</p>\n<p>对上式两边对$x$求导，如下所示:</p>\n<p>$\\bigtriangledown f(x) \\approx \\bigtriangledown f(x_{k+1}) + \\bigtriangledown^2 f’’(x_{k+1})(x-x_{k+1})\\;(7)$</p>\n<p>取$x=x_k$,则由式7可以得到：</p>\n<p>$g_{k+1}-g_k \\approx  H_{k+1}(x_{k+1}-x_k)\\;(8)$</p>\n<p>其中$g_k$为函数$f$在$x=x_k$的一阶导数， $H_{k+1}$为函数$f$在$x=x_{k+1}$的二阶导数(hessian矩阵)。令$y_k=g_{k+1}-g_k$, $s_k=x_{k+1}-x_k$, $G_{k+1}=H^{-1}_{k+1}$ 得：</p>\n<p>$y_k \\approx H_{k+1} s_k\\;(9)$</p>\n<p>$s_k \\approx G_{k+1}y_k\\;(10)$</p>\n<p>式9和式10是拟牛顿条件，在迭代过程中对hessian矩阵$H_{k+1}$做近似，或者对hessian矩阵的逆$G_{k+1}$做近似，而不是直接求解hessian矩阵，就是拟牛顿法。比较常用的拟牛顿法包括DFP算法和BFGS算法。</p>\n<h2 id=\"DFP算法\"><a href=\"#DFP算法\" class=\"headerlink\" title=\"DFP算法\"></a>DFP算法</h2><p>DFP算法的核心是通过迭代对hessian矩阵的逆进行近似，迭代公式：<br>$G_{k+1}=G_k + \\bigtriangleup G_k, \\; k = 0, 1, 2, … \\;(11)$</p>\n<p>其中$G_k$可以通过单位矩阵构造，关键在于如何构造$\\bigtriangleup G_k$，其构造过程如下：<br>为保证对称性，我们假定:</p>\n<p>$\\bigtriangleup G_k=\\alpha uu^T + \\beta vv^T\\;(12)$</p>\n<p>将式11和式12代入式10，可得：</p>\n<p>$s_k \\approx G_{k+1}y_k$</p>\n<p>$=&gt;s_k = (G_k + \\bigtriangleup G_k)y_k=G_ky_k+\\alpha u^Ty_ku+\\beta v^Ty_kv$</p>\n<p>$=&gt;s_k-G_ky_k=\\alpha u^Ty_ku+\\beta v^Ty_kv；（13）$</p>\n<p>为使得式12成立，直接使$\\alpha u^Ty_k=1$, $\\beta v^Ty_k=-1, u=s_k, v=G_ky_k$，得到$\\alpha=\\frac{1}{s^Ty_k}$, $\\beta = -\\frac{1}{y^T_kG_ky_k}$, 将$\\alpha,\\beta,u,v$代入式12，得：</p>\n<p>$\\bigtriangleup G_k=\\frac{s_ks^T_k}{s^T_ky_k}-\\frac{G_ky_ky^T_kG_k}{y^T_kG_ky_k} \\;(14)$</p>\n<p>DFP算法根据式11和式14，迭代求解hessian矩阵的逆$G_k$,其他步骤同牛顿法（或阻尼牛顿法）。</p>\n<h2 id=\"BFGS算法\"><a href=\"#BFGS算法\" class=\"headerlink\" title=\"BFGS算法\"></a>BFGS算法</h2><p>BFGS算法核心思想是通过迭代对hessian矩阵进行近似（和DFP算法不同之处在于，DFP算法是对hessian矩阵的逆进行近似）。相对于DFP算法，BFGS算法性能更佳，具有完善的局部收敛理论，在全局收敛性研究也取得重要进展[4]。</p>\n<p>BFGS算法和DFP算法推导类似，迭代公式：<br>$H_{k+1}=H_k + \\bigtriangleup H_k, \\; k = 0, 1, 2, … \\;(15)$</p>\n<p>其中H_0可以用单位矩阵进行构造，对于$\\bigtriangleup H_k$的构造如下：</p>\n<p>$\\bigtriangleup H_k= \\alpha uu^T + \\beta vv^T\\;(16)$</p>\n<p>将式15和式16代入式9，得：</p>\n<p>$y_k \\approx H_{k+1}s_k $</p>\n<p>$=&gt; y_k= H_ks_k+\\alpha u^Ts_ku + \\beta v^Ts_kv$</p>\n<p>$=&gt;y_k-H_ks_k=\\alpha u^Ts_ku + \\beta v^Ts_kv\\;(17)$</p>\n<p>为使式17成立，直接令$u=y_k$, $v=H_ks_k, \\alpha u^Ts_k=1, \\beta v^Ts_k=-1$,  将$\\alpha,\\beta,u,v$代入式15，得：</p>\n<p>$\\bigtriangleup H_k=\\frac{y_ky_k^T}{y_k^Ts_k}-\\frac{H_ks_ks_k^TH_k^T}{s_k^TH_ks_k}\\;(18)$</p>\n<p>BFGS算法通过式18更新hessian矩阵的求解过程，在求解搜索方向$d_k=H_k^{-1}g_k$时，通过求解线性方程组$H_kd_k=g_k$得到$d_k$的值。</p>\n<p>更一般的解法是通过sherman-morrison公式[6],直接得到$H_{k+1}^{-1}$和$H_k^{-1}$之间的关系如式19所示，并根据该关系迭代求解hessian矩阵的逆:<br>$H_{k+1}^{-1}=(I-\\frac{s_ky_k^T}{y_k^Ts_k})H_k^{-1}(I-\\frac{y_ks_k^T}{y_k^Ts_k})+\\frac{s_ks_k^T}{y_k^Ts_k}\\;($</p>\n<p>$=&gt; G_{k+1}=(I-\\frac{s_ky_k^T}{y_k^Ts_k})G_k(I-\\frac{y_ks_k^T}{y_k^Ts_k})+\\frac{s_ks_k^T}{y_k^Ts_k}\\;(19)$</p>\n<h1 id=\"LBFGS算法\"><a href=\"#LBFGS算法\" class=\"headerlink\" title=\"LBFGS算法\"></a>LBFGS算法</h1><p>BFGS算法需要存储完整的$H_k^{-1}$矩阵。因此，当矩阵的维度比较大时，需要占用大量的存储空间(空间复杂度为$O(N^2)$)。LBFGS算法通过使用最近$m$次迭代过程中的$s$和$y$向量，使得其存储复杂度由$O(N^2)$下降到$O(m\\times N)$[2]。</p>\n<p>本章节首先介绍lbfgs算法和求解推导、然后介绍带有L1正则的LBFGS算法求解（OWLQN算法）、最后介绍lbfgs算法在liblbfgs库[1]中的实现。</p>\n<h2 id=\"LBFGS算法及求解\"><a href=\"#LBFGS算法及求解\" class=\"headerlink\" title=\"LBFGS算法及求解\"></a>LBFGS算法及求解</h2><p>对于式19，我们令$\\rho_k=\\frac{1}{y_k^Ts_k}$, $v_k=(I-\\rho_ky_ks_k^T)$, 得：</p>\n<p>$G_{k+1}=v_k^TG_kv_k+\\rho_ks_ks_k^T\\;(20)$</p>\n<p>假定$G_0$是正定矩阵，则：</p>\n<p>$G_1=v_0^TG_0v_0+\\rho_0s_0s_0^T$</p>\n<p>$G_2=v_1^TG_1v_1+\\rho_1s_1s_1^T=v_1^Tv_0^TG_0v_0v_1+v_1^T\\rho_0s_0s_0^Tv_1+\\rho_1s_1s_1^T$</p>\n<p>$G_3=v_2^Tv_1^TG_2v_1v_2+\\rho_2s_2s_2^T=v_2^Tv_1^Tv_0^TG_0v_0v_1v_2+v_2^Tv_1^T\\rho_0s_0s_0^Tv_1v_2+v_2^T\\rho_1s_1s_1^Tv_2+\\rho_2s_2s_2^T$</p>\n<p>通过递归式20，可得：</p>\n<p>$G_{k+1}=v_k^Tv_{k-1}^T…v_0^TG_0v_0…v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^Tv_{k-1}^T…v_1^T\\rho_0 s_0 s_0^Tv_1…v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ …$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^Tv_{k-1}^T\\rho_{k-2} s_{k-2} s_{k-2}v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^T\\rho_{k-1} s_{k-1} s_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\rho_ks_ks_k^T\\;(21)$</p>\n<p>由式21可以得出，$G_{k+1}$的计算需要用到$G_0$,$s_i$, $y_i$,其中$i=0,1,2,…k$。而lbfgs算法最关键的点在于，通过使用距离当前迭代最近的$m$个$s$向量和$y$向量，近似求解$G_{k+1}$。当$k+1&lt;=m$,则根据式21直接求解$G_{k+1}$, 当$k+1&gt;m$时，只保留最近的$k$个$s$向量和$y$向量,具体计算如式22所示:</p>\n<p>$G_{k+1}=v_k^Tv_{k-1}^T…v_{k-m+1}^TG_0v_{k-m+1}…v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^Tv_{k-1}^T…v_{k-m+2}^T\\rho_{k-m+1} s_{k-m+1} s_{k-m+1}^Tv_{k-m+2}…v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ …$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^Tv_{k-1}^T\\rho_{k-2} s_{k-2} s_{k-2}v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^T\\rho_{k-1} s_{k-1} s_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\rho_ks_ks_k^T\\;(22)$</p>\n<p>虽然式21和式22可用于在任何情况下，对hessian矩阵的逆进行迭代求近似解，进而用于lbfgs算法求解。然而，仅仅通过式21和式22，依然需要存储hessian矩阵的逆，并不能节省存储空间。实际上，我们只要能求出$G_kg_k$(或$H_k^{-1}g_k$)，就可以避开存储完整的$G_{k+1}$,将存储空间由$O(N^2)$下降至$O(m\\times N)$。[2]提供了有效计算$G_kg_k$的一个迭代算法，如下所示：</p>\n<p><strong>算法1:</strong></p>\n<p>1） $if\\;iter &lt; M: incr = 0, bound= iter$</p>\n<p>$\\;\\;\\;\\; else \\; incr= iter - m, bound = m$</p>\n<p>2) $q_{bound} = g_{iter}$</p>\n<p>3) $for \\;i = (bound-1), … , 0$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;j = i + incr$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;\\alpha_i = \\rho_js_j^Tq_{i+1} (存储每个\\alpha_i)$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;q_i=q_{i+1}-\\alpha_iy_j$</p>\n<p>$\\;\\;\\;\\;r_0=G_0.q_0$</p>\n<p>$\\;\\;\\;\\;for\\;i=0, 1, …, (bound - 1)$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;j=i+incr$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;\\beta_i = \\rho_jy_j^Tr_i$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;r_{i+1}=r_i+s_j(\\alpha_i-\\beta_i)$</p>\n<p><strong>算法1的证明：</strong></p>\n<p>$q_{bound}=g_{iter}$</p>\n<p>对于$0&lt;i&lt;bound$,</p>\n<p>$q_i=q_{i+1}-\\alpha_iy_i \\ $</p>\n<p>$=q_{i+1}-\\rho_jy_js_j^Tq_{i+1}$</p>\n<p>$=(I-\\rho_jy_js_j^T)q_{i+1}$</p>\n<p>$=v_j^Tq_{i+1}$</p>\n<p>$=v_{inc+i}^Tq_{i+1}$</p>\n<p>$=v_{inc+i}v_{inc+i+1}q_{i+2}$</p>\n<p>$=v_{inc+i}v_{inc+i+1}v_{inc+i+2}…v_{inc+bound-1}q_{bound}\\;(23)$</p>\n<p>$\\alpha_i=\\rho_js_j^Tq_{i+1}$</p>\n<p>$=\\rho_{inc+i}s_{inc+i}^Tv_{inc+i+1}v_{inc+i+2}…v_{inc+bound-1}q_{bound}\\;(24)$</p>\n<p>$r_0=G_0q_0=G_0v_{inc}v_{inc+1}…v_{inc+bound-1}q_{bound}(25)$</p>\n<p>$r_{i+1}=r_i+s_j(\\alpha_i-\\beta_i)$</p>\n<p>$=r_i+s_j\\alpha_j-s_j\\rho_jy_j^Tr_i=(I-s_j\\rho_jy_j^T)r_i+s_j\\alpha_i=v_{inc+i}^Tr_i+s_{inc+i}\\alpha_i(26)$</p>\n<p>由式26可得：<br>$r_{bound}=s_{inc+bound-1}\\alpha_{bound-1}+v_{inc+bound-1}r_{bound-1}$</p>\n<p>$=s_{inc+bound-1}\\rho_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}+v_{inc+bound-1}r_{bound-1}$</p>\n<p>$=s_{inc+bound-1}\\rho_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T(s_{inc+bound-2}\\alpha_{bound-2}+v_{inc+bound-2}^Tr_{bound-2})$</p>\n<p>$=\\rho_{inc+bound-1}s_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T\\rho_{inc+bound-2}s_{inc+bound-2}s_{inc+bound-2}^Tv_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^Tr_{round-2}$</p>\n<p>$=\\rho_{inc+bound-1}s_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T\\rho_{inc+bound-2}s_{inc+bound-2}s_{inc+bound-2}^Tv_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\rho_{inc+bound-3}s_{inc+bound-3}s_{inc+bound-3}^Tv_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^Tv_{inc+bound-3}^Tr_{bound-3}$</p>\n<p>$=\\rho_{inc+bound-1}s_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T\\rho_{inc+bound-2}s_{inc+bound-2}s_{inc+bound-2}^Tv_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\rho_{inc+bound-3}s_{inc+bound-3}s_{inc+bound-3}^Tv_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;…$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\;…\\;v_{inc+1}^T\\rho_{inc}s_{inc}s_{inc}^Tv_{inc+1}\\;…\\;v_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\;…\\;v_{inc+1}^Tv_{inc}^Tr_0$</p>\n<p>$=\\rho_{inc+bound-1}s_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T\\rho_{inc+bound-2}s_{inc+bound-2}s_{inc+bound-2}^Tv_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\rho_{inc+bound-3}s_{inc+bound-3}s_{inc+bound-3}^Tv_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;…$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\;…\\;v_{inc+1}^T\\rho_{inc}s_{inc}s_{inc}^Tv_{inc+1}\\;…\\;v_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\;…\\;v_{inc+1}^Tv_{inc}^TG_0v_{inc}v_{inc+1}…v_{inc+bound-1}q_{bound}$</p>\n<p>$=G_{iter}g_{iter}$</p>\n<p>到此，lbfgs迭代求解证明完毕，其中[1]中实现的lbfgs求解，就是用的该迭代算法。</p>\n<h2 id=\"OWL-QN算法及求解\"><a href=\"#OWL-QN算法及求解\" class=\"headerlink\" title=\"OWL-QN算法及求解\"></a>OWL-QN算法及求解</h2><p>为了减少模型过拟合，我们在进行优化求解时，通常的方式是加入正则项。常见的正则因子包括$l1$正则和$l2$正则。相对于$l2$正则，$l1$正则的优势在于[3]:(1)当大多数特征之间不相关时，$l1$正则在理论和实践上都能够学习更好的模型;(2)$l1$正则能够学到更稀疏的参数空间，有更好的可解释型，在模型计算时能更高效的进行计算。</p>\n<p>由于$l1$正则的一阶导数是常数，迭代时使得每个变量尽量被更新为0（$l2$正则是一个比例值，使得每个变量逐渐接近0而不是直接更行为0）。由于$l1$正则在零点不可导，使得基于梯度的优化算法如lbfgs算法无法使用。 针对该问题，Galen Andrew等人提出了OWL-QN(Orthant-Wise Limited-memory Quasi-Newton)算法，用于求解带$l1$正则的log-linear model。</p>\n<h3 id=\"相关定义\"><a href=\"#相关定义\" class=\"headerlink\" title=\"相关定义\"></a>相关定义</h3><p>为方便描述OWL-QN算法，我们做如下一些定义：</p>\n<p>$f(x)$对$x_i$的右导数：$\\partial_i^+=lim_{\\alpha-&gt;0}\\frac{f(x+\\alpha e_i)-f(x)}{\\alpha}$</p>\n<p>$f(x)$对$x_i$的左导数：$\\partial_i^-=lim_{\\alpha-&gt;0}\\frac{f(x)-f(x+\\alpha e_i)}{\\alpha}$</p>\n<p>其中$e_i$是第$i$个维度的基向量。</p>\n<p>$f(x)$对方向$d$的偏导数：$f′(x;d)=lim_{\\alpha-&gt;0}\\frac{f(x+\\alpha d)-f(x)}{\\alpha}$</p>\n<p>符号函数:$\\sigma(x_i) =  \\begin{cases}<br>1,  &amp; x_i&gt;0\\\\<br>-1,  &amp; x_i&lt;0\\\\<br>0,  &amp; x_i=0<br>\\end{cases}<br>$</p>\n<p>象限投影函数:$\\pi(x_i,y_i) =  \\begin{cases}<br>x_i,  &amp; \\sigma(x_i) = \\sigma(y_i)\\\\<br>0,  &amp; otherwise<br>\\end{cases}<br>$</p>\n<h3 id=\"OWL-QN算法\"><a href=\"#OWL-QN算法\" class=\"headerlink\" title=\"OWL-QN算法\"></a>OWL-QN算法</h3><p><strong>基于象限建模</strong></p>\n<p>考虑L1正则，要求解的目标函数为：</p>\n<p>$F(x)=f(x)+C ||x||_{1}\\;(27)$</p>\n<p>其中$f(x)$为原始损失，$C ||x||_{1}$为正则惩罚。</p>\n<p>对于包含$L1$正则目标函数，当数据点集合在某个特定的象限内部（所有维度的符号保持不变），它是可导的。$L1$正则部分是参数的线性函数，且目标函数的二阶导数只取决于原始损失(不包括正则)的部分。基于这点，对于目标函数，可构建包括当前点的某个象限二阶泰勒展开（固定该象限时梯度可以求解，hessian矩阵只根据原始损失部分求解即可），并限制搜索的点，使得迭代后参数对应象限对于当前的近似依然是合法的。</p>\n<p>对于向量$\\varepsilon \\in \\lbrace -1, 0 , 1 \\rbrace ^n$, 我们定义其对应象限区域为：</p>\n<p>$\\Omega_\\varepsilon=\\lbrace x \\in R^n: \\pi(x;\\varepsilon)=x\\rbrace$</p>\n<p>对于该象限内的任意点$x$，$F(x)=f(x)+C \\varepsilon^Tx\\;(28)$</p>\n<p>我们在式28基础上，扩展定义$F_\\varepsilon$为定义在$R^n$上函数，在每个象限具有和$R_\\varepsilon$空间类似的导数。通过损失函数的hessian矩阵的逆$H_k$，以及$F_\\varepsilon$的负梯度在$\\Omega_\\varepsilon$的投影$v^k$，可以近似$F_\\varepsilon$在$\\Omega_\\varepsilon$的投影。为迭代求$F_\\varepsilon$最小值，出于技术原因，限制搜索的方向和$v^k$所在象限一致。</p>\n<p>$p^k=\\pi(H_kv^k;v^k)$</p>\n<p><strong>选择投影象限：</strong></p>\n<p>为了选择投影的象限，我们定义伪梯度：</p>\n<p>$\\diamond_iF(x)=\\begin{cases}<br>\\partial_i^{-}F(x),  &amp; if\\;\\partial_i^{-}F(x)&gt;0\\\\<br>\\partial_i^{+}F(x),  &amp; if\\;\\partial_i^{+}F(x)&lt;0\\\\<br>0,  &amp; otherwise<br>\\end{cases}\\;(29)<br>$</p>\n<p>其中，$\\partial_i^{+/-}F(x)$定义如下：<br>$\\partial_i^{+/-}F(x)=\\frac{\\partial}{\\partial x_i} f(x)+\\begin{cases}<br>C \\sigma(x_i) &amp; if\\;x_i\\neq 0\\\\<br>+/-C &amp; if\\;x_i=0<br>\\end{cases}\\;(30)$</p>\n<p>由式30可得，$\\partial_i^{-}F(x)\\leq \\partial_i^{+}F(x)$，因此式29能够精确定义。伪梯度是对梯度信息的泛化，$x$是极小值的充要条件是$\\diamond_iF(x)=0$</p>\n<p>一个合理的象限选择可以定义如下：</p>\n<p>$\\varepsilon_i^k=\\begin{cases}\\sigma(x_i^k) &amp;if(x_i^k\\neq0)\\\\<br>\\sigma(-\\diamond_iF(x)) &amp; if (x_i^k = 0)<br>\\end{cases}\\;(31)$</p>\n<p>这样选择象限的理由是：-$\\diamond_iF(x)$和$F_\\varepsilon$的负梯度在$\\Omega_\\varepsilon$的投影$v^k$相等。因此，在利用owl-qn算法求解时，并不需要显示的计算$\\varepsilon_i$,直接计算$-\\diamond_iF(x)$, 就等价于按照式31设置$\\varepsilon$,并代入式28求解梯度的投影。</p>\n<p><strong>有约束的线性搜索</strong></p>\n<p>为了确保每次迭代没有离开合法的象限空间，owl-qns算法对搜索的点重新投影到象限$\\Omega_\\varepsilon$，对于符号发生变化的每个维度，均置为0.如式32所示。</p>\n<p>$x_{k+1}=\\pi(x^k+\\alpha p^k; \\varepsilon^k)\\;(32)$</p>\n<p>有很多的线性搜索方法，[3]采用的方法是：</p>\n<p><strong>算法1:有约束的线性搜索</strong></p>\n<p>(1) $设置\\;\\beta,\\gamma \\in (0,1)$</p>\n<p>(2) $for\\;\\;n = 0, 1, 2…$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha=\\beta^n$</p>\n<p>$\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;if\\;\\;f(x^{k+1})\\leq f(x^k)-\\gamma v^T(x^{k+1}-x^k)$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;找到下个最优解 $</p>\n<p>$\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;else\\;\\;continue$</p>\n<p><strong>owl-qn算法</strong></p>\n<p>owl-qn算法同普通的lbfgs算法基本相同，不同之处主要在于：（1）需要计算伪梯度；（2）搜索方向对$v^k$对应的象限做做投影；（3）搜索的点需要限制在上次迭代点对应的象限。（4）目标函数的非正则部分的梯度用于更新$y$向量集合,而不是用伪梯度去更新$y$向量集合。</p>\n<p><strong>算法2:owl-qn算法描述</strong></p>\n<p>$初始化x_0,s=\\lbrace\\rbrace,y=\\lbrace\\rbrace$</p>\n<p>$for\\;\\; k = 0 \\;to \\;MaxIters$</p>\n<p>$\\;\\;\\;\\;计算梯度v^k=-\\diamond f(x^k)$</p>\n<p>$\\;\\;\\;\\;通过s,y向量集合,计算d^k=H_kv^k$</p>\n<p>$\\;\\;\\;\\;p^k=\\pi(d^k;v^k)$</p>\n<p>$\\;\\;\\;\\;根据算法1求解x_{k+1}$</p>\n<p>$\\;\\;\\;\\;如果达到终止条件，则终止算法，否则更新s^k=x_{k+1}-x_{k},y_{k+1}=\\triangledown f(x^{k+1})-\\triangledown f(x^{k}) 向量集合$</p>\n<h2 id=\"LBFGS在liblbfgs开源库的实现\"><a href=\"#LBFGS在liblbfgs开源库的实现\" class=\"headerlink\" title=\"LBFGS在liblbfgs开源库的实现\"></a>LBFGS在liblbfgs开源库的实现</h2><p>本章节主要介绍LBFGS算法在liblbfgs开源库[1]的实现，[1]不仅实现了普通的lbfgs算法，也实现了上个章节介绍的owl-qn算法。</p>\n<p><strong>相关数据结构:</strong><br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//定义callback_data_t结构</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">tag_callback_data</span> &#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> n;  <span class=\"comment\">//变量个数</span></span><br><span class=\"line\">    <span class=\"keyword\">void</span> *instance; <span class=\"comment\">//实例</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgs_evaluate_t</span> proc_evaluate; <span class=\"comment\">//计算目标函数及梯度的回调函数</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgs_progress_t</span> proc_progress; <span class=\"comment\">//接受优化过程进度的的回调函数</span></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">tag_callback_data</span> <span class=\"title\">callback_data_t</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//定义iteration_data_t，存储lbfgs迭代需要的s,y向量</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">tag_iteration_data</span> &#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgsfloatval_t</span> alpha;  <span class=\"comment\">//算法1迭代需要的alpha变量</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgsfloatval_t</span> *s;     <span class=\"comment\">//x(k+1) - x(k)</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgsfloatval_t</span> *y;     <span class=\"comment\">//g(k+1) - g(k)</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgsfloatval_t</span> ys;     <span class=\"comment\">//vecdot(y, s)</span></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">tag_iteration_data</span> <span class=\"title\">iteration_data_t</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//定义lbfgs参数</span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">lbfgs_parameter_t</span> _defparam = &#123;</span><br><span class=\"line\">    <span class=\"number\">6</span>, <span class=\"number\">1e-5</span>, <span class=\"number\">0</span>, <span class=\"number\">1e-5</span>,</span><br><span class=\"line\">    <span class=\"number\">0</span>, LBFGS_LINESEARCH_DEFAULT, <span class=\"number\">40</span>,</span><br><span class=\"line\">    <span class=\"number\">1e-20</span>, <span class=\"number\">1e20</span>, <span class=\"number\">1e-4</span>, <span class=\"number\">0.9</span>, <span class=\"number\">0.9</span>, <span class=\"number\">1.0e-16</span>,</span><br><span class=\"line\">    <span class=\"number\">0.0</span>, <span class=\"number\">0</span>, <span class=\"number\">-1</span>,</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p><strong>lbfgs算法:</strong><br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//lbfgs算法求解核心过程，为描述lbfgs算法核心流程，此处只保留主要代码</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">lbfgs</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">int</span> n, <span class=\"comment\">//变量个数</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgsfloatval_t</span> *x, <span class=\"comment\">//变量值</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgsfloatval_t</span> *ptr_fx, <span class=\"comment\">// 函数值</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgs_evaluate_t</span> proc_evaluate, <span class=\"comment\">//计算目标函数及梯度的回调函数</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgs_progress_t</span> proc_progress, <span class=\"comment\">//接受优化过程进度的的回调函数</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">void</span> *instance, <span class=\"comment\">//实例变量</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgs_parameter_t</span> *_param <span class=\"comment\">//lbfgs优化永的的参数变量</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    ... </span><br><span class=\"line\">    <span class=\"comment\">//构建callback_data_t</span></span><br><span class=\"line\">    <span class=\"keyword\">callback_data_t</span> cd;</span><br><span class=\"line\">    cd.n = n; <span class=\"comment\">//参数的维度</span></span><br><span class=\"line\">    cd.instance = instance; <span class=\"comment\">//实例变量</span></span><br><span class=\"line\">    cd.proc_evaluate = proc_evaluate; <span class=\"comment\">//计算目标函数及梯度的回调函数</span></span><br><span class=\"line\">    cd.proc_progress = proc_progress; <span class=\"comment\">//接受优化过程进度的的回调函数</span></span><br><span class=\"line\">   ...</span><br><span class=\"line\">    <span class=\"comment\">/* Allocate working space. */</span></span><br><span class=\"line\">    xp = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));<span class=\"comment\">//上次迭代的变量值</span></span><br><span class=\"line\">    g = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));<span class=\"comment\">//本次迭代对应的梯度值</span></span><br><span class=\"line\">    gp = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));<span class=\"comment\">//上次迭代的梯度址</span></span><br><span class=\"line\">    d = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));<span class=\"comment\">//迭代方向变量</span></span><br><span class=\"line\">    w = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">    <span class=\"comment\">//对l1正则，分配OW-LQN算法伪梯度需要的存储空间 */</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (param.orthantwise_c != <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">        pg = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (pg == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">            ret = LBFGSERR_OUTOFMEMORY;</span><br><span class=\"line\">            <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;    </span><br><span class=\"line\">    <span class=\"comment\">//最近m次迭代相关向量的存储</span></span><br><span class=\"line\">    lm = (<span class=\"keyword\">iteration_data_t</span>*)vecalloc(m * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">iteration_data_t</span>));</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (lm == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">        ret = LBFGSERR_OUTOFMEMORY;</span><br><span class=\"line\">        <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">    &#125;    </span><br><span class=\"line\">    <span class=\"comment\">//最近m次迭代相关向量的初始化</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>;i &lt; m;++i) &#123;</span><br><span class=\"line\">        it = &amp;lm[i];</span><br><span class=\"line\">        it-&gt;alpha = <span class=\"number\">0</span>;</span><br><span class=\"line\">        it-&gt;ys = <span class=\"number\">0</span>;</span><br><span class=\"line\">        it-&gt;s = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">        it-&gt;y = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (it-&gt;s == <span class=\"literal\">NULL</span> || it-&gt;y == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">            ret = LBFGSERR_OUTOFMEMORY;</span><br><span class=\"line\">            <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//最近的m次迭代的目标函数值</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"number\">0</span> &lt; param.past) &#123;</span><br><span class=\"line\">        pf = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(param.past * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//计算目标函数的值和梯度</span></span><br><span class=\"line\">    fx = cd.proc_evaluate(cd.instance, x, g, cd.n, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"comment\">//如果有l1正则，计算带l1正则的目标函数值和伪梯度信息 </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"number\">0.</span> != param.orthantwise_c) &#123;</span><br><span class=\"line\">        <span class=\"comment\">//有l1正则，计算l1正则对应的norm</span></span><br><span class=\"line\">        xnorm = owlqn_x1norm(x, param.orthantwise_start, param.orthantwise_end);</span><br><span class=\"line\">        <span class=\"comment\">//将l1z正则对应的值加入目标函数</span></span><br><span class=\"line\">        fx += xnorm * param.orthantwise_c;</span><br><span class=\"line\">        <span class=\"comment\">//计算伪梯度信息</span></span><br><span class=\"line\">        owlqn_pseudo_gradient(</span><br><span class=\"line\">            pg, x, g, n,</span><br><span class=\"line\">            param.orthantwise_c, param.orthantwise_start, param.orthantwise_end</span><br><span class=\"line\">            );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//存储目标函数值到pf[0]</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (pf != <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">        pf[<span class=\"number\">0</span>] = fx;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//存储迭代方向到d变量, 假定原始hessian矩阵G0为单位矩阵，G0 g = g</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">        vecncpy(d, g, n);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        vecncpy(d, pg, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//通过比较g_norm / max(1, x_norm)是否小于param.epsilon，确定是否已经达到极小值</span></span><br><span class=\"line\">    vec2norm(&amp;xnorm, x, n);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">        vec2norm(&amp;gnorm, g, n);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        vec2norm(&amp;gnorm, pg, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (xnorm &lt; <span class=\"number\">1.0</span>) xnorm = <span class=\"number\">1.0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (gnorm / xnorm &lt;= param.epsilon) &#123;</span><br><span class=\"line\">        ret = LBFGS_ALREADY_MINIMIZED;</span><br><span class=\"line\">        <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//初始化最优步长 step: 1.0 / sqrt(vecdot(d, d, n)) */</span></span><br><span class=\"line\">    vec2norminv(&amp;step, d, n);</span><br><span class=\"line\">    k = <span class=\"number\">1</span>;</span><br><span class=\"line\">    end = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        veccpy(xp, x, n);<span class=\"comment\">//存储变量值到xp</span></span><br><span class=\"line\">        veccpy(gp, g, n);<span class=\"comment\">//存储梯度值到gp</span></span><br><span class=\"line\">        <span class=\"comment\">/* Search for an optimal step. */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;<span class=\"comment\">//无l1正则，在d方向搜索最优解</span></span><br><span class=\"line\">            ls = linesearch(n, x, &amp;fx, g, d, &amp;step, xp, gp, w, &amp;cd, &amp;param);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">//有l1正则，在d方向搜索最优解</span></span><br><span class=\"line\">            ls = linesearch(n, x, &amp;fx, g, d, &amp;step, xp, pg, w, &amp;cd, &amp;param);</span><br><span class=\"line\">            <span class=\"comment\">//计算伪梯度</span></span><br><span class=\"line\">            owlqn_pseudo_gradient(</span><br><span class=\"line\">                pg, x, g, n,</span><br><span class=\"line\">                param.orthantwise_c, param.orthantwise_start, param.orthantwise_end</span><br><span class=\"line\">                );</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//达到终止条件</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ls &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* Revert to the previous point. */</span></span><br><span class=\"line\">            veccpy(x, xp, n);</span><br><span class=\"line\">            veccpy(g, gp, n);</span><br><span class=\"line\">            ret = ls;</span><br><span class=\"line\">            <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* Compute x and g norms. */</span></span><br><span class=\"line\">        <span class=\"comment\">//计算x范数，g范数</span></span><br><span class=\"line\">        vec2norm(&amp;xnorm, x, n);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">            vec2norm(&amp;gnorm, g, n);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            vec2norm(&amp;gnorm, pg, n);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//输出进度信息</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (cd.proc_progress) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> ((ret = cd.proc_progress(cd.instance, x, g, fx, xnorm, gnorm, step, cd.n, k, ls))) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//收敛测试， |g(x)| / \\max(1, |x|) &lt; \\epsil</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (xnorm &lt; <span class=\"number\">1.0</span>) xnorm = <span class=\"number\">1.0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (gnorm / xnorm &lt;= param.epsilon) &#123;</span><br><span class=\"line\">            ret = LBFGS_SUCCESS;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//以past为周期，根据当前函数值和1个周期之前的函数值判断是否停止迭代</span></span><br><span class=\"line\">        <span class=\"comment\">//停止条件：(f(past_x) - f(x)) / f(x) &lt; \\delta</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (pf != <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* We don't test the stopping criterion while k &lt; past. */</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (param.past &lt;= k) &#123;</span><br><span class=\"line\">                <span class=\"comment\">/* Compute the relative improvement from the past. */</span></span><br><span class=\"line\">                rate = (pf[k % param.past] - fx) / fx;</span><br><span class=\"line\">                <span class=\"comment\">/* The stopping criterion. */</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (rate &lt; param.delta) &#123;</span><br><span class=\"line\">                    ret = LBFGS_STOP;</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"comment\">/* Store the current value of the objective function. */</span></span><br><span class=\"line\">            pf[k % param.past] = fx;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//达到最大迭代次数</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (param.max_iterations != <span class=\"number\">0</span> &amp;&amp; param.max_iterations &lt; k+<span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* Maximum number of iterations. */</span></span><br><span class=\"line\">            ret = LBFGSERR_MAXIMUMITERATION;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//更新向量s, y  s_&#123;k+1&#125; = x_&#123;k+1&#125; - x_&#123;k&#125;，y_&#123;k+1&#125; = g_&#123;k+1&#125; - g_&#123;k&#125;</span></span><br><span class=\"line\">        it = &amp;lm[end];</span><br><span class=\"line\">        vecdiff(it-&gt;s, x, xp, n);</span><br><span class=\"line\">        vecdiff(it-&gt;y, g, gp, n);</span><br><span class=\"line\"></span><br><span class=\"line\">        vecdot(&amp;ys, it-&gt;y, it-&gt;s, n); <span class=\"comment\">//ys = y^t \\cdot s; 1 / \\rho</span></span><br><span class=\"line\">        vecdot(&amp;yy, it-&gt;y, it-&gt;y, n); <span class=\"comment\">//yy = y^t \\cdot y</span></span><br><span class=\"line\">        it-&gt;ys = ys;<span class=\"comment\">// y^t \\cdot s</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">           Recursive formula to compute dir = -(H \\cdot g).</span></span><br><span class=\"line\"><span class=\"comment\">               This is described in page 779 of:</span></span><br><span class=\"line\"><span class=\"comment\">               Jorge Nocedal.</span></span><br><span class=\"line\"><span class=\"comment\">               Updating Quasi-Newton Matrices with Limited Storage.</span></span><br><span class=\"line\"><span class=\"comment\">               Mathematics of Computation, Vol. 35, No. 151,</span></span><br><span class=\"line\"><span class=\"comment\">               pp. 773--782, 1980.</span></span><br><span class=\"line\"><span class=\"comment\">        */</span></span><br><span class=\"line\">        <span class=\"comment\">//根据文献[1]中算法（对应本文算法1），计算 -(G \\cdot g)</span></span><br><span class=\"line\">        bound = (m &lt;= k) ? m : k;</span><br><span class=\"line\">        ++k;</span><br><span class=\"line\">        end = (end + <span class=\"number\">1</span>) % m;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* Compute the steepest direction. */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* Compute the negative of gradients. */</span></span><br><span class=\"line\">            vecncpy(d, g, n);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            vecncpy(d, pg, n);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        j = end;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>;i &lt; bound;++i) &#123;</span><br><span class=\"line\">            j = (j + m - <span class=\"number\">1</span>) % m;    <span class=\"comment\">/* if (--j == -1) j = m-1; */</span></span><br><span class=\"line\">            it = &amp;lm[j];</span><br><span class=\"line\">            <span class=\"comment\">/* \\alpha_&#123;j&#125; = \\rho_&#123;j&#125; s^&#123;t&#125;_&#123;j&#125; \\cdot q_&#123;k+1&#125;. */</span></span><br><span class=\"line\">            vecdot(&amp;it-&gt;alpha, it-&gt;s, d, n);</span><br><span class=\"line\">            it-&gt;alpha /= it-&gt;ys;</span><br><span class=\"line\">            <span class=\"comment\">/* q_&#123;i&#125; = q_&#123;i+1&#125; - \\alpha_&#123;i&#125; y_&#123;i&#125;. */</span></span><br><span class=\"line\">            vecadd(d, it-&gt;y, -it-&gt;alpha, n);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        vecscale(d, ys / yy, n);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>;i &lt; bound;++i) &#123;</span><br><span class=\"line\">            it = &amp;lm[j];</span><br><span class=\"line\">            <span class=\"comment\">/* \\beta_&#123;j&#125; = \\rho_&#123;j&#125; y^t_&#123;j&#125; \\cdot \\gamma_&#123;i&#125;. */</span></span><br><span class=\"line\">            vecdot(&amp;beta, it-&gt;y, d, n);</span><br><span class=\"line\">            beta /= it-&gt;ys;</span><br><span class=\"line\">            <span class=\"comment\">/* \\gamma_&#123;i+1&#125; = \\gamma_&#123;i&#125; + (\\alpha_&#123;j&#125; - \\beta_&#123;j&#125;) s_&#123;j&#125;. */</span></span><br><span class=\"line\">            vecadd(d, it-&gt;s, it-&gt;alpha - beta, n);</span><br><span class=\"line\">            j = (j + <span class=\"number\">1</span>) % m;        <span class=\"comment\">/* if (++j == m) j = 0; */</span></span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>[1] chokkan, <a href=\"https://github.com/chokkan/liblbfgs\" target=\"_blank\" rel=\"noopener\">https://github.com/chokkan/liblbfgs</a></p>\n<p>[2] Jorge Nocedal, Updating Quasi-Newton Matrices With Limited Storage</p>\n<p>[3] Galen Andrew, Jianfeng Gao, Scalable Training of L1-Regularized Log-Linear Models</p>\n<p>[4] 皮果提, <a href=\"http://blog.csdn.net/itplus/article/details/21896453\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/itplus/article/details/21896453</a></p>\n<p>[5] <a href=\"http://blog.sina.com.cn/s/blog_eb3aea990101gflj.html\" target=\"_blank\" rel=\"noopener\">http://blog.sina.com.cn/s/blog_eb3aea990101gflj.html</a></p>\n<p>[6] <a href=\"https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula</a></p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<p>LBFGS（limited-memory BFGS或limited-strorate BFGS）算法具备牛顿法收敛速度快的优点，同时又不需要存储和计算完整的hessian矩阵，能够节省大量的存储和计算资源，非常适用于解决无约束的大规模非线性优化问题。</p>\n<p>本文从牛顿法出发，先简要介绍牛顿法、拟牛顿法，然后从分别从原理和源码实现的角度介绍lbfgs优化算法。其源码主要来自chokkan等人贡献[1]。</p>\n<h1 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a>牛顿法</h1><p><strong>原始牛顿法:</strong></p>\n<p>目标函数: $min\\;\\;f(x)\\;(1)$</p>\n<p>函数$f(x)$在$x=x_k$附近进行二阶泰勒展开，如下所示：</p>\n<p>$f(x) \\approx f(x_k) + \\bigtriangledown f(x_k)(x-x_k) + \\frac{1}{2}(x-x_k)^T \\bigtriangledown^2 f’’(x_k)(x-x_k)\\;(2)$</p>\n<p>为了求$f(x)$的极小值,令$f(x)$的导数为0，得到：</p>\n<p>$x = x_k - H_k^{-1}g_k\\;(3)$</p>\n<p>其中$g_k$为函数$f$在$x=x_k$的一阶导数， $H_k$为函数$f$在$x=x_k$的二阶导数(hessian矩阵)。<br>因此，为求解下次迭代结果，可直接令:</p>\n<p>$x_{k+1}=x_k-H_k^{-1}g_k\\;(4)$</p>\n<p>算法在利用牛顿法求解时，从$x=x_0$出发，逐步迭代直到终止。终止的条件可以是梯度的二范数小于一定值，或者达到最大迭代次数等。</p>\n<p><strong>阻尼牛顿法:</strong></p>\n<p>原始牛顿法是固定步长迭代，对于非二次型目标函数，不能保证目标函数值稳定下降。严重情况下可能造成迭代点序列发散，使得计算失败。为消除该缺点，采用阻尼牛顿法，在更新迭代点时寻求最优步长$ \\lambda_k$。</p>\n<p>$\\lambda_k=argmin_{\\lambda}f(x_k+\\lambda H_k^{-1}g_k)\\;(5)$</p>\n<p>$x_{k+1}=x_k+\\lambda H_k^{-1}g_k\\;(6)$</p>\n<p><strong>牛顿法及阻尼牛顿法优点：</strong>当目标函数$f$为二次函数，且hessian矩阵正定时，通过牛顿法一步就可以得到最优解。当目标函数$f$为非二次函数，但是其二次性较强或迭代点已进入极小点附近，其收敛速度也很快。</p>\n<p><strong>牛顿法及阻尼牛顿法缺点:</strong>要求目标函数$f$需要具有连续的一、二阶导数，且hessian矩阵正定；当特征维度很高时，hessian矩阵存储需要很大空间，求逆计算量也很大，不适合用于大规模问题的优化。</p>\n<h1 id=\"拟牛顿法\"><a href=\"#拟牛顿法\" class=\"headerlink\" title=\"拟牛顿法\"></a>拟牛顿法</h1><p>拟牛顿法的核心思想是：直接构造hessian矩阵或hessian矩阵的逆，从而在构造的近似hessian矩阵基础上按照式4或式6进行迭代求解。</p>\n<h2 id=\"拟牛顿条件\"><a href=\"#拟牛顿条件\" class=\"headerlink\" title=\"拟牛顿条件\"></a>拟牛顿条件</h2><p>函数$f(x)$在$x=x_{k+1}$附近进行二阶泰勒展开，如下所示：</p>\n<p>$f(x) \\approx f(x_{k+1}) + \\bigtriangledown f(x_{k+1})(x-x_{k+1}) + \\frac{1}{2}(x-x_{k+1})^T \\bigtriangledown^2 f(x_{k+1})(x-x_{k+1})\\;(7)$</p>\n<p>对上式两边对$x$求导，如下所示:</p>\n<p>$\\bigtriangledown f(x) \\approx \\bigtriangledown f(x_{k+1}) + \\bigtriangledown^2 f’’(x_{k+1})(x-x_{k+1})\\;(7)$</p>\n<p>取$x=x_k$,则由式7可以得到：</p>\n<p>$g_{k+1}-g_k \\approx  H_{k+1}(x_{k+1}-x_k)\\;(8)$</p>\n<p>其中$g_k$为函数$f$在$x=x_k$的一阶导数， $H_{k+1}$为函数$f$在$x=x_{k+1}$的二阶导数(hessian矩阵)。令$y_k=g_{k+1}-g_k$, $s_k=x_{k+1}-x_k$, $G_{k+1}=H^{-1}_{k+1}$ 得：</p>\n<p>$y_k \\approx H_{k+1} s_k\\;(9)$</p>\n<p>$s_k \\approx G_{k+1}y_k\\;(10)$</p>\n<p>式9和式10是拟牛顿条件，在迭代过程中对hessian矩阵$H_{k+1}$做近似，或者对hessian矩阵的逆$G_{k+1}$做近似，而不是直接求解hessian矩阵，就是拟牛顿法。比较常用的拟牛顿法包括DFP算法和BFGS算法。</p>\n<h2 id=\"DFP算法\"><a href=\"#DFP算法\" class=\"headerlink\" title=\"DFP算法\"></a>DFP算法</h2><p>DFP算法的核心是通过迭代对hessian矩阵的逆进行近似，迭代公式：<br>$G_{k+1}=G_k + \\bigtriangleup G_k, \\; k = 0, 1, 2, … \\;(11)$</p>\n<p>其中$G_k$可以通过单位矩阵构造，关键在于如何构造$\\bigtriangleup G_k$，其构造过程如下：<br>为保证对称性，我们假定:</p>\n<p>$\\bigtriangleup G_k=\\alpha uu^T + \\beta vv^T\\;(12)$</p>\n<p>将式11和式12代入式10，可得：</p>\n<p>$s_k \\approx G_{k+1}y_k$</p>\n<p>$=&gt;s_k = (G_k + \\bigtriangleup G_k)y_k=G_ky_k+\\alpha u^Ty_ku+\\beta v^Ty_kv$</p>\n<p>$=&gt;s_k-G_ky_k=\\alpha u^Ty_ku+\\beta v^Ty_kv；（13）$</p>\n<p>为使得式12成立，直接使$\\alpha u^Ty_k=1$, $\\beta v^Ty_k=-1, u=s_k, v=G_ky_k$，得到$\\alpha=\\frac{1}{s^Ty_k}$, $\\beta = -\\frac{1}{y^T_kG_ky_k}$, 将$\\alpha,\\beta,u,v$代入式12，得：</p>\n<p>$\\bigtriangleup G_k=\\frac{s_ks^T_k}{s^T_ky_k}-\\frac{G_ky_ky^T_kG_k}{y^T_kG_ky_k} \\;(14)$</p>\n<p>DFP算法根据式11和式14，迭代求解hessian矩阵的逆$G_k$,其他步骤同牛顿法（或阻尼牛顿法）。</p>\n<h2 id=\"BFGS算法\"><a href=\"#BFGS算法\" class=\"headerlink\" title=\"BFGS算法\"></a>BFGS算法</h2><p>BFGS算法核心思想是通过迭代对hessian矩阵进行近似（和DFP算法不同之处在于，DFP算法是对hessian矩阵的逆进行近似）。相对于DFP算法，BFGS算法性能更佳，具有完善的局部收敛理论，在全局收敛性研究也取得重要进展[4]。</p>\n<p>BFGS算法和DFP算法推导类似，迭代公式：<br>$H_{k+1}=H_k + \\bigtriangleup H_k, \\; k = 0, 1, 2, … \\;(15)$</p>\n<p>其中H_0可以用单位矩阵进行构造，对于$\\bigtriangleup H_k$的构造如下：</p>\n<p>$\\bigtriangleup H_k= \\alpha uu^T + \\beta vv^T\\;(16)$</p>\n<p>将式15和式16代入式9，得：</p>\n<p>$y_k \\approx H_{k+1}s_k $</p>\n<p>$=&gt; y_k= H_ks_k+\\alpha u^Ts_ku + \\beta v^Ts_kv$</p>\n<p>$=&gt;y_k-H_ks_k=\\alpha u^Ts_ku + \\beta v^Ts_kv\\;(17)$</p>\n<p>为使式17成立，直接令$u=y_k$, $v=H_ks_k, \\alpha u^Ts_k=1, \\beta v^Ts_k=-1$,  将$\\alpha,\\beta,u,v$代入式15，得：</p>\n<p>$\\bigtriangleup H_k=\\frac{y_ky_k^T}{y_k^Ts_k}-\\frac{H_ks_ks_k^TH_k^T}{s_k^TH_ks_k}\\;(18)$</p>\n<p>BFGS算法通过式18更新hessian矩阵的求解过程，在求解搜索方向$d_k=H_k^{-1}g_k$时，通过求解线性方程组$H_kd_k=g_k$得到$d_k$的值。</p>\n<p>更一般的解法是通过sherman-morrison公式[6],直接得到$H_{k+1}^{-1}$和$H_k^{-1}$之间的关系如式19所示，并根据该关系迭代求解hessian矩阵的逆:<br>$H_{k+1}^{-1}=(I-\\frac{s_ky_k^T}{y_k^Ts_k})H_k^{-1}(I-\\frac{y_ks_k^T}{y_k^Ts_k})+\\frac{s_ks_k^T}{y_k^Ts_k}\\;($</p>\n<p>$=&gt; G_{k+1}=(I-\\frac{s_ky_k^T}{y_k^Ts_k})G_k(I-\\frac{y_ks_k^T}{y_k^Ts_k})+\\frac{s_ks_k^T}{y_k^Ts_k}\\;(19)$</p>\n<h1 id=\"LBFGS算法\"><a href=\"#LBFGS算法\" class=\"headerlink\" title=\"LBFGS算法\"></a>LBFGS算法</h1><p>BFGS算法需要存储完整的$H_k^{-1}$矩阵。因此，当矩阵的维度比较大时，需要占用大量的存储空间(空间复杂度为$O(N^2)$)。LBFGS算法通过使用最近$m$次迭代过程中的$s$和$y$向量，使得其存储复杂度由$O(N^2)$下降到$O(m\\times N)$[2]。</p>\n<p>本章节首先介绍lbfgs算法和求解推导、然后介绍带有L1正则的LBFGS算法求解（OWLQN算法）、最后介绍lbfgs算法在liblbfgs库[1]中的实现。</p>\n<h2 id=\"LBFGS算法及求解\"><a href=\"#LBFGS算法及求解\" class=\"headerlink\" title=\"LBFGS算法及求解\"></a>LBFGS算法及求解</h2><p>对于式19，我们令$\\rho_k=\\frac{1}{y_k^Ts_k}$, $v_k=(I-\\rho_ky_ks_k^T)$, 得：</p>\n<p>$G_{k+1}=v_k^TG_kv_k+\\rho_ks_ks_k^T\\;(20)$</p>\n<p>假定$G_0$是正定矩阵，则：</p>\n<p>$G_1=v_0^TG_0v_0+\\rho_0s_0s_0^T$</p>\n<p>$G_2=v_1^TG_1v_1+\\rho_1s_1s_1^T=v_1^Tv_0^TG_0v_0v_1+v_1^T\\rho_0s_0s_0^Tv_1+\\rho_1s_1s_1^T$</p>\n<p>$G_3=v_2^Tv_1^TG_2v_1v_2+\\rho_2s_2s_2^T=v_2^Tv_1^Tv_0^TG_0v_0v_1v_2+v_2^Tv_1^T\\rho_0s_0s_0^Tv_1v_2+v_2^T\\rho_1s_1s_1^Tv_2+\\rho_2s_2s_2^T$</p>\n<p>通过递归式20，可得：</p>\n<p>$G_{k+1}=v_k^Tv_{k-1}^T…v_0^TG_0v_0…v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^Tv_{k-1}^T…v_1^T\\rho_0 s_0 s_0^Tv_1…v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ …$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^Tv_{k-1}^T\\rho_{k-2} s_{k-2} s_{k-2}v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^T\\rho_{k-1} s_{k-1} s_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\rho_ks_ks_k^T\\;(21)$</p>\n<p>由式21可以得出，$G_{k+1}$的计算需要用到$G_0$,$s_i$, $y_i$,其中$i=0,1,2,…k$。而lbfgs算法最关键的点在于，通过使用距离当前迭代最近的$m$个$s$向量和$y$向量，近似求解$G_{k+1}$。当$k+1&lt;=m$,则根据式21直接求解$G_{k+1}$, 当$k+1&gt;m$时，只保留最近的$k$个$s$向量和$y$向量,具体计算如式22所示:</p>\n<p>$G_{k+1}=v_k^Tv_{k-1}^T…v_{k-m+1}^TG_0v_{k-m+1}…v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^Tv_{k-1}^T…v_{k-m+2}^T\\rho_{k-m+1} s_{k-m+1} s_{k-m+1}^Tv_{k-m+2}…v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ …$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^Tv_{k-1}^T\\rho_{k-2} s_{k-2} s_{k-2}v_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + v_k^T\\rho_{k-1} s_{k-1} s_{k-1}v_k$</p>\n<p>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\rho_ks_ks_k^T\\;(22)$</p>\n<p>虽然式21和式22可用于在任何情况下，对hessian矩阵的逆进行迭代求近似解，进而用于lbfgs算法求解。然而，仅仅通过式21和式22，依然需要存储hessian矩阵的逆，并不能节省存储空间。实际上，我们只要能求出$G_kg_k$(或$H_k^{-1}g_k$)，就可以避开存储完整的$G_{k+1}$,将存储空间由$O(N^2)$下降至$O(m\\times N)$。[2]提供了有效计算$G_kg_k$的一个迭代算法，如下所示：</p>\n<p><strong>算法1:</strong></p>\n<p>1） $if\\;iter &lt; M: incr = 0, bound= iter$</p>\n<p>$\\;\\;\\;\\; else \\; incr= iter - m, bound = m$</p>\n<p>2) $q_{bound} = g_{iter}$</p>\n<p>3) $for \\;i = (bound-1), … , 0$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;j = i + incr$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;\\alpha_i = \\rho_js_j^Tq_{i+1} (存储每个\\alpha_i)$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;q_i=q_{i+1}-\\alpha_iy_j$</p>\n<p>$\\;\\;\\;\\;r_0=G_0.q_0$</p>\n<p>$\\;\\;\\;\\;for\\;i=0, 1, …, (bound - 1)$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;j=i+incr$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;\\beta_i = \\rho_jy_j^Tr_i$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;r_{i+1}=r_i+s_j(\\alpha_i-\\beta_i)$</p>\n<p><strong>算法1的证明：</strong></p>\n<p>$q_{bound}=g_{iter}$</p>\n<p>对于$0&lt;i&lt;bound$,</p>\n<p>$q_i=q_{i+1}-\\alpha_iy_i \\ $</p>\n<p>$=q_{i+1}-\\rho_jy_js_j^Tq_{i+1}$</p>\n<p>$=(I-\\rho_jy_js_j^T)q_{i+1}$</p>\n<p>$=v_j^Tq_{i+1}$</p>\n<p>$=v_{inc+i}^Tq_{i+1}$</p>\n<p>$=v_{inc+i}v_{inc+i+1}q_{i+2}$</p>\n<p>$=v_{inc+i}v_{inc+i+1}v_{inc+i+2}…v_{inc+bound-1}q_{bound}\\;(23)$</p>\n<p>$\\alpha_i=\\rho_js_j^Tq_{i+1}$</p>\n<p>$=\\rho_{inc+i}s_{inc+i}^Tv_{inc+i+1}v_{inc+i+2}…v_{inc+bound-1}q_{bound}\\;(24)$</p>\n<p>$r_0=G_0q_0=G_0v_{inc}v_{inc+1}…v_{inc+bound-1}q_{bound}(25)$</p>\n<p>$r_{i+1}=r_i+s_j(\\alpha_i-\\beta_i)$</p>\n<p>$=r_i+s_j\\alpha_j-s_j\\rho_jy_j^Tr_i=(I-s_j\\rho_jy_j^T)r_i+s_j\\alpha_i=v_{inc+i}^Tr_i+s_{inc+i}\\alpha_i(26)$</p>\n<p>由式26可得：<br>$r_{bound}=s_{inc+bound-1}\\alpha_{bound-1}+v_{inc+bound-1}r_{bound-1}$</p>\n<p>$=s_{inc+bound-1}\\rho_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}+v_{inc+bound-1}r_{bound-1}$</p>\n<p>$=s_{inc+bound-1}\\rho_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T(s_{inc+bound-2}\\alpha_{bound-2}+v_{inc+bound-2}^Tr_{bound-2})$</p>\n<p>$=\\rho_{inc+bound-1}s_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T\\rho_{inc+bound-2}s_{inc+bound-2}s_{inc+bound-2}^Tv_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^Tr_{round-2}$</p>\n<p>$=\\rho_{inc+bound-1}s_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T\\rho_{inc+bound-2}s_{inc+bound-2}s_{inc+bound-2}^Tv_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\rho_{inc+bound-3}s_{inc+bound-3}s_{inc+bound-3}^Tv_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^Tv_{inc+bound-3}^Tr_{bound-3}$</p>\n<p>$=\\rho_{inc+bound-1}s_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T\\rho_{inc+bound-2}s_{inc+bound-2}s_{inc+bound-2}^Tv_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\rho_{inc+bound-3}s_{inc+bound-3}s_{inc+bound-3}^Tv_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;…$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\;…\\;v_{inc+1}^T\\rho_{inc}s_{inc}s_{inc}^Tv_{inc+1}\\;…\\;v_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\;…\\;v_{inc+1}^Tv_{inc}^Tr_0$</p>\n<p>$=\\rho_{inc+bound-1}s_{inc+bound-1}s_{inc+bound-1}^Tq_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^T\\rho_{inc+bound-2}s_{inc+bound-2}s_{inc+bound-2}^Tv_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\rho_{inc+bound-3}s_{inc+bound-3}s_{inc+bound-3}^Tv_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;…$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\;…\\;v_{inc+1}^T\\rho_{inc}s_{inc}s_{inc}^Tv_{inc+1}\\;…\\;v_{inc+bound-2}v_{inc+bound-1}q_{bound}$</p>\n<p>$\\;\\;\\;+v_{inc+bound-1}^Tv_{inc+bound-2}^T\\;…\\;v_{inc+1}^Tv_{inc}^TG_0v_{inc}v_{inc+1}…v_{inc+bound-1}q_{bound}$</p>\n<p>$=G_{iter}g_{iter}$</p>\n<p>到此，lbfgs迭代求解证明完毕，其中[1]中实现的lbfgs求解，就是用的该迭代算法。</p>\n<h2 id=\"OWL-QN算法及求解\"><a href=\"#OWL-QN算法及求解\" class=\"headerlink\" title=\"OWL-QN算法及求解\"></a>OWL-QN算法及求解</h2><p>为了减少模型过拟合，我们在进行优化求解时，通常的方式是加入正则项。常见的正则因子包括$l1$正则和$l2$正则。相对于$l2$正则，$l1$正则的优势在于[3]:(1)当大多数特征之间不相关时，$l1$正则在理论和实践上都能够学习更好的模型;(2)$l1$正则能够学到更稀疏的参数空间，有更好的可解释型，在模型计算时能更高效的进行计算。</p>\n<p>由于$l1$正则的一阶导数是常数，迭代时使得每个变量尽量被更新为0（$l2$正则是一个比例值，使得每个变量逐渐接近0而不是直接更行为0）。由于$l1$正则在零点不可导，使得基于梯度的优化算法如lbfgs算法无法使用。 针对该问题，Galen Andrew等人提出了OWL-QN(Orthant-Wise Limited-memory Quasi-Newton)算法，用于求解带$l1$正则的log-linear model。</p>\n<h3 id=\"相关定义\"><a href=\"#相关定义\" class=\"headerlink\" title=\"相关定义\"></a>相关定义</h3><p>为方便描述OWL-QN算法，我们做如下一些定义：</p>\n<p>$f(x)$对$x_i$的右导数：$\\partial_i^+=lim_{\\alpha-&gt;0}\\frac{f(x+\\alpha e_i)-f(x)}{\\alpha}$</p>\n<p>$f(x)$对$x_i$的左导数：$\\partial_i^-=lim_{\\alpha-&gt;0}\\frac{f(x)-f(x+\\alpha e_i)}{\\alpha}$</p>\n<p>其中$e_i$是第$i$个维度的基向量。</p>\n<p>$f(x)$对方向$d$的偏导数：$f′(x;d)=lim_{\\alpha-&gt;0}\\frac{f(x+\\alpha d)-f(x)}{\\alpha}$</p>\n<p>符号函数:$\\sigma(x_i) =  \\begin{cases}<br>1,  &amp; x_i&gt;0\\\\<br>-1,  &amp; x_i&lt;0\\\\<br>0,  &amp; x_i=0<br>\\end{cases}<br>$</p>\n<p>象限投影函数:$\\pi(x_i,y_i) =  \\begin{cases}<br>x_i,  &amp; \\sigma(x_i) = \\sigma(y_i)\\\\<br>0,  &amp; otherwise<br>\\end{cases}<br>$</p>\n<h3 id=\"OWL-QN算法\"><a href=\"#OWL-QN算法\" class=\"headerlink\" title=\"OWL-QN算法\"></a>OWL-QN算法</h3><p><strong>基于象限建模</strong></p>\n<p>考虑L1正则，要求解的目标函数为：</p>\n<p>$F(x)=f(x)+C ||x||_{1}\\;(27)$</p>\n<p>其中$f(x)$为原始损失，$C ||x||_{1}$为正则惩罚。</p>\n<p>对于包含$L1$正则目标函数，当数据点集合在某个特定的象限内部（所有维度的符号保持不变），它是可导的。$L1$正则部分是参数的线性函数，且目标函数的二阶导数只取决于原始损失(不包括正则)的部分。基于这点，对于目标函数，可构建包括当前点的某个象限二阶泰勒展开（固定该象限时梯度可以求解，hessian矩阵只根据原始损失部分求解即可），并限制搜索的点，使得迭代后参数对应象限对于当前的近似依然是合法的。</p>\n<p>对于向量$\\varepsilon \\in \\lbrace -1, 0 , 1 \\rbrace ^n$, 我们定义其对应象限区域为：</p>\n<p>$\\Omega_\\varepsilon=\\lbrace x \\in R^n: \\pi(x;\\varepsilon)=x\\rbrace$</p>\n<p>对于该象限内的任意点$x$，$F(x)=f(x)+C \\varepsilon^Tx\\;(28)$</p>\n<p>我们在式28基础上，扩展定义$F_\\varepsilon$为定义在$R^n$上函数，在每个象限具有和$R_\\varepsilon$空间类似的导数。通过损失函数的hessian矩阵的逆$H_k$，以及$F_\\varepsilon$的负梯度在$\\Omega_\\varepsilon$的投影$v^k$，可以近似$F_\\varepsilon$在$\\Omega_\\varepsilon$的投影。为迭代求$F_\\varepsilon$最小值，出于技术原因，限制搜索的方向和$v^k$所在象限一致。</p>\n<p>$p^k=\\pi(H_kv^k;v^k)$</p>\n<p><strong>选择投影象限：</strong></p>\n<p>为了选择投影的象限，我们定义伪梯度：</p>\n<p>$\\diamond_iF(x)=\\begin{cases}<br>\\partial_i^{-}F(x),  &amp; if\\;\\partial_i^{-}F(x)&gt;0\\\\<br>\\partial_i^{+}F(x),  &amp; if\\;\\partial_i^{+}F(x)&lt;0\\\\<br>0,  &amp; otherwise<br>\\end{cases}\\;(29)<br>$</p>\n<p>其中，$\\partial_i^{+/-}F(x)$定义如下：<br>$\\partial_i^{+/-}F(x)=\\frac{\\partial}{\\partial x_i} f(x)+\\begin{cases}<br>C \\sigma(x_i) &amp; if\\;x_i\\neq 0\\\\<br>+/-C &amp; if\\;x_i=0<br>\\end{cases}\\;(30)$</p>\n<p>由式30可得，$\\partial_i^{-}F(x)\\leq \\partial_i^{+}F(x)$，因此式29能够精确定义。伪梯度是对梯度信息的泛化，$x$是极小值的充要条件是$\\diamond_iF(x)=0$</p>\n<p>一个合理的象限选择可以定义如下：</p>\n<p>$\\varepsilon_i^k=\\begin{cases}\\sigma(x_i^k) &amp;if(x_i^k\\neq0)\\\\<br>\\sigma(-\\diamond_iF(x)) &amp; if (x_i^k = 0)<br>\\end{cases}\\;(31)$</p>\n<p>这样选择象限的理由是：-$\\diamond_iF(x)$和$F_\\varepsilon$的负梯度在$\\Omega_\\varepsilon$的投影$v^k$相等。因此，在利用owl-qn算法求解时，并不需要显示的计算$\\varepsilon_i$,直接计算$-\\diamond_iF(x)$, 就等价于按照式31设置$\\varepsilon$,并代入式28求解梯度的投影。</p>\n<p><strong>有约束的线性搜索</strong></p>\n<p>为了确保每次迭代没有离开合法的象限空间，owl-qns算法对搜索的点重新投影到象限$\\Omega_\\varepsilon$，对于符号发生变化的每个维度，均置为0.如式32所示。</p>\n<p>$x_{k+1}=\\pi(x^k+\\alpha p^k; \\varepsilon^k)\\;(32)$</p>\n<p>有很多的线性搜索方法，[3]采用的方法是：</p>\n<p><strong>算法1:有约束的线性搜索</strong></p>\n<p>(1) $设置\\;\\beta,\\gamma \\in (0,1)$</p>\n<p>(2) $for\\;\\;n = 0, 1, 2…$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\alpha=\\beta^n$</p>\n<p>$\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;if\\;\\;f(x^{k+1})\\leq f(x^k)-\\gamma v^T(x^{k+1}-x^k)$</p>\n<p>$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;找到下个最优解 $</p>\n<p>$\\;\\;\\;\\;\\;\\; \\;\\;\\;\\;else\\;\\;continue$</p>\n<p><strong>owl-qn算法</strong></p>\n<p>owl-qn算法同普通的lbfgs算法基本相同，不同之处主要在于：（1）需要计算伪梯度；（2）搜索方向对$v^k$对应的象限做做投影；（3）搜索的点需要限制在上次迭代点对应的象限。（4）目标函数的非正则部分的梯度用于更新$y$向量集合,而不是用伪梯度去更新$y$向量集合。</p>\n<p><strong>算法2:owl-qn算法描述</strong></p>\n<p>$初始化x_0,s=\\lbrace\\rbrace,y=\\lbrace\\rbrace$</p>\n<p>$for\\;\\; k = 0 \\;to \\;MaxIters$</p>\n<p>$\\;\\;\\;\\;计算梯度v^k=-\\diamond f(x^k)$</p>\n<p>$\\;\\;\\;\\;通过s,y向量集合,计算d^k=H_kv^k$</p>\n<p>$\\;\\;\\;\\;p^k=\\pi(d^k;v^k)$</p>\n<p>$\\;\\;\\;\\;根据算法1求解x_{k+1}$</p>\n<p>$\\;\\;\\;\\;如果达到终止条件，则终止算法，否则更新s^k=x_{k+1}-x_{k},y_{k+1}=\\triangledown f(x^{k+1})-\\triangledown f(x^{k}) 向量集合$</p>\n<h2 id=\"LBFGS在liblbfgs开源库的实现\"><a href=\"#LBFGS在liblbfgs开源库的实现\" class=\"headerlink\" title=\"LBFGS在liblbfgs开源库的实现\"></a>LBFGS在liblbfgs开源库的实现</h2><p>本章节主要介绍LBFGS算法在liblbfgs开源库[1]的实现，[1]不仅实现了普通的lbfgs算法，也实现了上个章节介绍的owl-qn算法。</p>\n<p><strong>相关数据结构:</strong><br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//定义callback_data_t结构</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">tag_callback_data</span> &#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">int</span> n;  <span class=\"comment\">//变量个数</span></span><br><span class=\"line\">    <span class=\"keyword\">void</span> *instance; <span class=\"comment\">//实例</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgs_evaluate_t</span> proc_evaluate; <span class=\"comment\">//计算目标函数及梯度的回调函数</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgs_progress_t</span> proc_progress; <span class=\"comment\">//接受优化过程进度的的回调函数</span></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">tag_callback_data</span> <span class=\"title\">callback_data_t</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//定义iteration_data_t，存储lbfgs迭代需要的s,y向量</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">tag_iteration_data</span> &#123;</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgsfloatval_t</span> alpha;  <span class=\"comment\">//算法1迭代需要的alpha变量</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgsfloatval_t</span> *s;     <span class=\"comment\">//x(k+1) - x(k)</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgsfloatval_t</span> *y;     <span class=\"comment\">//g(k+1) - g(k)</span></span><br><span class=\"line\">    <span class=\"keyword\">lbfgsfloatval_t</span> ys;     <span class=\"comment\">//vecdot(y, s)</span></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">tag_iteration_data</span> <span class=\"title\">iteration_data_t</span>;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//定义lbfgs参数</span></span><br><span class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">lbfgs_parameter_t</span> _defparam = &#123;</span><br><span class=\"line\">    <span class=\"number\">6</span>, <span class=\"number\">1e-5</span>, <span class=\"number\">0</span>, <span class=\"number\">1e-5</span>,</span><br><span class=\"line\">    <span class=\"number\">0</span>, LBFGS_LINESEARCH_DEFAULT, <span class=\"number\">40</span>,</span><br><span class=\"line\">    <span class=\"number\">1e-20</span>, <span class=\"number\">1e20</span>, <span class=\"number\">1e-4</span>, <span class=\"number\">0.9</span>, <span class=\"number\">0.9</span>, <span class=\"number\">1.0e-16</span>,</span><br><span class=\"line\">    <span class=\"number\">0.0</span>, <span class=\"number\">0</span>, <span class=\"number\">-1</span>,</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p><strong>lbfgs算法:</strong><br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//lbfgs算法求解核心过程，为描述lbfgs算法核心流程，此处只保留主要代码</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">lbfgs</span><span class=\"params\">(</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">int</span> n, <span class=\"comment\">//变量个数</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgsfloatval_t</span> *x, <span class=\"comment\">//变量值</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgsfloatval_t</span> *ptr_fx, <span class=\"comment\">// 函数值</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgs_evaluate_t</span> proc_evaluate, <span class=\"comment\">//计算目标函数及梯度的回调函数</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgs_progress_t</span> proc_progress, <span class=\"comment\">//接受优化过程进度的的回调函数</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">void</span> *instance, <span class=\"comment\">//实例变量</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    <span class=\"keyword\">lbfgs_parameter_t</span> *_param <span class=\"comment\">//lbfgs优化永的的参数变量</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    ... </span><br><span class=\"line\">    <span class=\"comment\">//构建callback_data_t</span></span><br><span class=\"line\">    <span class=\"keyword\">callback_data_t</span> cd;</span><br><span class=\"line\">    cd.n = n; <span class=\"comment\">//参数的维度</span></span><br><span class=\"line\">    cd.instance = instance; <span class=\"comment\">//实例变量</span></span><br><span class=\"line\">    cd.proc_evaluate = proc_evaluate; <span class=\"comment\">//计算目标函数及梯度的回调函数</span></span><br><span class=\"line\">    cd.proc_progress = proc_progress; <span class=\"comment\">//接受优化过程进度的的回调函数</span></span><br><span class=\"line\">   ...</span><br><span class=\"line\">    <span class=\"comment\">/* Allocate working space. */</span></span><br><span class=\"line\">    xp = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));<span class=\"comment\">//上次迭代的变量值</span></span><br><span class=\"line\">    g = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));<span class=\"comment\">//本次迭代对应的梯度值</span></span><br><span class=\"line\">    gp = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));<span class=\"comment\">//上次迭代的梯度址</span></span><br><span class=\"line\">    d = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));<span class=\"comment\">//迭代方向变量</span></span><br><span class=\"line\">    w = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">    <span class=\"comment\">//对l1正则，分配OW-LQN算法伪梯度需要的存储空间 */</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (param.orthantwise_c != <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">        pg = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (pg == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">            ret = LBFGSERR_OUTOFMEMORY;</span><br><span class=\"line\">            <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;    </span><br><span class=\"line\">    <span class=\"comment\">//最近m次迭代相关向量的存储</span></span><br><span class=\"line\">    lm = (<span class=\"keyword\">iteration_data_t</span>*)vecalloc(m * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">iteration_data_t</span>));</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (lm == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">        ret = LBFGSERR_OUTOFMEMORY;</span><br><span class=\"line\">        <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">    &#125;    </span><br><span class=\"line\">    <span class=\"comment\">//最近m次迭代相关向量的初始化</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>;i &lt; m;++i) &#123;</span><br><span class=\"line\">        it = &amp;lm[i];</span><br><span class=\"line\">        it-&gt;alpha = <span class=\"number\">0</span>;</span><br><span class=\"line\">        it-&gt;ys = <span class=\"number\">0</span>;</span><br><span class=\"line\">        it-&gt;s = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">        it-&gt;y = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(n * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (it-&gt;s == <span class=\"literal\">NULL</span> || it-&gt;y == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">            ret = LBFGSERR_OUTOFMEMORY;</span><br><span class=\"line\">            <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//最近的m次迭代的目标函数值</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"number\">0</span> &lt; param.past) &#123;</span><br><span class=\"line\">        pf = (<span class=\"keyword\">lbfgsfloatval_t</span>*)vecalloc(param.past * <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">lbfgsfloatval_t</span>));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//计算目标函数的值和梯度</span></span><br><span class=\"line\">    fx = cd.proc_evaluate(cd.instance, x, g, cd.n, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"comment\">//如果有l1正则，计算带l1正则的目标函数值和伪梯度信息 </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"number\">0.</span> != param.orthantwise_c) &#123;</span><br><span class=\"line\">        <span class=\"comment\">//有l1正则，计算l1正则对应的norm</span></span><br><span class=\"line\">        xnorm = owlqn_x1norm(x, param.orthantwise_start, param.orthantwise_end);</span><br><span class=\"line\">        <span class=\"comment\">//将l1z正则对应的值加入目标函数</span></span><br><span class=\"line\">        fx += xnorm * param.orthantwise_c;</span><br><span class=\"line\">        <span class=\"comment\">//计算伪梯度信息</span></span><br><span class=\"line\">        owlqn_pseudo_gradient(</span><br><span class=\"line\">            pg, x, g, n,</span><br><span class=\"line\">            param.orthantwise_c, param.orthantwise_start, param.orthantwise_end</span><br><span class=\"line\">            );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//存储目标函数值到pf[0]</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (pf != <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">        pf[<span class=\"number\">0</span>] = fx;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//存储迭代方向到d变量, 假定原始hessian矩阵G0为单位矩阵，G0 g = g</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">        vecncpy(d, g, n);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        vecncpy(d, pg, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//通过比较g_norm / max(1, x_norm)是否小于param.epsilon，确定是否已经达到极小值</span></span><br><span class=\"line\">    vec2norm(&amp;xnorm, x, n);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">        vec2norm(&amp;gnorm, g, n);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        vec2norm(&amp;gnorm, pg, n);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (xnorm &lt; <span class=\"number\">1.0</span>) xnorm = <span class=\"number\">1.0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (gnorm / xnorm &lt;= param.epsilon) &#123;</span><br><span class=\"line\">        ret = LBFGS_ALREADY_MINIMIZED;</span><br><span class=\"line\">        <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//初始化最优步长 step: 1.0 / sqrt(vecdot(d, d, n)) */</span></span><br><span class=\"line\">    vec2norminv(&amp;step, d, n);</span><br><span class=\"line\">    k = <span class=\"number\">1</span>;</span><br><span class=\"line\">    end = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (;;) &#123;</span><br><span class=\"line\">        veccpy(xp, x, n);<span class=\"comment\">//存储变量值到xp</span></span><br><span class=\"line\">        veccpy(gp, g, n);<span class=\"comment\">//存储梯度值到gp</span></span><br><span class=\"line\">        <span class=\"comment\">/* Search for an optimal step. */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;<span class=\"comment\">//无l1正则，在d方向搜索最优解</span></span><br><span class=\"line\">            ls = linesearch(n, x, &amp;fx, g, d, &amp;step, xp, gp, w, &amp;cd, &amp;param);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">//有l1正则，在d方向搜索最优解</span></span><br><span class=\"line\">            ls = linesearch(n, x, &amp;fx, g, d, &amp;step, xp, pg, w, &amp;cd, &amp;param);</span><br><span class=\"line\">            <span class=\"comment\">//计算伪梯度</span></span><br><span class=\"line\">            owlqn_pseudo_gradient(</span><br><span class=\"line\">                pg, x, g, n,</span><br><span class=\"line\">                param.orthantwise_c, param.orthantwise_start, param.orthantwise_end</span><br><span class=\"line\">                );</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//达到终止条件</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ls &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* Revert to the previous point. */</span></span><br><span class=\"line\">            veccpy(x, xp, n);</span><br><span class=\"line\">            veccpy(g, gp, n);</span><br><span class=\"line\">            ret = ls;</span><br><span class=\"line\">            <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* Compute x and g norms. */</span></span><br><span class=\"line\">        <span class=\"comment\">//计算x范数，g范数</span></span><br><span class=\"line\">        vec2norm(&amp;xnorm, x, n);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">            vec2norm(&amp;gnorm, g, n);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            vec2norm(&amp;gnorm, pg, n);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//输出进度信息</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (cd.proc_progress) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> ((ret = cd.proc_progress(cd.instance, x, g, fx, xnorm, gnorm, step, cd.n, k, ls))) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">goto</span> lbfgs_exit;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//收敛测试， |g(x)| / \\max(1, |x|) &lt; \\epsil</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (xnorm &lt; <span class=\"number\">1.0</span>) xnorm = <span class=\"number\">1.0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (gnorm / xnorm &lt;= param.epsilon) &#123;</span><br><span class=\"line\">            ret = LBFGS_SUCCESS;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//以past为周期，根据当前函数值和1个周期之前的函数值判断是否停止迭代</span></span><br><span class=\"line\">        <span class=\"comment\">//停止条件：(f(past_x) - f(x)) / f(x) &lt; \\delta</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (pf != <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* We don't test the stopping criterion while k &lt; past. */</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (param.past &lt;= k) &#123;</span><br><span class=\"line\">                <span class=\"comment\">/* Compute the relative improvement from the past. */</span></span><br><span class=\"line\">                rate = (pf[k % param.past] - fx) / fx;</span><br><span class=\"line\">                <span class=\"comment\">/* The stopping criterion. */</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (rate &lt; param.delta) &#123;</span><br><span class=\"line\">                    ret = LBFGS_STOP;</span><br><span class=\"line\">                    <span class=\"keyword\">break</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            <span class=\"comment\">/* Store the current value of the objective function. */</span></span><br><span class=\"line\">            pf[k % param.past] = fx;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">//达到最大迭代次数</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (param.max_iterations != <span class=\"number\">0</span> &amp;&amp; param.max_iterations &lt; k+<span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* Maximum number of iterations. */</span></span><br><span class=\"line\">            ret = LBFGSERR_MAXIMUMITERATION;</span><br><span class=\"line\">            <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//更新向量s, y  s_&#123;k+1&#125; = x_&#123;k+1&#125; - x_&#123;k&#125;，y_&#123;k+1&#125; = g_&#123;k+1&#125; - g_&#123;k&#125;</span></span><br><span class=\"line\">        it = &amp;lm[end];</span><br><span class=\"line\">        vecdiff(it-&gt;s, x, xp, n);</span><br><span class=\"line\">        vecdiff(it-&gt;y, g, gp, n);</span><br><span class=\"line\"></span><br><span class=\"line\">        vecdot(&amp;ys, it-&gt;y, it-&gt;s, n); <span class=\"comment\">//ys = y^t \\cdot s; 1 / \\rho</span></span><br><span class=\"line\">        vecdot(&amp;yy, it-&gt;y, it-&gt;y, n); <span class=\"comment\">//yy = y^t \\cdot y</span></span><br><span class=\"line\">        it-&gt;ys = ys;<span class=\"comment\">// y^t \\cdot s</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/*</span></span><br><span class=\"line\"><span class=\"comment\">           Recursive formula to compute dir = -(H \\cdot g).</span></span><br><span class=\"line\"><span class=\"comment\">               This is described in page 779 of:</span></span><br><span class=\"line\"><span class=\"comment\">               Jorge Nocedal.</span></span><br><span class=\"line\"><span class=\"comment\">               Updating Quasi-Newton Matrices with Limited Storage.</span></span><br><span class=\"line\"><span class=\"comment\">               Mathematics of Computation, Vol. 35, No. 151,</span></span><br><span class=\"line\"><span class=\"comment\">               pp. 773--782, 1980.</span></span><br><span class=\"line\"><span class=\"comment\">        */</span></span><br><span class=\"line\">        <span class=\"comment\">//根据文献[1]中算法（对应本文算法1），计算 -(G \\cdot g)</span></span><br><span class=\"line\">        bound = (m &lt;= k) ? m : k;</span><br><span class=\"line\">        ++k;</span><br><span class=\"line\">        end = (end + <span class=\"number\">1</span>) % m;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* Compute the steepest direction. */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (param.orthantwise_c == <span class=\"number\">0.</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* Compute the negative of gradients. */</span></span><br><span class=\"line\">            vecncpy(d, g, n);</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            vecncpy(d, pg, n);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        j = end;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>;i &lt; bound;++i) &#123;</span><br><span class=\"line\">            j = (j + m - <span class=\"number\">1</span>) % m;    <span class=\"comment\">/* if (--j == -1) j = m-1; */</span></span><br><span class=\"line\">            it = &amp;lm[j];</span><br><span class=\"line\">            <span class=\"comment\">/* \\alpha_&#123;j&#125; = \\rho_&#123;j&#125; s^&#123;t&#125;_&#123;j&#125; \\cdot q_&#123;k+1&#125;. */</span></span><br><span class=\"line\">            vecdot(&amp;it-&gt;alpha, it-&gt;s, d, n);</span><br><span class=\"line\">            it-&gt;alpha /= it-&gt;ys;</span><br><span class=\"line\">            <span class=\"comment\">/* q_&#123;i&#125; = q_&#123;i+1&#125; - \\alpha_&#123;i&#125; y_&#123;i&#125;. */</span></span><br><span class=\"line\">            vecadd(d, it-&gt;y, -it-&gt;alpha, n);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        vecscale(d, ys / yy, n);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>;i &lt; bound;++i) &#123;</span><br><span class=\"line\">            it = &amp;lm[j];</span><br><span class=\"line\">            <span class=\"comment\">/* \\beta_&#123;j&#125; = \\rho_&#123;j&#125; y^t_&#123;j&#125; \\cdot \\gamma_&#123;i&#125;. */</span></span><br><span class=\"line\">            vecdot(&amp;beta, it-&gt;y, d, n);</span><br><span class=\"line\">            beta /= it-&gt;ys;</span><br><span class=\"line\">            <span class=\"comment\">/* \\gamma_&#123;i+1&#125; = \\gamma_&#123;i&#125; + (\\alpha_&#123;j&#125; - \\beta_&#123;j&#125;) s_&#123;j&#125;. */</span></span><br><span class=\"line\">            vecadd(d, it-&gt;s, it-&gt;alpha - beta, n);</span><br><span class=\"line\">            j = (j + <span class=\"number\">1</span>) % m;        <span class=\"comment\">/* if (++j == m) j = 0; */</span></span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>[1] chokkan, <a href=\"https://github.com/chokkan/liblbfgs\" target=\"_blank\" rel=\"noopener\">https://github.com/chokkan/liblbfgs</a></p>\n<p>[2] Jorge Nocedal, Updating Quasi-Newton Matrices With Limited Storage</p>\n<p>[3] Galen Andrew, Jianfeng Gao, Scalable Training of L1-Regularized Log-Linear Models</p>\n<p>[4] 皮果提, <a href=\"http://blog.csdn.net/itplus/article/details/21896453\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/itplus/article/details/21896453</a></p>\n<p>[5] <a href=\"http://blog.sina.com.cn/s/blog_eb3aea990101gflj.html\" target=\"_blank\" rel=\"noopener\">http://blog.sina.com.cn/s/blog_eb3aea990101gflj.html</a></p>\n<p>[6] <a href=\"https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula</a></p>\n"},{"title":"一种简单的优质挖掘模型","date":"2017-05-29T16:00:00.000Z","toc":true,"description":"该模型可对用户优质度进行建模，同时适用于其它相关的优质度建模","mathjax":true,"_content":"\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n# 优质挖掘的意义\n\n**有助于找到有潜力的用户**: 对于in这样基于内容的社交平台，能持续地沉淀优质的用户，将这些用户挖掘出来并有效利用，就意味着可以持续地产出优质内容、有助于更好的社交体验。通过自动化的方式对用户优质度进行建模，不仅有助于挖掘有潜力的用户，也能对现有达人做更加科学的评估，有助于达人运营的效率提升。\n\n**更好地服务内容推荐**:对于in这样的内容社交平台，每个用户都可以自由地发表图片，创建话题，发起直播等。然而，这些内容如果不经过任何过滤，而通过个性化推荐的方式直接推荐给来消费内容的用户，会带来非常不好的的体验。对于那些到in来消费内容的用户而言，更希望看到的是一些优质、有趣、能吸引个人兴趣的内容。如果不对内容进行优质建模，而直接根据推荐算法为用户推荐内容，不可避免地会为用户推出质量不好的图片、话题、直播等，导致用户体验下降。\n \n# 优质挖掘模型\n**为方便说明，我们以优质用户挖掘为例进行说明，对图片、话题、直播等优质挖掘模型类似，不同之处在于特征不同、以及针对具体的应用可能有对应的特殊规则处理。**\n\n## 模型建立\n    \n用户是否优质反应在多个方面。如发图在质量较高、图片等点击率高、粉丝多等。通过其中任何一个特征$i$，我们都能建立该特征和优质得分之间的关系。其中特征$i$的的取值越大，我们认为该用户越可能是优质用户，对应的优质得分越大。根据分析，我们建立模型如式1所示：\n \n  \n $p\\_i = \\begin{cases}\n1,  & if x\\_i > T\\_{up}\\\\\\\\\n0,  & if x\\_i < T\\_{low}\\\\\\\\\n\\frac{x\\_i-T\\_{low}}{T\\_{up}-T\\_{low}} & else\n\\end{cases} （式1）$\n \n 其对应的几何意义如图1所示，当特征值落在$T\\_{low}$左边时，用户为非优质用户（得分为0），落在$T\\_{up}$右边时，为优质用户（得分为1）；当落在$T\\_{low}$和$T\\_{up}$之间时，用户对应的得分取值为0到1之间，具体取决于其具体的位置，越靠近$T\\_{up}$, 其对应的得分越高。\n \n <center> 图1: 特征与优质的关系图 \n ![“优质模型示意图”](/qualified_model/probability.png)\n</center>\n\n**通过一个特征进行优质用户的确定，往往有比较大的偏差**。如有的用户A，虽然粉丝数量不多，但是其发图的点击率很高；而另一部分用户B可能粉丝数量较多，但是其图片的点击率却一般。如果只通过一个特征来进行优质用户建模，不仅容易导致个别特征异常导致准确率低，也会存在较多的用户优质无法召回。**我们采用的方法是，对每个特征计算用户的优质得分，最后将多个特征得到的得分进行加权，使得模型更加鲁棒，提升优质用户挖掘的准确率和召回率。** 如式2所示：\n\n$score\\_i=\\sum\\_k w\\_k p\\_{i,k} (式2)$\n\n其中$score\\_i$表示第i个用户的得分，$w\\_k $表示第$k$个特征得分对应的权重，$p\\_{i,k}$表示第$i$个用户的第$k$个权重的得分。\n由式1和式2得知，我们在模型训练阶段，得到每个特征对应的阈值$T\\_{low}$， $T\\_{up}$, 以及每个特征对应的权值$w$，在预测阶段，直接根据式2计算得到用户的优质得分。\n\n## 特征提取\n\n特征提取主要包括数据清洗、数据预处理、特征生成三部分。\n**数据清洗:**过滤日志中的非法数据，异常数据等，如日志非法数据、运营号、异常点数据等。\n**数据预处理:**对清洗后对数据进行预处理。如针对用户点击大于pv的情况，将pv设置为点击的值，点赞、评论等可以做类似的处理。\n**特征生成:**在预处理结果基础上进行特征提取，如数值特征包括发图数、pv，click、点击率、点赞数、点赞率、评论数、评论率，收藏数、收藏率、好友数、关注数量、粉丝数等；非数值特征数值化，如用户级别、是否达人等。\n\n## 模型训练\n**样本生成:**\n正样本: 筛选发优质图超过一定数量的用户,再经过人工审核最终确定正样本集合。\n负样本: 从total集合里过滤正样本，剩余的所有样本作为负样本。严格来说，负样本中可能包括一些正样本，但对于我们所采用的模型，只要保证大部分样本都是负样本，就基本不受影响。这是因为该模型在每个特征维度，得分都是和正负样本中的特征均值相关的，即使负样本包含一定数量的正样本，其均值依然会被真正的负样本主导而不会有太大影响。 \n\n\n**模型训练**\n模型训练的目的，就是通过计算得到每个特征对应的阈值$T\\_{low}$， $T\\_{up}$, 以及每个特征对应的权值$w$。训练步骤如下所示：\n\nstep1: 对于所有特征，初始化$T\\_{i,low}$，$T\\_{i,up}$，其中$T\\_{i,low}$为所有负样本对应特征$i$的均值$u\\_{i,low}$，$T\\_{i,up}$为所有正样本对应特征$i$的均值$u\\_{i,up}$\n\nstep2: 根据如下公式计算特征$i$的权重$w\\_i$.\n       $w\\_i=\\frac{u\\_{i,up} - u\\_{i,low} }{u\\_{i,up}}$\n       \nstep3: 调整不同阈值T，使得准确率和召回率大于一定值，并取f1最高的阈值作为当前最优参数。\n\nstep4: 调整步骤3得到的参数，允许每个参数分别向上和向下波动一定幅度，对于每种参数组合，分别执行步骤2和步骤3，根据f1指标，选择最优的参数组合，作为最终的训练结果。\n\n\n## 优质预测和挖掘\n\n对于测试用户，首先提取用户相关特征，然后利用式2进行优质得分预测。将所有得分大于阈值T的用户作为优质用户的挖掘结果。\n\n# 总结\n本文提出了一种优质用户的挖掘方法，该方法的**优势在于模型简单、训练速度快；只需要将特征转化为数值特征即可（而不需要进一步处理）；同时该模型对于正负样本的构造成本较低，允许有一定的噪声样本混入而不受较大影响；允许正负样本的数量不均衡。** **该方法的缺点是没有考虑特征之间的关联，没有对多个特征直接进行建模，如果将多个特征之间的组合关系考虑进去，可能会有更好的效果**\n","source":"_posts/qualified_model.md","raw":"---\ntitle: 一种简单的优质挖掘模型\ndate: 2017-05-30\ntoc: true\ncategories: 用户画像\ntags: [优质挖掘]\ndescription: 该模型可对用户优质度进行建模，同时适用于其它相关的优质度建模\nmathjax: true\n---\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n# 优质挖掘的意义\n\n**有助于找到有潜力的用户**: 对于in这样基于内容的社交平台，能持续地沉淀优质的用户，将这些用户挖掘出来并有效利用，就意味着可以持续地产出优质内容、有助于更好的社交体验。通过自动化的方式对用户优质度进行建模，不仅有助于挖掘有潜力的用户，也能对现有达人做更加科学的评估，有助于达人运营的效率提升。\n\n**更好地服务内容推荐**:对于in这样的内容社交平台，每个用户都可以自由地发表图片，创建话题，发起直播等。然而，这些内容如果不经过任何过滤，而通过个性化推荐的方式直接推荐给来消费内容的用户，会带来非常不好的的体验。对于那些到in来消费内容的用户而言，更希望看到的是一些优质、有趣、能吸引个人兴趣的内容。如果不对内容进行优质建模，而直接根据推荐算法为用户推荐内容，不可避免地会为用户推出质量不好的图片、话题、直播等，导致用户体验下降。\n \n# 优质挖掘模型\n**为方便说明，我们以优质用户挖掘为例进行说明，对图片、话题、直播等优质挖掘模型类似，不同之处在于特征不同、以及针对具体的应用可能有对应的特殊规则处理。**\n\n## 模型建立\n    \n用户是否优质反应在多个方面。如发图在质量较高、图片等点击率高、粉丝多等。通过其中任何一个特征$i$，我们都能建立该特征和优质得分之间的关系。其中特征$i$的的取值越大，我们认为该用户越可能是优质用户，对应的优质得分越大。根据分析，我们建立模型如式1所示：\n \n  \n $p\\_i = \\begin{cases}\n1,  & if x\\_i > T\\_{up}\\\\\\\\\n0,  & if x\\_i < T\\_{low}\\\\\\\\\n\\frac{x\\_i-T\\_{low}}{T\\_{up}-T\\_{low}} & else\n\\end{cases} （式1）$\n \n 其对应的几何意义如图1所示，当特征值落在$T\\_{low}$左边时，用户为非优质用户（得分为0），落在$T\\_{up}$右边时，为优质用户（得分为1）；当落在$T\\_{low}$和$T\\_{up}$之间时，用户对应的得分取值为0到1之间，具体取决于其具体的位置，越靠近$T\\_{up}$, 其对应的得分越高。\n \n <center> 图1: 特征与优质的关系图 \n ![“优质模型示意图”](/qualified_model/probability.png)\n</center>\n\n**通过一个特征进行优质用户的确定，往往有比较大的偏差**。如有的用户A，虽然粉丝数量不多，但是其发图的点击率很高；而另一部分用户B可能粉丝数量较多，但是其图片的点击率却一般。如果只通过一个特征来进行优质用户建模，不仅容易导致个别特征异常导致准确率低，也会存在较多的用户优质无法召回。**我们采用的方法是，对每个特征计算用户的优质得分，最后将多个特征得到的得分进行加权，使得模型更加鲁棒，提升优质用户挖掘的准确率和召回率。** 如式2所示：\n\n$score\\_i=\\sum\\_k w\\_k p\\_{i,k} (式2)$\n\n其中$score\\_i$表示第i个用户的得分，$w\\_k $表示第$k$个特征得分对应的权重，$p\\_{i,k}$表示第$i$个用户的第$k$个权重的得分。\n由式1和式2得知，我们在模型训练阶段，得到每个特征对应的阈值$T\\_{low}$， $T\\_{up}$, 以及每个特征对应的权值$w$，在预测阶段，直接根据式2计算得到用户的优质得分。\n\n## 特征提取\n\n特征提取主要包括数据清洗、数据预处理、特征生成三部分。\n**数据清洗:**过滤日志中的非法数据，异常数据等，如日志非法数据、运营号、异常点数据等。\n**数据预处理:**对清洗后对数据进行预处理。如针对用户点击大于pv的情况，将pv设置为点击的值，点赞、评论等可以做类似的处理。\n**特征生成:**在预处理结果基础上进行特征提取，如数值特征包括发图数、pv，click、点击率、点赞数、点赞率、评论数、评论率，收藏数、收藏率、好友数、关注数量、粉丝数等；非数值特征数值化，如用户级别、是否达人等。\n\n## 模型训练\n**样本生成:**\n正样本: 筛选发优质图超过一定数量的用户,再经过人工审核最终确定正样本集合。\n负样本: 从total集合里过滤正样本，剩余的所有样本作为负样本。严格来说，负样本中可能包括一些正样本，但对于我们所采用的模型，只要保证大部分样本都是负样本，就基本不受影响。这是因为该模型在每个特征维度，得分都是和正负样本中的特征均值相关的，即使负样本包含一定数量的正样本，其均值依然会被真正的负样本主导而不会有太大影响。 \n\n\n**模型训练**\n模型训练的目的，就是通过计算得到每个特征对应的阈值$T\\_{low}$， $T\\_{up}$, 以及每个特征对应的权值$w$。训练步骤如下所示：\n\nstep1: 对于所有特征，初始化$T\\_{i,low}$，$T\\_{i,up}$，其中$T\\_{i,low}$为所有负样本对应特征$i$的均值$u\\_{i,low}$，$T\\_{i,up}$为所有正样本对应特征$i$的均值$u\\_{i,up}$\n\nstep2: 根据如下公式计算特征$i$的权重$w\\_i$.\n       $w\\_i=\\frac{u\\_{i,up} - u\\_{i,low} }{u\\_{i,up}}$\n       \nstep3: 调整不同阈值T，使得准确率和召回率大于一定值，并取f1最高的阈值作为当前最优参数。\n\nstep4: 调整步骤3得到的参数，允许每个参数分别向上和向下波动一定幅度，对于每种参数组合，分别执行步骤2和步骤3，根据f1指标，选择最优的参数组合，作为最终的训练结果。\n\n\n## 优质预测和挖掘\n\n对于测试用户，首先提取用户相关特征，然后利用式2进行优质得分预测。将所有得分大于阈值T的用户作为优质用户的挖掘结果。\n\n# 总结\n本文提出了一种优质用户的挖掘方法，该方法的**优势在于模型简单、训练速度快；只需要将特征转化为数值特征即可（而不需要进一步处理）；同时该模型对于正负样本的构造成本较低，允许有一定的噪声样本混入而不受较大影响；允许正负样本的数量不均衡。** **该方法的缺点是没有考虑特征之间的关联，没有对多个特征直接进行建模，如果将多个特征之间的组合关系考虑进去，可能会有更好的效果**\n","slug":"qualified_model","published":1,"updated":"2018-02-11T08:33:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjdikgudd000aga01zx4i7zz0","content":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n<h1 id=\"优质挖掘的意义\"><a href=\"#优质挖掘的意义\" class=\"headerlink\" title=\"优质挖掘的意义\"></a>优质挖掘的意义</h1><p><strong>有助于找到有潜力的用户</strong>: 对于in这样基于内容的社交平台，能持续地沉淀优质的用户，将这些用户挖掘出来并有效利用，就意味着可以持续地产出优质内容、有助于更好的社交体验。通过自动化的方式对用户优质度进行建模，不仅有助于挖掘有潜力的用户，也能对现有达人做更加科学的评估，有助于达人运营的效率提升。</p>\n<p><strong>更好地服务内容推荐</strong>:对于in这样的内容社交平台，每个用户都可以自由地发表图片，创建话题，发起直播等。然而，这些内容如果不经过任何过滤，而通过个性化推荐的方式直接推荐给来消费内容的用户，会带来非常不好的的体验。对于那些到in来消费内容的用户而言，更希望看到的是一些优质、有趣、能吸引个人兴趣的内容。如果不对内容进行优质建模，而直接根据推荐算法为用户推荐内容，不可避免地会为用户推出质量不好的图片、话题、直播等，导致用户体验下降。</p>\n<h1 id=\"优质挖掘模型\"><a href=\"#优质挖掘模型\" class=\"headerlink\" title=\"优质挖掘模型\"></a>优质挖掘模型</h1><p><strong>为方便说明，我们以优质用户挖掘为例进行说明，对图片、话题、直播等优质挖掘模型类似，不同之处在于特征不同、以及针对具体的应用可能有对应的特殊规则处理。</strong></p>\n<h2 id=\"模型建立\"><a href=\"#模型建立\" class=\"headerlink\" title=\"模型建立\"></a>模型建立</h2><p>用户是否优质反应在多个方面。如发图在质量较高、图片等点击率高、粉丝多等。通过其中任何一个特征$i$，我们都能建立该特征和优质得分之间的关系。其中特征$i$的的取值越大，我们认为该用户越可能是优质用户，对应的优质得分越大。根据分析，我们建立模型如式1所示：</p>\n<p> $p_i = \\begin{cases}<br>1,  &amp; if x_i &gt; T_{up}\\\\<br>0,  &amp; if x_i &lt; T_{low}\\\\<br>\\frac{x_i-T_{low}}{T_{up}-T_{low}} &amp; else<br>\\end{cases} （式1）$</p>\n<p> 其对应的几何意义如图1所示，当特征值落在$T_{low}$左边时，用户为非优质用户（得分为0），落在$T_{up}$右边时，为优质用户（得分为1）；当落在$T_{low}$和$T_{up}$之间时，用户对应的得分取值为0到1之间，具体取决于其具体的位置，越靠近$T_{up}$, 其对应的得分越高。</p>\n <center> 图1: 特征与优质的关系图<br> <img src=\"/qualified_model/probability.png\" alt=\"“优质模型示意图”\"><br></center>\n\n<p><strong>通过一个特征进行优质用户的确定，往往有比较大的偏差</strong>。如有的用户A，虽然粉丝数量不多，但是其发图的点击率很高；而另一部分用户B可能粉丝数量较多，但是其图片的点击率却一般。如果只通过一个特征来进行优质用户建模，不仅容易导致个别特征异常导致准确率低，也会存在较多的用户优质无法召回。<strong>我们采用的方法是，对每个特征计算用户的优质得分，最后将多个特征得到的得分进行加权，使得模型更加鲁棒，提升优质用户挖掘的准确率和召回率。</strong> 如式2所示：</p>\n<p>$score_i=\\sum_k w_k p_{i,k} (式2)$</p>\n<p>其中$score_i$表示第i个用户的得分，$w_k $表示第$k$个特征得分对应的权重，$p_{i,k}$表示第$i$个用户的第$k$个权重的得分。<br>由式1和式2得知，我们在模型训练阶段，得到每个特征对应的阈值$T_{low}$， $T_{up}$, 以及每个特征对应的权值$w$，在预测阶段，直接根据式2计算得到用户的优质得分。</p>\n<h2 id=\"特征提取\"><a href=\"#特征提取\" class=\"headerlink\" title=\"特征提取\"></a>特征提取</h2><p>特征提取主要包括数据清洗、数据预处理、特征生成三部分。<br><strong>数据清洗:</strong>过滤日志中的非法数据，异常数据等，如日志非法数据、运营号、异常点数据等。<br><strong>数据预处理:</strong>对清洗后对数据进行预处理。如针对用户点击大于pv的情况，将pv设置为点击的值，点赞、评论等可以做类似的处理。<br><strong>特征生成:</strong>在预处理结果基础上进行特征提取，如数值特征包括发图数、pv，click、点击率、点赞数、点赞率、评论数、评论率，收藏数、收藏率、好友数、关注数量、粉丝数等；非数值特征数值化，如用户级别、是否达人等。</p>\n<h2 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h2><p><strong>样本生成:</strong><br>正样本: 筛选发优质图超过一定数量的用户,再经过人工审核最终确定正样本集合。<br>负样本: 从total集合里过滤正样本，剩余的所有样本作为负样本。严格来说，负样本中可能包括一些正样本，但对于我们所采用的模型，只要保证大部分样本都是负样本，就基本不受影响。这是因为该模型在每个特征维度，得分都是和正负样本中的特征均值相关的，即使负样本包含一定数量的正样本，其均值依然会被真正的负样本主导而不会有太大影响。 </p>\n<p><strong>模型训练</strong><br>模型训练的目的，就是通过计算得到每个特征对应的阈值$T_{low}$， $T_{up}$, 以及每个特征对应的权值$w$。训练步骤如下所示：</p>\n<p>step1: 对于所有特征，初始化$T_{i,low}$，$T_{i,up}$，其中$T_{i,low}$为所有负样本对应特征$i$的均值$u_{i,low}$，$T_{i,up}$为所有正样本对应特征$i$的均值$u_{i,up}$</p>\n<p>step2: 根据如下公式计算特征$i$的权重$w_i$.<br>       $w_i=\\frac{u_{i,up} - u_{i,low} }{u_{i,up}}$</p>\n<p>step3: 调整不同阈值T，使得准确率和召回率大于一定值，并取f1最高的阈值作为当前最优参数。</p>\n<p>step4: 调整步骤3得到的参数，允许每个参数分别向上和向下波动一定幅度，对于每种参数组合，分别执行步骤2和步骤3，根据f1指标，选择最优的参数组合，作为最终的训练结果。</p>\n<h2 id=\"优质预测和挖掘\"><a href=\"#优质预测和挖掘\" class=\"headerlink\" title=\"优质预测和挖掘\"></a>优质预测和挖掘</h2><p>对于测试用户，首先提取用户相关特征，然后利用式2进行优质得分预测。将所有得分大于阈值T的用户作为优质用户的挖掘结果。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>本文提出了一种优质用户的挖掘方法，该方法的<strong>优势在于模型简单、训练速度快；只需要将特征转化为数值特征即可（而不需要进一步处理）；同时该模型对于正负样本的构造成本较低，允许有一定的噪声样本混入而不受较大影响；允许正负样本的数量不均衡。</strong> <strong>该方法的缺点是没有考虑特征之间的关联，没有对多个特征直接进行建模，如果将多个特征之间的组合关系考虑进去，可能会有更好的效果</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n<h1 id=\"优质挖掘的意义\"><a href=\"#优质挖掘的意义\" class=\"headerlink\" title=\"优质挖掘的意义\"></a>优质挖掘的意义</h1><p><strong>有助于找到有潜力的用户</strong>: 对于in这样基于内容的社交平台，能持续地沉淀优质的用户，将这些用户挖掘出来并有效利用，就意味着可以持续地产出优质内容、有助于更好的社交体验。通过自动化的方式对用户优质度进行建模，不仅有助于挖掘有潜力的用户，也能对现有达人做更加科学的评估，有助于达人运营的效率提升。</p>\n<p><strong>更好地服务内容推荐</strong>:对于in这样的内容社交平台，每个用户都可以自由地发表图片，创建话题，发起直播等。然而，这些内容如果不经过任何过滤，而通过个性化推荐的方式直接推荐给来消费内容的用户，会带来非常不好的的体验。对于那些到in来消费内容的用户而言，更希望看到的是一些优质、有趣、能吸引个人兴趣的内容。如果不对内容进行优质建模，而直接根据推荐算法为用户推荐内容，不可避免地会为用户推出质量不好的图片、话题、直播等，导致用户体验下降。</p>\n<h1 id=\"优质挖掘模型\"><a href=\"#优质挖掘模型\" class=\"headerlink\" title=\"优质挖掘模型\"></a>优质挖掘模型</h1><p><strong>为方便说明，我们以优质用户挖掘为例进行说明，对图片、话题、直播等优质挖掘模型类似，不同之处在于特征不同、以及针对具体的应用可能有对应的特殊规则处理。</strong></p>\n<h2 id=\"模型建立\"><a href=\"#模型建立\" class=\"headerlink\" title=\"模型建立\"></a>模型建立</h2><p>用户是否优质反应在多个方面。如发图在质量较高、图片等点击率高、粉丝多等。通过其中任何一个特征$i$，我们都能建立该特征和优质得分之间的关系。其中特征$i$的的取值越大，我们认为该用户越可能是优质用户，对应的优质得分越大。根据分析，我们建立模型如式1所示：</p>\n<p> $p_i = \\begin{cases}<br>1,  &amp; if x_i &gt; T_{up}\\\\<br>0,  &amp; if x_i &lt; T_{low}\\\\<br>\\frac{x_i-T_{low}}{T_{up}-T_{low}} &amp; else<br>\\end{cases} （式1）$</p>\n<p> 其对应的几何意义如图1所示，当特征值落在$T_{low}$左边时，用户为非优质用户（得分为0），落在$T_{up}$右边时，为优质用户（得分为1）；当落在$T_{low}$和$T_{up}$之间时，用户对应的得分取值为0到1之间，具体取决于其具体的位置，越靠近$T_{up}$, 其对应的得分越高。</p>\n <center> 图1: 特征与优质的关系图<br> <img src=\"/qualified_model/probability.png\" alt=\"“优质模型示意图”\"><br></center>\n\n<p><strong>通过一个特征进行优质用户的确定，往往有比较大的偏差</strong>。如有的用户A，虽然粉丝数量不多，但是其发图的点击率很高；而另一部分用户B可能粉丝数量较多，但是其图片的点击率却一般。如果只通过一个特征来进行优质用户建模，不仅容易导致个别特征异常导致准确率低，也会存在较多的用户优质无法召回。<strong>我们采用的方法是，对每个特征计算用户的优质得分，最后将多个特征得到的得分进行加权，使得模型更加鲁棒，提升优质用户挖掘的准确率和召回率。</strong> 如式2所示：</p>\n<p>$score_i=\\sum_k w_k p_{i,k} (式2)$</p>\n<p>其中$score_i$表示第i个用户的得分，$w_k $表示第$k$个特征得分对应的权重，$p_{i,k}$表示第$i$个用户的第$k$个权重的得分。<br>由式1和式2得知，我们在模型训练阶段，得到每个特征对应的阈值$T_{low}$， $T_{up}$, 以及每个特征对应的权值$w$，在预测阶段，直接根据式2计算得到用户的优质得分。</p>\n<h2 id=\"特征提取\"><a href=\"#特征提取\" class=\"headerlink\" title=\"特征提取\"></a>特征提取</h2><p>特征提取主要包括数据清洗、数据预处理、特征生成三部分。<br><strong>数据清洗:</strong>过滤日志中的非法数据，异常数据等，如日志非法数据、运营号、异常点数据等。<br><strong>数据预处理:</strong>对清洗后对数据进行预处理。如针对用户点击大于pv的情况，将pv设置为点击的值，点赞、评论等可以做类似的处理。<br><strong>特征生成:</strong>在预处理结果基础上进行特征提取，如数值特征包括发图数、pv，click、点击率、点赞数、点赞率、评论数、评论率，收藏数、收藏率、好友数、关注数量、粉丝数等；非数值特征数值化，如用户级别、是否达人等。</p>\n<h2 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h2><p><strong>样本生成:</strong><br>正样本: 筛选发优质图超过一定数量的用户,再经过人工审核最终确定正样本集合。<br>负样本: 从total集合里过滤正样本，剩余的所有样本作为负样本。严格来说，负样本中可能包括一些正样本，但对于我们所采用的模型，只要保证大部分样本都是负样本，就基本不受影响。这是因为该模型在每个特征维度，得分都是和正负样本中的特征均值相关的，即使负样本包含一定数量的正样本，其均值依然会被真正的负样本主导而不会有太大影响。 </p>\n<p><strong>模型训练</strong><br>模型训练的目的，就是通过计算得到每个特征对应的阈值$T_{low}$， $T_{up}$, 以及每个特征对应的权值$w$。训练步骤如下所示：</p>\n<p>step1: 对于所有特征，初始化$T_{i,low}$，$T_{i,up}$，其中$T_{i,low}$为所有负样本对应特征$i$的均值$u_{i,low}$，$T_{i,up}$为所有正样本对应特征$i$的均值$u_{i,up}$</p>\n<p>step2: 根据如下公式计算特征$i$的权重$w_i$.<br>       $w_i=\\frac{u_{i,up} - u_{i,low} }{u_{i,up}}$</p>\n<p>step3: 调整不同阈值T，使得准确率和召回率大于一定值，并取f1最高的阈值作为当前最优参数。</p>\n<p>step4: 调整步骤3得到的参数，允许每个参数分别向上和向下波动一定幅度，对于每种参数组合，分别执行步骤2和步骤3，根据f1指标，选择最优的参数组合，作为最终的训练结果。</p>\n<h2 id=\"优质预测和挖掘\"><a href=\"#优质预测和挖掘\" class=\"headerlink\" title=\"优质预测和挖掘\"></a>优质预测和挖掘</h2><p>对于测试用户，首先提取用户相关特征，然后利用式2进行优质得分预测。将所有得分大于阈值T的用户作为优质用户的挖掘结果。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>本文提出了一种优质用户的挖掘方法，该方法的<strong>优势在于模型简单、训练速度快；只需要将特征转化为数值特征即可（而不需要进一步处理）；同时该模型对于正负样本的构造成本较低，允许有一定的噪声样本混入而不受较大影响；允许正负样本的数量不均衡。</strong> <strong>该方法的缺点是没有考虑特征之间的关联，没有对多个特征直接进行建模，如果将多个特征之间的组合关系考虑进去，可能会有更好的效果</strong></p>\n"},{"title":"用户画像学习与总结","date":"2018-02-07T16:00:00.000Z","toc":true,"description":"用户画像学习与总结","mathjax":true,"_content":"\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n# 用户画像是什么\n\n用户画像是根据用户的社会属性及各类行为，抽象出一个标签化的用户模型。其核心工作主要包括两点：\n   \n**构建标签集：** 根据实际业务需求、平台、数据等，确定用户画像的标签集合。如针对不同需求，可能需要用户兴趣画像、年龄性别画像、人群画像、地址画像、生命周期画像等，每类用户画像都可以确定对应的标签集合。\n\n**为用户贴上标签：** 根据用户的社会属性和各类行为数据，利用机器学习模型或者相关规则，为用户贴上对应的标签。\n \n\n# 用户画像的作用\n\n通过构建用户画像，可以帮助我们更好地了解用户和产品，在个性化推荐和排序、用户精细化运营、产品分析，及辅助决策等方面，发挥很大的作用。如图1所示。\n\n\n![“为什么需要用户画像”](user_profile/function_of_user_profile.png)\n<center>图1 &ensp;用户画像的作用</center>\n\n\n+ **个性化推荐和排序**\n**个性化推荐：**通过构建用户兴趣画像，可直接用于基于内容的推荐算法中，在一定程度上解决推荐过程中的冷启动问题；另外，用户画像可用于候选召回模块，在推荐排序阶段可作为有效的特征进行使用。\n**搜索排序：**通过加入用户画像特征，能够产生更加个性化的搜索效果，提升用户体验。\n\n+ **用户精细化运营**\n通过用户画像，使得在用户运营时，只选择相关的用户进行运营，提升运营的效率，节省运营成本。如通过用户流失预测模型，得到预流失用户，可以只针对这部分用户采取挽回措施；在进行相关运营推送活动中，只针对目标用户进行推送,可以减少不必要的资源浪费和用户干扰。\n\n+ **产品分析报告**\n通过构建用户画像，有助于对产品分析，如产品人群分布，产品趋势预测等，产出相关的产品分析报告。\n\n+ **决策支持**\n通过构建用户画像，能够更加了解平台用户相关信息，有助于产品决策。\n\n+ **其它**\n另外，用户画像还可用于定向广告投放，垂直行业分析等。\n\n\n# 用户画像建模\n\n在用户画像建模时，要建立哪些维度用户画像，往往和实际的业务需求相关。如为了在推荐中解决item冷启动问题，需要建立用户内容兴趣画像、内容风格画像；为了个性化运营，需要建立人群画像、地域画像、年龄性别画像等；为挽回可能流失用户，需要建立用户生命周期画像等。\n\n## 兴趣画像\n不同的时间段，用户兴趣会有变化，针对该特点，在建立用户画像时，可以考虑建立三类兴趣画像：长期兴趣、短期兴趣和即时兴趣。其中长期兴趣是指比较长的时间内（如一年），用户表现出的持续的稳定的兴趣；短期兴趣是指用户最近一段时间内（如一个月），用户表现出的兴趣；即时兴趣是指用户当前上下文环境中（如单次会话或单次浏览），用户表现出的临时兴趣。\n\n本文我们主要针对用户长期兴趣进行建模。对于短期兴趣，可以对相关的行为权重根据时间进行衰减；对于即时兴趣，可以对当前上下文中用户的交互行为进行建模。\n\n用户兴趣画像(长期兴趣)建立的流程如图2所示：\n\n![“用户兴趣画像流程”](user_profile/user_interest_profile.png)\n<center>图2 &ensp;用户兴趣画像流程</center>\n\n+ 兴趣标签确定\n兴趣标签建立，要根据具体的业务需求，平台特点等进行构建。如对于电商平台，用户的兴趣更多的是指用户日常喜欢买的产品类别；对于资讯类平台，用户兴趣画像更多的是指资讯所属的类别、标签等；对于ins, in等图片社交平台，用户兴趣画像更多的是指图片内容或场景信息。\n\n  具体而言，兴趣标签由两部分组成：兴趣类目和详细标签。对于兴趣类目，通过借鉴相关平台的分类信息（如新浪，优酷，pinterest等），结合自身平台特点，由专业人员制定。对于详细标签，可通过爬虫、nlp等相关技术，并依靠一定的人工审核进行确定。\n\n\n+ 用户兴趣挖掘\n\n **数据准备：**主要包括基础行为日志、用户基础属性信息、item基础属性信息等。对于每个数据源，做数据清洗、数据规范化等，形成方便使用的中间数据。\n\n **特征提取和样本构建：**根据准备好的中间数据，通过用户属性信息、item属性信息、行为日志构建正负样本。提取的特征可以为：基于user维度的特征、基于label维度的特征、基于user和label的组合特征。\n user维度特征：user对应的曝光、点击、点赞、评论、发表、收藏、关注、搜索等相关特征\n label维度特征：item对应的曝光、点击、点赞、评论、发表、收藏、label类别特征、搜索等特征\n user和label组合特征：user对应label的曝光、点击、点赞、评论、发表、收藏、搜索等特征。\n 对于构建的正负样本，划分为训练集、验证集和测试集。\n\n **模型训练和预测：**对上述步骤生成的正负样本，训练GBDT/LR模型。在互网产品中，很多情况下，由于负样本比例明显高于正样本，如点击行为小于曝光，购买行为小于曝光等。此时，需要对负样本进行采样后，再训练相关模型。在模型预测时，只保留预测得分大于一定阀值的用户兴趣。\n\n **兴趣协同扩展：**仅通过模型预测阶段，存在一个潜在的问题，当用户对一个标签没发生过任何行为，或者行为次数很少时，该兴趣很可能永远不会被预测出来。一种比较好的方式是采用协同过滤，利用矩阵分解模型，得到用户和兴趣的隐语义向量，并根据在用户和兴趣在隐语义空间的内积计算用户对每个兴趣的得分。关于矩阵分解模型一般可采SGD和ALS算法进行求解，可参考[1]。\n\n+ 用户兴趣评估\n  **指标评估：**在验证集上对准确率和召回率进行评估，并通过特征优化、模型优化不断提升验证集上的准召率。对于验证集上表现最好的特征和模型，作为最终的用户画像模型，并在测试集上评估效果。\n  **用户调研：**对于预测出的用户画像结果，采用用户调研的方式进行评估。\n+ 用户兴趣画像应用\n  用户兴趣画像预测和评估通过后，可进一步用于个性化推荐、推送、个性化搜索排序等相关应用中。\n\n## 年龄性别画像\n年龄性别画像和兴趣画像构建过程比较类似，不同之处在于：年龄性别标签容易构建；年龄性别画像不需要通过矩阵分解进行标签扩展。\n\n+ 标签确定\n **性别标签：**男、女分别对应一个标签。\n **年龄标签：**根据业务需求，确定需要划分的年龄段，每个年龄段对应一个年龄标签。 \n \n+ 年龄性别挖掘\n 年龄性别画像可以像兴趣画像那样，通过GBDT/LR模型进行预测。也可以采用其它方法，如[2]采用朴素贝叶斯相关方法进行挖掘，并通过在隐语义空间寻找k近邻，根据其邻居信息来平滑标签挖掘结果。\n \n+ 年龄性别挖掘评估\n 年龄性别的评估和兴趣挖掘评估类似。但是，由于平台上用户的年龄和性别分布有时候很不均衡，对所有年龄段的用户进行评估会使得整体的评估结果受主体年龄段用户的影响很大，因此，我们采用对每个年龄段单独评估准确率，召回率和F值，再求所有类别的均值。对于性别的评估类似，对每个性别的用户单独评估，然后再求两个性别的评估均值。\n \n+ 年龄性别挖掘应用\n 年龄性别挖掘好之后，可用于推荐、排序、定向广告、个性化用户运营等。\n\n## 地域画像\n地域画像主要根据用户与地址相关的信息进行构建，相对兴趣、年龄性别画像，地域画像不需要比较复杂的模型，往往通过规则就可以得到比较合理的结果。此处我们以常住地用户画像挖掘为例进行说明。\n\n+ 标签确定\n  可以直接使用省份、城市、区、县等名称作为地域标签。\n  \n+ 地域画像挖掘\n  用户常住地相关的特征有：访问app时的gps地址，访问app时的ip对应的地址，手机号码归属地，用户注册时填写地址等,可以根据每个特征计算用户对应地址的得分，然后对各个来源的计算结果进行加权，最后根据加权结果确定用户常住地信息。\n  \n  + **用户对应某个特征的地址得分：**\n  根据特征$s$计算的用户$u$对于地址$i$的得分如式1所示:\n  $score\\_{u,s,i} = \\sum \\lambda\\_t \\; p\\_{u,s,t,i}\\;\\; (式1)$\n  其中，$p\\_{u,s,t,i}$表示用户$u$在距离当前日期的第$t$天，对应特征$s$和地址$i$访问app的次数， $\\lambda\\_t$为时间衰减系数，可以按照如下式2和式3进行计算。\n  $\\lambda\\_t= \\begin{cases}\n1-t/T\\,  & t\\leq T\\\\\\\\\n0,  & t>T\n\\end{cases}\n\\;\\;(式2)$\n  $\\lambda\\_t=\\lambda\\_0 \\alpha^t\\;\\;(式3)$\n  其中$\\alpha$为0到1之间的数值。\n  \n  + **容和各特征s对应地址得分：**\n  $score\\_{u,i}=\\sum score\\_{u,s,i}\\;w\\_s\\;attr\\_i$\n  其中$w\\_s$是根据特征重要性设定的权重，$attr\\_i$是属性$i$对应的权重，如可以根据地址是否够详细，设置相对应的的权重。地址越详细，权重越高。\n  \n  + **确定用户常住地**\n  对于所有地址得分从高到低排序，得到列表$L$。然后从高到低开始，如果连续两个地址得分相差不大，则将当前地址加入常住地集合$S$，继续向后遍历列表；否则停止遍历。此时列列表$S$中所有地址为用户的所有常住地。\n\n+ 地域画像评估\n  地域信息的挖掘结果，来自和标签强相关的特征统计，准确率比较高。在评估时，可以将某一项特征的统计结果作为标签，衡量根据其它特征挖掘得到的结果的准确率和召回率。\n  \n+ 地域画像应用\n  用户地域画像可用于精细化用户运营，定向广告等相关场景。\n\n## 生命周期画像\n用户生命周期画像，对于公司了解产品趋势，分析产品对用户的粘性具有重要作用。同时，可针对不同生命周期的用户采用不同的运营方案，提升运营效果，减少对用户的干扰。如针对初期用户进行功能的教育引导，对流失用户进行唤醒等。另外我们还可以针对非流失用户，建立流失预测模型，进行流失预警。\n\n### 生命周期画像划分\n由于app功能、用户使用频次、使用需求等不同，其对应生命周期划分也不完全一致。可采用如下方法来划分用户的生命周期：\n\n+ 得到留存用户\n  根据所有用户注册后，$N$天后是否还在使用app，得到$N$天后的留存用户集合S。一般$N$可以根据app性质设定，原则是$N$天后用户还在使用app，证明该用户已经习惯使用该app。  \n\n+ 留存用户使用频次趋势统计\n  对留存用户，统计从注册后到稳定使用app之间，平均每个时间周期$T$的登录频次。综合所有用户情况，得到每个时间周期$T$的平均使用频次。其中时间周期$T$根据app性质进行设定。\n\n+ 标签确定及划分标注\n  根据每个时间周期$T$的登录频次变化趋势，确定生命周期的标签及划分标准。\n  + 标签确定\n    我们根据登录趋势变化，确定生命周期标准。如图3所示(虚拟数据)，在第一个到第二个周期$T$用户登录频次变化非常明明显，从第二个到第八个周期用户登录频次变化相对较小，从第八后周期之后用户登录频次几本趋于稳定。基于此，我们可以确定划分标准为**初级用户、成长期用户、稳定期用户和流失用户**。其中初期用户对应登录频次较高，成长期用户对应登录频次逐渐降低，稳定期用户是登录频次趋于稳定，流失用户是$N$天后不再登录app的用户。\n   \n  ![“用户登录频次变化”](user_profile/life_recycle.png)\n<center>图3 &ensp;用户登录频次变化</center>\n\n  + 划分标准\n    对于流失用户，明确为为$N$天后不再登录的用户。对于其它三个标签划分，一种比较直观的划分是：根据图3的两个箭头s1和s2作为分界点，s1划分初级用户和成长期用户，s2划分成长期用户和稳定期用户。但是由于不同用户使用频率存在有较大差别，会导致划分不合理的情况。如有的用户可能只使用过很少的次数就进入成熟期。因此，我们可以使用的方法是：**将对应周期结束时的登录总次数作为划分标准**。我们假定$f$表示用户当前使用app次数，具体划分标准如下所示：\n    **初期用户：** $f \\lt 12$\n    **成长期用户：** $12\\leq f \\leq 63$， 63为前7个周期$T$的总登录次数。\n    **稳定期用户：** $f \\gt 63$\n  \n    \n### 流失预测\n用户的各个生命周期之间存在转换关系，如图5所示：初期用户、成长期用户、稳定期用户。如果$N$天未登录，会变成流失用户。流失用户重新登录，则变为初期用户。此处，我们认为如果用户$N$天未来，对于app有些新功能没有使用过，需要引导教育。因此，该类用户被划分为初期用户。\n\n  ![“用户生命周期转换”](user_profile/life_recycle_conversion.png)\n<center>图4 &ensp;用户生命周期转换</center>\n\n\n用户流失预测模型和兴趣画像构建类似，不同之处在于：标签易确定；不需要矩阵分解进行标签扩展；需要采用较多的时间序列相关特征。\n\n+ 标签确定\n  对于用户流失预测模型，主要包括2个标签:预警用户和正常用户\n  \n+ 模型训练和预测\n  模型的训练和预测和兴趣画像基本类似，主要是使用的特征不同、不需要进行标签扩展。\n  + 特征提取\n    用户使用app频率、频率的变化趋势、使用功能等，都和用户是否将要流失有很大的关系。我们可以提取用户每个周期的登录频次、间隔的1-N个周期登录频次差值、每个周期使用的各个功能模块的频次、间隔1－N个周期各个功能模块使用频次的差值等，作为流失预测模型的特征。\n  + 模型训练和预测\n    用户在平台上注册的时间不同，如有的用户注册只有不到一周，有的用户可能超过几年。针对这种情况，可以针对注册时间不同的用户，划分为多个类别，同一个类别的用户注册日期相近。这样，对于注册时间距离当前时间较近（如不到1周）的用户，可以采用其所有时间段的行为进行训练和预测。对于注册时间距离当前时间较长（如超过2个月）的用户，可以采用最近一段时间（如最近2个月）的行为进行训练和预测。\n  \n+ 模型评估\n  对于训练好的模型，在测试集上评估模型的准确率、召回率和F值\n+ 模型应用\n  针对处于未流失用户，通过流失预测模型，可以有效地找出即将流失的用户，从而进行用户预警，通过相关运营手段防止其变为流失用户。\n\n\n\n## 其它画像\n除了上述画像之外，还可以从很多的维度去描述用户，并为之建立画像。如人群画像、功能偏好画像等，p图相关画像（如贴纸风格画像、滤镜画像等）。针对社交网络，可以有消费兴趣画像、生产兴趣画像等。各类画像的挖掘方法很多，本文描述了自己学习过程中使用的一些方法，希望能对大家有用。\n\n# 参考资料\n\n[1] iamhere1, “矩阵分解模型的分布式求解”, 2018.01, https：//iamhere1.github.io/2018/01/03/mf/\n[2] Hu J, Zeng H J, Li H, et al. \"Demographic prediction based on user's browsing behavior\", International Conference on World Wide Web, 2007.05, pp.151-160.\n\n\n\n\n","source":"_posts/user_profile.md","raw":"---\ntitle: 用户画像学习与总结\ndate: 2018-02-08\ntoc: true\ncategories: 用户画像\ntags: [用户画像,兴趣挖掘,年龄性别挖掘,常住地挖掘,生命周期画像,流失预测模型]\ndescription: 用户画像学习与总结\nmathjax: true\n---\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n# 用户画像是什么\n\n用户画像是根据用户的社会属性及各类行为，抽象出一个标签化的用户模型。其核心工作主要包括两点：\n   \n**构建标签集：** 根据实际业务需求、平台、数据等，确定用户画像的标签集合。如针对不同需求，可能需要用户兴趣画像、年龄性别画像、人群画像、地址画像、生命周期画像等，每类用户画像都可以确定对应的标签集合。\n\n**为用户贴上标签：** 根据用户的社会属性和各类行为数据，利用机器学习模型或者相关规则，为用户贴上对应的标签。\n \n\n# 用户画像的作用\n\n通过构建用户画像，可以帮助我们更好地了解用户和产品，在个性化推荐和排序、用户精细化运营、产品分析，及辅助决策等方面，发挥很大的作用。如图1所示。\n\n\n![“为什么需要用户画像”](user_profile/function_of_user_profile.png)\n<center>图1 &ensp;用户画像的作用</center>\n\n\n+ **个性化推荐和排序**\n**个性化推荐：**通过构建用户兴趣画像，可直接用于基于内容的推荐算法中，在一定程度上解决推荐过程中的冷启动问题；另外，用户画像可用于候选召回模块，在推荐排序阶段可作为有效的特征进行使用。\n**搜索排序：**通过加入用户画像特征，能够产生更加个性化的搜索效果，提升用户体验。\n\n+ **用户精细化运营**\n通过用户画像，使得在用户运营时，只选择相关的用户进行运营，提升运营的效率，节省运营成本。如通过用户流失预测模型，得到预流失用户，可以只针对这部分用户采取挽回措施；在进行相关运营推送活动中，只针对目标用户进行推送,可以减少不必要的资源浪费和用户干扰。\n\n+ **产品分析报告**\n通过构建用户画像，有助于对产品分析，如产品人群分布，产品趋势预测等，产出相关的产品分析报告。\n\n+ **决策支持**\n通过构建用户画像，能够更加了解平台用户相关信息，有助于产品决策。\n\n+ **其它**\n另外，用户画像还可用于定向广告投放，垂直行业分析等。\n\n\n# 用户画像建模\n\n在用户画像建模时，要建立哪些维度用户画像，往往和实际的业务需求相关。如为了在推荐中解决item冷启动问题，需要建立用户内容兴趣画像、内容风格画像；为了个性化运营，需要建立人群画像、地域画像、年龄性别画像等；为挽回可能流失用户，需要建立用户生命周期画像等。\n\n## 兴趣画像\n不同的时间段，用户兴趣会有变化，针对该特点，在建立用户画像时，可以考虑建立三类兴趣画像：长期兴趣、短期兴趣和即时兴趣。其中长期兴趣是指比较长的时间内（如一年），用户表现出的持续的稳定的兴趣；短期兴趣是指用户最近一段时间内（如一个月），用户表现出的兴趣；即时兴趣是指用户当前上下文环境中（如单次会话或单次浏览），用户表现出的临时兴趣。\n\n本文我们主要针对用户长期兴趣进行建模。对于短期兴趣，可以对相关的行为权重根据时间进行衰减；对于即时兴趣，可以对当前上下文中用户的交互行为进行建模。\n\n用户兴趣画像(长期兴趣)建立的流程如图2所示：\n\n![“用户兴趣画像流程”](user_profile/user_interest_profile.png)\n<center>图2 &ensp;用户兴趣画像流程</center>\n\n+ 兴趣标签确定\n兴趣标签建立，要根据具体的业务需求，平台特点等进行构建。如对于电商平台，用户的兴趣更多的是指用户日常喜欢买的产品类别；对于资讯类平台，用户兴趣画像更多的是指资讯所属的类别、标签等；对于ins, in等图片社交平台，用户兴趣画像更多的是指图片内容或场景信息。\n\n  具体而言，兴趣标签由两部分组成：兴趣类目和详细标签。对于兴趣类目，通过借鉴相关平台的分类信息（如新浪，优酷，pinterest等），结合自身平台特点，由专业人员制定。对于详细标签，可通过爬虫、nlp等相关技术，并依靠一定的人工审核进行确定。\n\n\n+ 用户兴趣挖掘\n\n **数据准备：**主要包括基础行为日志、用户基础属性信息、item基础属性信息等。对于每个数据源，做数据清洗、数据规范化等，形成方便使用的中间数据。\n\n **特征提取和样本构建：**根据准备好的中间数据，通过用户属性信息、item属性信息、行为日志构建正负样本。提取的特征可以为：基于user维度的特征、基于label维度的特征、基于user和label的组合特征。\n user维度特征：user对应的曝光、点击、点赞、评论、发表、收藏、关注、搜索等相关特征\n label维度特征：item对应的曝光、点击、点赞、评论、发表、收藏、label类别特征、搜索等特征\n user和label组合特征：user对应label的曝光、点击、点赞、评论、发表、收藏、搜索等特征。\n 对于构建的正负样本，划分为训练集、验证集和测试集。\n\n **模型训练和预测：**对上述步骤生成的正负样本，训练GBDT/LR模型。在互网产品中，很多情况下，由于负样本比例明显高于正样本，如点击行为小于曝光，购买行为小于曝光等。此时，需要对负样本进行采样后，再训练相关模型。在模型预测时，只保留预测得分大于一定阀值的用户兴趣。\n\n **兴趣协同扩展：**仅通过模型预测阶段，存在一个潜在的问题，当用户对一个标签没发生过任何行为，或者行为次数很少时，该兴趣很可能永远不会被预测出来。一种比较好的方式是采用协同过滤，利用矩阵分解模型，得到用户和兴趣的隐语义向量，并根据在用户和兴趣在隐语义空间的内积计算用户对每个兴趣的得分。关于矩阵分解模型一般可采SGD和ALS算法进行求解，可参考[1]。\n\n+ 用户兴趣评估\n  **指标评估：**在验证集上对准确率和召回率进行评估，并通过特征优化、模型优化不断提升验证集上的准召率。对于验证集上表现最好的特征和模型，作为最终的用户画像模型，并在测试集上评估效果。\n  **用户调研：**对于预测出的用户画像结果，采用用户调研的方式进行评估。\n+ 用户兴趣画像应用\n  用户兴趣画像预测和评估通过后，可进一步用于个性化推荐、推送、个性化搜索排序等相关应用中。\n\n## 年龄性别画像\n年龄性别画像和兴趣画像构建过程比较类似，不同之处在于：年龄性别标签容易构建；年龄性别画像不需要通过矩阵分解进行标签扩展。\n\n+ 标签确定\n **性别标签：**男、女分别对应一个标签。\n **年龄标签：**根据业务需求，确定需要划分的年龄段，每个年龄段对应一个年龄标签。 \n \n+ 年龄性别挖掘\n 年龄性别画像可以像兴趣画像那样，通过GBDT/LR模型进行预测。也可以采用其它方法，如[2]采用朴素贝叶斯相关方法进行挖掘，并通过在隐语义空间寻找k近邻，根据其邻居信息来平滑标签挖掘结果。\n \n+ 年龄性别挖掘评估\n 年龄性别的评估和兴趣挖掘评估类似。但是，由于平台上用户的年龄和性别分布有时候很不均衡，对所有年龄段的用户进行评估会使得整体的评估结果受主体年龄段用户的影响很大，因此，我们采用对每个年龄段单独评估准确率，召回率和F值，再求所有类别的均值。对于性别的评估类似，对每个性别的用户单独评估，然后再求两个性别的评估均值。\n \n+ 年龄性别挖掘应用\n 年龄性别挖掘好之后，可用于推荐、排序、定向广告、个性化用户运营等。\n\n## 地域画像\n地域画像主要根据用户与地址相关的信息进行构建，相对兴趣、年龄性别画像，地域画像不需要比较复杂的模型，往往通过规则就可以得到比较合理的结果。此处我们以常住地用户画像挖掘为例进行说明。\n\n+ 标签确定\n  可以直接使用省份、城市、区、县等名称作为地域标签。\n  \n+ 地域画像挖掘\n  用户常住地相关的特征有：访问app时的gps地址，访问app时的ip对应的地址，手机号码归属地，用户注册时填写地址等,可以根据每个特征计算用户对应地址的得分，然后对各个来源的计算结果进行加权，最后根据加权结果确定用户常住地信息。\n  \n  + **用户对应某个特征的地址得分：**\n  根据特征$s$计算的用户$u$对于地址$i$的得分如式1所示:\n  $score\\_{u,s,i} = \\sum \\lambda\\_t \\; p\\_{u,s,t,i}\\;\\; (式1)$\n  其中，$p\\_{u,s,t,i}$表示用户$u$在距离当前日期的第$t$天，对应特征$s$和地址$i$访问app的次数， $\\lambda\\_t$为时间衰减系数，可以按照如下式2和式3进行计算。\n  $\\lambda\\_t= \\begin{cases}\n1-t/T\\,  & t\\leq T\\\\\\\\\n0,  & t>T\n\\end{cases}\n\\;\\;(式2)$\n  $\\lambda\\_t=\\lambda\\_0 \\alpha^t\\;\\;(式3)$\n  其中$\\alpha$为0到1之间的数值。\n  \n  + **容和各特征s对应地址得分：**\n  $score\\_{u,i}=\\sum score\\_{u,s,i}\\;w\\_s\\;attr\\_i$\n  其中$w\\_s$是根据特征重要性设定的权重，$attr\\_i$是属性$i$对应的权重，如可以根据地址是否够详细，设置相对应的的权重。地址越详细，权重越高。\n  \n  + **确定用户常住地**\n  对于所有地址得分从高到低排序，得到列表$L$。然后从高到低开始，如果连续两个地址得分相差不大，则将当前地址加入常住地集合$S$，继续向后遍历列表；否则停止遍历。此时列列表$S$中所有地址为用户的所有常住地。\n\n+ 地域画像评估\n  地域信息的挖掘结果，来自和标签强相关的特征统计，准确率比较高。在评估时，可以将某一项特征的统计结果作为标签，衡量根据其它特征挖掘得到的结果的准确率和召回率。\n  \n+ 地域画像应用\n  用户地域画像可用于精细化用户运营，定向广告等相关场景。\n\n## 生命周期画像\n用户生命周期画像，对于公司了解产品趋势，分析产品对用户的粘性具有重要作用。同时，可针对不同生命周期的用户采用不同的运营方案，提升运营效果，减少对用户的干扰。如针对初期用户进行功能的教育引导，对流失用户进行唤醒等。另外我们还可以针对非流失用户，建立流失预测模型，进行流失预警。\n\n### 生命周期画像划分\n由于app功能、用户使用频次、使用需求等不同，其对应生命周期划分也不完全一致。可采用如下方法来划分用户的生命周期：\n\n+ 得到留存用户\n  根据所有用户注册后，$N$天后是否还在使用app，得到$N$天后的留存用户集合S。一般$N$可以根据app性质设定，原则是$N$天后用户还在使用app，证明该用户已经习惯使用该app。  \n\n+ 留存用户使用频次趋势统计\n  对留存用户，统计从注册后到稳定使用app之间，平均每个时间周期$T$的登录频次。综合所有用户情况，得到每个时间周期$T$的平均使用频次。其中时间周期$T$根据app性质进行设定。\n\n+ 标签确定及划分标注\n  根据每个时间周期$T$的登录频次变化趋势，确定生命周期的标签及划分标准。\n  + 标签确定\n    我们根据登录趋势变化，确定生命周期标准。如图3所示(虚拟数据)，在第一个到第二个周期$T$用户登录频次变化非常明明显，从第二个到第八个周期用户登录频次变化相对较小，从第八后周期之后用户登录频次几本趋于稳定。基于此，我们可以确定划分标准为**初级用户、成长期用户、稳定期用户和流失用户**。其中初期用户对应登录频次较高，成长期用户对应登录频次逐渐降低，稳定期用户是登录频次趋于稳定，流失用户是$N$天后不再登录app的用户。\n   \n  ![“用户登录频次变化”](user_profile/life_recycle.png)\n<center>图3 &ensp;用户登录频次变化</center>\n\n  + 划分标准\n    对于流失用户，明确为为$N$天后不再登录的用户。对于其它三个标签划分，一种比较直观的划分是：根据图3的两个箭头s1和s2作为分界点，s1划分初级用户和成长期用户，s2划分成长期用户和稳定期用户。但是由于不同用户使用频率存在有较大差别，会导致划分不合理的情况。如有的用户可能只使用过很少的次数就进入成熟期。因此，我们可以使用的方法是：**将对应周期结束时的登录总次数作为划分标准**。我们假定$f$表示用户当前使用app次数，具体划分标准如下所示：\n    **初期用户：** $f \\lt 12$\n    **成长期用户：** $12\\leq f \\leq 63$， 63为前7个周期$T$的总登录次数。\n    **稳定期用户：** $f \\gt 63$\n  \n    \n### 流失预测\n用户的各个生命周期之间存在转换关系，如图5所示：初期用户、成长期用户、稳定期用户。如果$N$天未登录，会变成流失用户。流失用户重新登录，则变为初期用户。此处，我们认为如果用户$N$天未来，对于app有些新功能没有使用过，需要引导教育。因此，该类用户被划分为初期用户。\n\n  ![“用户生命周期转换”](user_profile/life_recycle_conversion.png)\n<center>图4 &ensp;用户生命周期转换</center>\n\n\n用户流失预测模型和兴趣画像构建类似，不同之处在于：标签易确定；不需要矩阵分解进行标签扩展；需要采用较多的时间序列相关特征。\n\n+ 标签确定\n  对于用户流失预测模型，主要包括2个标签:预警用户和正常用户\n  \n+ 模型训练和预测\n  模型的训练和预测和兴趣画像基本类似，主要是使用的特征不同、不需要进行标签扩展。\n  + 特征提取\n    用户使用app频率、频率的变化趋势、使用功能等，都和用户是否将要流失有很大的关系。我们可以提取用户每个周期的登录频次、间隔的1-N个周期登录频次差值、每个周期使用的各个功能模块的频次、间隔1－N个周期各个功能模块使用频次的差值等，作为流失预测模型的特征。\n  + 模型训练和预测\n    用户在平台上注册的时间不同，如有的用户注册只有不到一周，有的用户可能超过几年。针对这种情况，可以针对注册时间不同的用户，划分为多个类别，同一个类别的用户注册日期相近。这样，对于注册时间距离当前时间较近（如不到1周）的用户，可以采用其所有时间段的行为进行训练和预测。对于注册时间距离当前时间较长（如超过2个月）的用户，可以采用最近一段时间（如最近2个月）的行为进行训练和预测。\n  \n+ 模型评估\n  对于训练好的模型，在测试集上评估模型的准确率、召回率和F值\n+ 模型应用\n  针对处于未流失用户，通过流失预测模型，可以有效地找出即将流失的用户，从而进行用户预警，通过相关运营手段防止其变为流失用户。\n\n\n\n## 其它画像\n除了上述画像之外，还可以从很多的维度去描述用户，并为之建立画像。如人群画像、功能偏好画像等，p图相关画像（如贴纸风格画像、滤镜画像等）。针对社交网络，可以有消费兴趣画像、生产兴趣画像等。各类画像的挖掘方法很多，本文描述了自己学习过程中使用的一些方法，希望能对大家有用。\n\n# 参考资料\n\n[1] iamhere1, “矩阵分解模型的分布式求解”, 2018.01, https：//iamhere1.github.io/2018/01/03/mf/\n[2] Hu J, Zeng H J, Li H, et al. \"Demographic prediction based on user's browsing behavior\", International Conference on World Wide Web, 2007.05, pp.151-160.\n\n\n\n\n","slug":"user_profile","published":1,"updated":"2018-02-11T08:33:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjdikguhn001qga01k24kjnr4","content":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<h1 id=\"用户画像是什么\"><a href=\"#用户画像是什么\" class=\"headerlink\" title=\"用户画像是什么\"></a>用户画像是什么</h1><p>用户画像是根据用户的社会属性及各类行为，抽象出一个标签化的用户模型。其核心工作主要包括两点：</p>\n<p><strong>构建标签集：</strong> 根据实际业务需求、平台、数据等，确定用户画像的标签集合。如针对不同需求，可能需要用户兴趣画像、年龄性别画像、人群画像、地址画像、生命周期画像等，每类用户画像都可以确定对应的标签集合。</p>\n<p><strong>为用户贴上标签：</strong> 根据用户的社会属性和各类行为数据，利用机器学习模型或者相关规则，为用户贴上对应的标签。</p>\n<h1 id=\"用户画像的作用\"><a href=\"#用户画像的作用\" class=\"headerlink\" title=\"用户画像的作用\"></a>用户画像的作用</h1><p>通过构建用户画像，可以帮助我们更好地了解用户和产品，在个性化推荐和排序、用户精细化运营、产品分析，及辅助决策等方面，发挥很大的作用。如图1所示。</p>\n<p><img src=\"/2018/02/08/user_profile/function_of_user_profile.png\" alt=\"“为什么需要用户画像”\"></p>\n<center>图1 &ensp;用户画像的作用</center>\n\n\n<ul>\n<li><p><strong>个性化推荐和排序</strong><br><strong>个性化推荐：</strong>通过构建用户兴趣画像，可直接用于基于内容的推荐算法中，在一定程度上解决推荐过程中的冷启动问题；另外，用户画像可用于候选召回模块，在推荐排序阶段可作为有效的特征进行使用。<br><strong>搜索排序：</strong>通过加入用户画像特征，能够产生更加个性化的搜索效果，提升用户体验。</p>\n</li>\n<li><p><strong>用户精细化运营</strong><br>通过用户画像，使得在用户运营时，只选择相关的用户进行运营，提升运营的效率，节省运营成本。如通过用户流失预测模型，得到预流失用户，可以只针对这部分用户采取挽回措施；在进行相关运营推送活动中，只针对目标用户进行推送,可以减少不必要的资源浪费和用户干扰。</p>\n</li>\n<li><p><strong>产品分析报告</strong><br>通过构建用户画像，有助于对产品分析，如产品人群分布，产品趋势预测等，产出相关的产品分析报告。</p>\n</li>\n<li><p><strong>决策支持</strong><br>通过构建用户画像，能够更加了解平台用户相关信息，有助于产品决策。</p>\n</li>\n<li><p><strong>其它</strong><br>另外，用户画像还可用于定向广告投放，垂直行业分析等。</p>\n</li>\n</ul>\n<h1 id=\"用户画像建模\"><a href=\"#用户画像建模\" class=\"headerlink\" title=\"用户画像建模\"></a>用户画像建模</h1><p>在用户画像建模时，要建立哪些维度用户画像，往往和实际的业务需求相关。如为了在推荐中解决item冷启动问题，需要建立用户内容兴趣画像、内容风格画像；为了个性化运营，需要建立人群画像、地域画像、年龄性别画像等；为挽回可能流失用户，需要建立用户生命周期画像等。</p>\n<h2 id=\"兴趣画像\"><a href=\"#兴趣画像\" class=\"headerlink\" title=\"兴趣画像\"></a>兴趣画像</h2><p>不同的时间段，用户兴趣会有变化，针对该特点，在建立用户画像时，可以考虑建立三类兴趣画像：长期兴趣、短期兴趣和即时兴趣。其中长期兴趣是指比较长的时间内（如一年），用户表现出的持续的稳定的兴趣；短期兴趣是指用户最近一段时间内（如一个月），用户表现出的兴趣；即时兴趣是指用户当前上下文环境中（如单次会话或单次浏览），用户表现出的临时兴趣。</p>\n<p>本文我们主要针对用户长期兴趣进行建模。对于短期兴趣，可以对相关的行为权重根据时间进行衰减；对于即时兴趣，可以对当前上下文中用户的交互行为进行建模。</p>\n<p>用户兴趣画像(长期兴趣)建立的流程如图2所示：</p>\n<p><img src=\"/2018/02/08/user_profile/user_interest_profile.png\" alt=\"“用户兴趣画像流程”\"></p>\n<center>图2 &ensp;用户兴趣画像流程</center>\n\n<ul>\n<li><p>兴趣标签确定<br>兴趣标签建立，要根据具体的业务需求，平台特点等进行构建。如对于电商平台，用户的兴趣更多的是指用户日常喜欢买的产品类别；对于资讯类平台，用户兴趣画像更多的是指资讯所属的类别、标签等；对于ins, in等图片社交平台，用户兴趣画像更多的是指图片内容或场景信息。</p>\n<p>具体而言，兴趣标签由两部分组成：兴趣类目和详细标签。对于兴趣类目，通过借鉴相关平台的分类信息（如新浪，优酷，pinterest等），结合自身平台特点，由专业人员制定。对于详细标签，可通过爬虫、nlp等相关技术，并依靠一定的人工审核进行确定。</p>\n</li>\n</ul>\n<ul>\n<li><p>用户兴趣挖掘</p>\n<p><strong>数据准备：</strong>主要包括基础行为日志、用户基础属性信息、item基础属性信息等。对于每个数据源，做数据清洗、数据规范化等，形成方便使用的中间数据。</p>\n<p><strong>特征提取和样本构建：</strong>根据准备好的中间数据，通过用户属性信息、item属性信息、行为日志构建正负样本。提取的特征可以为：基于user维度的特征、基于label维度的特征、基于user和label的组合特征。<br>user维度特征：user对应的曝光、点击、点赞、评论、发表、收藏、关注、搜索等相关特征<br>label维度特征：item对应的曝光、点击、点赞、评论、发表、收藏、label类别特征、搜索等特征<br>user和label组合特征：user对应label的曝光、点击、点赞、评论、发表、收藏、搜索等特征。<br>对于构建的正负样本，划分为训练集、验证集和测试集。</p>\n<p><strong>模型训练和预测：</strong>对上述步骤生成的正负样本，训练GBDT/LR模型。在互网产品中，很多情况下，由于负样本比例明显高于正样本，如点击行为小于曝光，购买行为小于曝光等。此时，需要对负样本进行采样后，再训练相关模型。在模型预测时，只保留预测得分大于一定阀值的用户兴趣。</p>\n<p><strong>兴趣协同扩展：</strong>仅通过模型预测阶段，存在一个潜在的问题，当用户对一个标签没发生过任何行为，或者行为次数很少时，该兴趣很可能永远不会被预测出来。一种比较好的方式是采用协同过滤，利用矩阵分解模型，得到用户和兴趣的隐语义向量，并根据在用户和兴趣在隐语义空间的内积计算用户对每个兴趣的得分。关于矩阵分解模型一般可采SGD和ALS算法进行求解，可参考[1]。</p>\n</li>\n<li><p>用户兴趣评估<br><strong>指标评估：</strong>在验证集上对准确率和召回率进行评估，并通过特征优化、模型优化不断提升验证集上的准召率。对于验证集上表现最好的特征和模型，作为最终的用户画像模型，并在测试集上评估效果。<br><strong>用户调研：</strong>对于预测出的用户画像结果，采用用户调研的方式进行评估。</p>\n</li>\n<li>用户兴趣画像应用<br>用户兴趣画像预测和评估通过后，可进一步用于个性化推荐、推送、个性化搜索排序等相关应用中。</li>\n</ul>\n<h2 id=\"年龄性别画像\"><a href=\"#年龄性别画像\" class=\"headerlink\" title=\"年龄性别画像\"></a>年龄性别画像</h2><p>年龄性别画像和兴趣画像构建过程比较类似，不同之处在于：年龄性别标签容易构建；年龄性别画像不需要通过矩阵分解进行标签扩展。</p>\n<ul>\n<li><p>标签确定<br><strong>性别标签：</strong>男、女分别对应一个标签。<br><strong>年龄标签：</strong>根据业务需求，确定需要划分的年龄段，每个年龄段对应一个年龄标签。 </p>\n</li>\n<li><p>年龄性别挖掘<br>年龄性别画像可以像兴趣画像那样，通过GBDT/LR模型进行预测。也可以采用其它方法，如[2]采用朴素贝叶斯相关方法进行挖掘，并通过在隐语义空间寻找k近邻，根据其邻居信息来平滑标签挖掘结果。</p>\n</li>\n<li><p>年龄性别挖掘评估<br>年龄性别的评估和兴趣挖掘评估类似。但是，由于平台上用户的年龄和性别分布有时候很不均衡，对所有年龄段的用户进行评估会使得整体的评估结果受主体年龄段用户的影响很大，因此，我们采用对每个年龄段单独评估准确率，召回率和F值，再求所有类别的均值。对于性别的评估类似，对每个性别的用户单独评估，然后再求两个性别的评估均值。</p>\n</li>\n<li><p>年龄性别挖掘应用<br>年龄性别挖掘好之后，可用于推荐、排序、定向广告、个性化用户运营等。</p>\n</li>\n</ul>\n<h2 id=\"地域画像\"><a href=\"#地域画像\" class=\"headerlink\" title=\"地域画像\"></a>地域画像</h2><p>地域画像主要根据用户与地址相关的信息进行构建，相对兴趣、年龄性别画像，地域画像不需要比较复杂的模型，往往通过规则就可以得到比较合理的结果。此处我们以常住地用户画像挖掘为例进行说明。</p>\n<ul>\n<li><p>标签确定<br>可以直接使用省份、城市、区、县等名称作为地域标签。</p>\n</li>\n<li><p>地域画像挖掘<br>用户常住地相关的特征有：访问app时的gps地址，访问app时的ip对应的地址，手机号码归属地，用户注册时填写地址等,可以根据每个特征计算用户对应地址的得分，然后对各个来源的计算结果进行加权，最后根据加权结果确定用户常住地信息。</p>\n<ul>\n<li><p><strong>用户对应某个特征的地址得分：</strong><br>根据特征$s$计算的用户$u$对于地址$i$的得分如式1所示:<br>$score_{u,s,i} = \\sum \\lambda_t \\; p_{u,s,t,i}\\;\\; (式1)$<br>其中，$p_{u,s,t,i}$表示用户$u$在距离当前日期的第$t$天，对应特征$s$和地址$i$访问app的次数， $\\lambda_t$为时间衰减系数，可以按照如下式2和式3进行计算。<br>$\\lambda_t= \\begin{cases}<br>1-t/T\\,  &amp; t\\leq T\\\\<br>0,  &amp; t&gt;T<br>\\end{cases}<br>\\;\\;(式2)$<br>$\\lambda_t=\\lambda_0 \\alpha^t\\;\\;(式3)$<br>其中$\\alpha$为0到1之间的数值。</p>\n</li>\n<li><p><strong>容和各特征s对应地址得分：</strong><br>$score_{u,i}=\\sum score_{u,s,i}\\;w_s\\;attr_i$<br>其中$w_s$是根据特征重要性设定的权重，$attr_i$是属性$i$对应的权重，如可以根据地址是否够详细，设置相对应的的权重。地址越详细，权重越高。</p>\n</li>\n<li><p><strong>确定用户常住地</strong><br>对于所有地址得分从高到低排序，得到列表$L$。然后从高到低开始，如果连续两个地址得分相差不大，则将当前地址加入常住地集合$S$，继续向后遍历列表；否则停止遍历。此时列列表$S$中所有地址为用户的所有常住地。</p>\n</li>\n</ul>\n</li>\n<li><p>地域画像评估<br>地域信息的挖掘结果，来自和标签强相关的特征统计，准确率比较高。在评估时，可以将某一项特征的统计结果作为标签，衡量根据其它特征挖掘得到的结果的准确率和召回率。</p>\n</li>\n<li><p>地域画像应用<br>用户地域画像可用于精细化用户运营，定向广告等相关场景。</p>\n</li>\n</ul>\n<h2 id=\"生命周期画像\"><a href=\"#生命周期画像\" class=\"headerlink\" title=\"生命周期画像\"></a>生命周期画像</h2><p>用户生命周期画像，对于公司了解产品趋势，分析产品对用户的粘性具有重要作用。同时，可针对不同生命周期的用户采用不同的运营方案，提升运营效果，减少对用户的干扰。如针对初期用户进行功能的教育引导，对流失用户进行唤醒等。另外我们还可以针对非流失用户，建立流失预测模型，进行流失预警。</p>\n<h3 id=\"生命周期画像划分\"><a href=\"#生命周期画像划分\" class=\"headerlink\" title=\"生命周期画像划分\"></a>生命周期画像划分</h3><p>由于app功能、用户使用频次、使用需求等不同，其对应生命周期划分也不完全一致。可采用如下方法来划分用户的生命周期：</p>\n<ul>\n<li><p>得到留存用户<br>根据所有用户注册后，$N$天后是否还在使用app，得到$N$天后的留存用户集合S。一般$N$可以根据app性质设定，原则是$N$天后用户还在使用app，证明该用户已经习惯使用该app。  </p>\n</li>\n<li><p>留存用户使用频次趋势统计<br>对留存用户，统计从注册后到稳定使用app之间，平均每个时间周期$T$的登录频次。综合所有用户情况，得到每个时间周期$T$的平均使用频次。其中时间周期$T$根据app性质进行设定。</p>\n</li>\n<li><p>标签确定及划分标注<br>根据每个时间周期$T$的登录频次变化趋势，确定生命周期的标签及划分标准。</p>\n<ul>\n<li>标签确定<br>我们根据登录趋势变化，确定生命周期标准。如图3所示(虚拟数据)，在第一个到第二个周期$T$用户登录频次变化非常明明显，从第二个到第八个周期用户登录频次变化相对较小，从第八后周期之后用户登录频次几本趋于稳定。基于此，我们可以确定划分标准为<strong>初级用户、成长期用户、稳定期用户和流失用户</strong>。其中初期用户对应登录频次较高，成长期用户对应登录频次逐渐降低，稳定期用户是登录频次趋于稳定，流失用户是$N$天后不再登录app的用户。</li>\n</ul>\n<p><img src=\"/2018/02/08/user_profile/life_recycle.png\" alt=\"“用户登录频次变化”\"></p>\n<center>图3 &ensp;用户登录频次变化</center>\n\n<ul>\n<li>划分标准<br>对于流失用户，明确为为$N$天后不再登录的用户。对于其它三个标签划分，一种比较直观的划分是：根据图3的两个箭头s1和s2作为分界点，s1划分初级用户和成长期用户，s2划分成长期用户和稳定期用户。但是由于不同用户使用频率存在有较大差别，会导致划分不合理的情况。如有的用户可能只使用过很少的次数就进入成熟期。因此，我们可以使用的方法是：<strong>将对应周期结束时的登录总次数作为划分标准</strong>。我们假定$f$表示用户当前使用app次数，具体划分标准如下所示：<br><strong>初期用户：</strong> $f \\lt 12$<br><strong>成长期用户：</strong> $12\\leq f \\leq 63$， 63为前7个周期$T$的总登录次数。<br><strong>稳定期用户：</strong> $f \\gt 63$</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"流失预测\"><a href=\"#流失预测\" class=\"headerlink\" title=\"流失预测\"></a>流失预测</h3><p>用户的各个生命周期之间存在转换关系，如图5所示：初期用户、成长期用户、稳定期用户。如果$N$天未登录，会变成流失用户。流失用户重新登录，则变为初期用户。此处，我们认为如果用户$N$天未来，对于app有些新功能没有使用过，需要引导教育。因此，该类用户被划分为初期用户。</p>\n<p>  <img src=\"/2018/02/08/user_profile/life_recycle_conversion.png\" alt=\"“用户生命周期转换”\"></p>\n<center>图4 &ensp;用户生命周期转换</center>\n\n\n<p>用户流失预测模型和兴趣画像构建类似，不同之处在于：标签易确定；不需要矩阵分解进行标签扩展；需要采用较多的时间序列相关特征。</p>\n<ul>\n<li><p>标签确定<br>对于用户流失预测模型，主要包括2个标签:预警用户和正常用户</p>\n</li>\n<li><p>模型训练和预测<br>模型的训练和预测和兴趣画像基本类似，主要是使用的特征不同、不需要进行标签扩展。</p>\n<ul>\n<li>特征提取<br>用户使用app频率、频率的变化趋势、使用功能等，都和用户是否将要流失有很大的关系。我们可以提取用户每个周期的登录频次、间隔的1-N个周期登录频次差值、每个周期使用的各个功能模块的频次、间隔1－N个周期各个功能模块使用频次的差值等，作为流失预测模型的特征。</li>\n<li>模型训练和预测<br>用户在平台上注册的时间不同，如有的用户注册只有不到一周，有的用户可能超过几年。针对这种情况，可以针对注册时间不同的用户，划分为多个类别，同一个类别的用户注册日期相近。这样，对于注册时间距离当前时间较近（如不到1周）的用户，可以采用其所有时间段的行为进行训练和预测。对于注册时间距离当前时间较长（如超过2个月）的用户，可以采用最近一段时间（如最近2个月）的行为进行训练和预测。</li>\n</ul>\n</li>\n<li><p>模型评估<br>对于训练好的模型，在测试集上评估模型的准确率、召回率和F值</p>\n</li>\n<li>模型应用<br>针对处于未流失用户，通过流失预测模型，可以有效地找出即将流失的用户，从而进行用户预警，通过相关运营手段防止其变为流失用户。</li>\n</ul>\n<h2 id=\"其它画像\"><a href=\"#其它画像\" class=\"headerlink\" title=\"其它画像\"></a>其它画像</h2><p>除了上述画像之外，还可以从很多的维度去描述用户，并为之建立画像。如人群画像、功能偏好画像等，p图相关画像（如贴纸风格画像、滤镜画像等）。针对社交网络，可以有消费兴趣画像、生产兴趣画像等。各类画像的挖掘方法很多，本文描述了自己学习过程中使用的一些方法，希望能对大家有用。</p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>[1] iamhere1, “矩阵分解模型的分布式求解”, 2018.01, https：//iamhere1.github.io/2018/01/03/mf/<br>[2] Hu J, Zeng H J, Li H, et al. “Demographic prediction based on user’s browsing behavior”, International Conference on World Wide Web, 2007.05, pp.151-160.</p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n\n<h1 id=\"用户画像是什么\"><a href=\"#用户画像是什么\" class=\"headerlink\" title=\"用户画像是什么\"></a>用户画像是什么</h1><p>用户画像是根据用户的社会属性及各类行为，抽象出一个标签化的用户模型。其核心工作主要包括两点：</p>\n<p><strong>构建标签集：</strong> 根据实际业务需求、平台、数据等，确定用户画像的标签集合。如针对不同需求，可能需要用户兴趣画像、年龄性别画像、人群画像、地址画像、生命周期画像等，每类用户画像都可以确定对应的标签集合。</p>\n<p><strong>为用户贴上标签：</strong> 根据用户的社会属性和各类行为数据，利用机器学习模型或者相关规则，为用户贴上对应的标签。</p>\n<h1 id=\"用户画像的作用\"><a href=\"#用户画像的作用\" class=\"headerlink\" title=\"用户画像的作用\"></a>用户画像的作用</h1><p>通过构建用户画像，可以帮助我们更好地了解用户和产品，在个性化推荐和排序、用户精细化运营、产品分析，及辅助决策等方面，发挥很大的作用。如图1所示。</p>\n<p><img src=\"/2018/02/08/user_profile/function_of_user_profile.png\" alt=\"“为什么需要用户画像”\"></p>\n<center>图1 &ensp;用户画像的作用</center>\n\n\n<ul>\n<li><p><strong>个性化推荐和排序</strong><br><strong>个性化推荐：</strong>通过构建用户兴趣画像，可直接用于基于内容的推荐算法中，在一定程度上解决推荐过程中的冷启动问题；另外，用户画像可用于候选召回模块，在推荐排序阶段可作为有效的特征进行使用。<br><strong>搜索排序：</strong>通过加入用户画像特征，能够产生更加个性化的搜索效果，提升用户体验。</p>\n</li>\n<li><p><strong>用户精细化运营</strong><br>通过用户画像，使得在用户运营时，只选择相关的用户进行运营，提升运营的效率，节省运营成本。如通过用户流失预测模型，得到预流失用户，可以只针对这部分用户采取挽回措施；在进行相关运营推送活动中，只针对目标用户进行推送,可以减少不必要的资源浪费和用户干扰。</p>\n</li>\n<li><p><strong>产品分析报告</strong><br>通过构建用户画像，有助于对产品分析，如产品人群分布，产品趋势预测等，产出相关的产品分析报告。</p>\n</li>\n<li><p><strong>决策支持</strong><br>通过构建用户画像，能够更加了解平台用户相关信息，有助于产品决策。</p>\n</li>\n<li><p><strong>其它</strong><br>另外，用户画像还可用于定向广告投放，垂直行业分析等。</p>\n</li>\n</ul>\n<h1 id=\"用户画像建模\"><a href=\"#用户画像建模\" class=\"headerlink\" title=\"用户画像建模\"></a>用户画像建模</h1><p>在用户画像建模时，要建立哪些维度用户画像，往往和实际的业务需求相关。如为了在推荐中解决item冷启动问题，需要建立用户内容兴趣画像、内容风格画像；为了个性化运营，需要建立人群画像、地域画像、年龄性别画像等；为挽回可能流失用户，需要建立用户生命周期画像等。</p>\n<h2 id=\"兴趣画像\"><a href=\"#兴趣画像\" class=\"headerlink\" title=\"兴趣画像\"></a>兴趣画像</h2><p>不同的时间段，用户兴趣会有变化，针对该特点，在建立用户画像时，可以考虑建立三类兴趣画像：长期兴趣、短期兴趣和即时兴趣。其中长期兴趣是指比较长的时间内（如一年），用户表现出的持续的稳定的兴趣；短期兴趣是指用户最近一段时间内（如一个月），用户表现出的兴趣；即时兴趣是指用户当前上下文环境中（如单次会话或单次浏览），用户表现出的临时兴趣。</p>\n<p>本文我们主要针对用户长期兴趣进行建模。对于短期兴趣，可以对相关的行为权重根据时间进行衰减；对于即时兴趣，可以对当前上下文中用户的交互行为进行建模。</p>\n<p>用户兴趣画像(长期兴趣)建立的流程如图2所示：</p>\n<p><img src=\"/2018/02/08/user_profile/user_interest_profile.png\" alt=\"“用户兴趣画像流程”\"></p>\n<center>图2 &ensp;用户兴趣画像流程</center>\n\n<ul>\n<li><p>兴趣标签确定<br>兴趣标签建立，要根据具体的业务需求，平台特点等进行构建。如对于电商平台，用户的兴趣更多的是指用户日常喜欢买的产品类别；对于资讯类平台，用户兴趣画像更多的是指资讯所属的类别、标签等；对于ins, in等图片社交平台，用户兴趣画像更多的是指图片内容或场景信息。</p>\n<p>具体而言，兴趣标签由两部分组成：兴趣类目和详细标签。对于兴趣类目，通过借鉴相关平台的分类信息（如新浪，优酷，pinterest等），结合自身平台特点，由专业人员制定。对于详细标签，可通过爬虫、nlp等相关技术，并依靠一定的人工审核进行确定。</p>\n</li>\n</ul>\n<ul>\n<li><p>用户兴趣挖掘</p>\n<p><strong>数据准备：</strong>主要包括基础行为日志、用户基础属性信息、item基础属性信息等。对于每个数据源，做数据清洗、数据规范化等，形成方便使用的中间数据。</p>\n<p><strong>特征提取和样本构建：</strong>根据准备好的中间数据，通过用户属性信息、item属性信息、行为日志构建正负样本。提取的特征可以为：基于user维度的特征、基于label维度的特征、基于user和label的组合特征。<br>user维度特征：user对应的曝光、点击、点赞、评论、发表、收藏、关注、搜索等相关特征<br>label维度特征：item对应的曝光、点击、点赞、评论、发表、收藏、label类别特征、搜索等特征<br>user和label组合特征：user对应label的曝光、点击、点赞、评论、发表、收藏、搜索等特征。<br>对于构建的正负样本，划分为训练集、验证集和测试集。</p>\n<p><strong>模型训练和预测：</strong>对上述步骤生成的正负样本，训练GBDT/LR模型。在互网产品中，很多情况下，由于负样本比例明显高于正样本，如点击行为小于曝光，购买行为小于曝光等。此时，需要对负样本进行采样后，再训练相关模型。在模型预测时，只保留预测得分大于一定阀值的用户兴趣。</p>\n<p><strong>兴趣协同扩展：</strong>仅通过模型预测阶段，存在一个潜在的问题，当用户对一个标签没发生过任何行为，或者行为次数很少时，该兴趣很可能永远不会被预测出来。一种比较好的方式是采用协同过滤，利用矩阵分解模型，得到用户和兴趣的隐语义向量，并根据在用户和兴趣在隐语义空间的内积计算用户对每个兴趣的得分。关于矩阵分解模型一般可采SGD和ALS算法进行求解，可参考[1]。</p>\n</li>\n<li><p>用户兴趣评估<br><strong>指标评估：</strong>在验证集上对准确率和召回率进行评估，并通过特征优化、模型优化不断提升验证集上的准召率。对于验证集上表现最好的特征和模型，作为最终的用户画像模型，并在测试集上评估效果。<br><strong>用户调研：</strong>对于预测出的用户画像结果，采用用户调研的方式进行评估。</p>\n</li>\n<li>用户兴趣画像应用<br>用户兴趣画像预测和评估通过后，可进一步用于个性化推荐、推送、个性化搜索排序等相关应用中。</li>\n</ul>\n<h2 id=\"年龄性别画像\"><a href=\"#年龄性别画像\" class=\"headerlink\" title=\"年龄性别画像\"></a>年龄性别画像</h2><p>年龄性别画像和兴趣画像构建过程比较类似，不同之处在于：年龄性别标签容易构建；年龄性别画像不需要通过矩阵分解进行标签扩展。</p>\n<ul>\n<li><p>标签确定<br><strong>性别标签：</strong>男、女分别对应一个标签。<br><strong>年龄标签：</strong>根据业务需求，确定需要划分的年龄段，每个年龄段对应一个年龄标签。 </p>\n</li>\n<li><p>年龄性别挖掘<br>年龄性别画像可以像兴趣画像那样，通过GBDT/LR模型进行预测。也可以采用其它方法，如[2]采用朴素贝叶斯相关方法进行挖掘，并通过在隐语义空间寻找k近邻，根据其邻居信息来平滑标签挖掘结果。</p>\n</li>\n<li><p>年龄性别挖掘评估<br>年龄性别的评估和兴趣挖掘评估类似。但是，由于平台上用户的年龄和性别分布有时候很不均衡，对所有年龄段的用户进行评估会使得整体的评估结果受主体年龄段用户的影响很大，因此，我们采用对每个年龄段单独评估准确率，召回率和F值，再求所有类别的均值。对于性别的评估类似，对每个性别的用户单独评估，然后再求两个性别的评估均值。</p>\n</li>\n<li><p>年龄性别挖掘应用<br>年龄性别挖掘好之后，可用于推荐、排序、定向广告、个性化用户运营等。</p>\n</li>\n</ul>\n<h2 id=\"地域画像\"><a href=\"#地域画像\" class=\"headerlink\" title=\"地域画像\"></a>地域画像</h2><p>地域画像主要根据用户与地址相关的信息进行构建，相对兴趣、年龄性别画像，地域画像不需要比较复杂的模型，往往通过规则就可以得到比较合理的结果。此处我们以常住地用户画像挖掘为例进行说明。</p>\n<ul>\n<li><p>标签确定<br>可以直接使用省份、城市、区、县等名称作为地域标签。</p>\n</li>\n<li><p>地域画像挖掘<br>用户常住地相关的特征有：访问app时的gps地址，访问app时的ip对应的地址，手机号码归属地，用户注册时填写地址等,可以根据每个特征计算用户对应地址的得分，然后对各个来源的计算结果进行加权，最后根据加权结果确定用户常住地信息。</p>\n<ul>\n<li><p><strong>用户对应某个特征的地址得分：</strong><br>根据特征$s$计算的用户$u$对于地址$i$的得分如式1所示:<br>$score_{u,s,i} = \\sum \\lambda_t \\; p_{u,s,t,i}\\;\\; (式1)$<br>其中，$p_{u,s,t,i}$表示用户$u$在距离当前日期的第$t$天，对应特征$s$和地址$i$访问app的次数， $\\lambda_t$为时间衰减系数，可以按照如下式2和式3进行计算。<br>$\\lambda_t= \\begin{cases}<br>1-t/T\\,  &amp; t\\leq T\\\\<br>0,  &amp; t&gt;T<br>\\end{cases}<br>\\;\\;(式2)$<br>$\\lambda_t=\\lambda_0 \\alpha^t\\;\\;(式3)$<br>其中$\\alpha$为0到1之间的数值。</p>\n</li>\n<li><p><strong>容和各特征s对应地址得分：</strong><br>$score_{u,i}=\\sum score_{u,s,i}\\;w_s\\;attr_i$<br>其中$w_s$是根据特征重要性设定的权重，$attr_i$是属性$i$对应的权重，如可以根据地址是否够详细，设置相对应的的权重。地址越详细，权重越高。</p>\n</li>\n<li><p><strong>确定用户常住地</strong><br>对于所有地址得分从高到低排序，得到列表$L$。然后从高到低开始，如果连续两个地址得分相差不大，则将当前地址加入常住地集合$S$，继续向后遍历列表；否则停止遍历。此时列列表$S$中所有地址为用户的所有常住地。</p>\n</li>\n</ul>\n</li>\n<li><p>地域画像评估<br>地域信息的挖掘结果，来自和标签强相关的特征统计，准确率比较高。在评估时，可以将某一项特征的统计结果作为标签，衡量根据其它特征挖掘得到的结果的准确率和召回率。</p>\n</li>\n<li><p>地域画像应用<br>用户地域画像可用于精细化用户运营，定向广告等相关场景。</p>\n</li>\n</ul>\n<h2 id=\"生命周期画像\"><a href=\"#生命周期画像\" class=\"headerlink\" title=\"生命周期画像\"></a>生命周期画像</h2><p>用户生命周期画像，对于公司了解产品趋势，分析产品对用户的粘性具有重要作用。同时，可针对不同生命周期的用户采用不同的运营方案，提升运营效果，减少对用户的干扰。如针对初期用户进行功能的教育引导，对流失用户进行唤醒等。另外我们还可以针对非流失用户，建立流失预测模型，进行流失预警。</p>\n<h3 id=\"生命周期画像划分\"><a href=\"#生命周期画像划分\" class=\"headerlink\" title=\"生命周期画像划分\"></a>生命周期画像划分</h3><p>由于app功能、用户使用频次、使用需求等不同，其对应生命周期划分也不完全一致。可采用如下方法来划分用户的生命周期：</p>\n<ul>\n<li><p>得到留存用户<br>根据所有用户注册后，$N$天后是否还在使用app，得到$N$天后的留存用户集合S。一般$N$可以根据app性质设定，原则是$N$天后用户还在使用app，证明该用户已经习惯使用该app。  </p>\n</li>\n<li><p>留存用户使用频次趋势统计<br>对留存用户，统计从注册后到稳定使用app之间，平均每个时间周期$T$的登录频次。综合所有用户情况，得到每个时间周期$T$的平均使用频次。其中时间周期$T$根据app性质进行设定。</p>\n</li>\n<li><p>标签确定及划分标注<br>根据每个时间周期$T$的登录频次变化趋势，确定生命周期的标签及划分标准。</p>\n<ul>\n<li>标签确定<br>我们根据登录趋势变化，确定生命周期标准。如图3所示(虚拟数据)，在第一个到第二个周期$T$用户登录频次变化非常明明显，从第二个到第八个周期用户登录频次变化相对较小，从第八后周期之后用户登录频次几本趋于稳定。基于此，我们可以确定划分标准为<strong>初级用户、成长期用户、稳定期用户和流失用户</strong>。其中初期用户对应登录频次较高，成长期用户对应登录频次逐渐降低，稳定期用户是登录频次趋于稳定，流失用户是$N$天后不再登录app的用户。</li>\n</ul>\n<p><img src=\"/2018/02/08/user_profile/life_recycle.png\" alt=\"“用户登录频次变化”\"></p>\n<center>图3 &ensp;用户登录频次变化</center>\n\n<ul>\n<li>划分标准<br>对于流失用户，明确为为$N$天后不再登录的用户。对于其它三个标签划分，一种比较直观的划分是：根据图3的两个箭头s1和s2作为分界点，s1划分初级用户和成长期用户，s2划分成长期用户和稳定期用户。但是由于不同用户使用频率存在有较大差别，会导致划分不合理的情况。如有的用户可能只使用过很少的次数就进入成熟期。因此，我们可以使用的方法是：<strong>将对应周期结束时的登录总次数作为划分标准</strong>。我们假定$f$表示用户当前使用app次数，具体划分标准如下所示：<br><strong>初期用户：</strong> $f \\lt 12$<br><strong>成长期用户：</strong> $12\\leq f \\leq 63$， 63为前7个周期$T$的总登录次数。<br><strong>稳定期用户：</strong> $f \\gt 63$</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"流失预测\"><a href=\"#流失预测\" class=\"headerlink\" title=\"流失预测\"></a>流失预测</h3><p>用户的各个生命周期之间存在转换关系，如图5所示：初期用户、成长期用户、稳定期用户。如果$N$天未登录，会变成流失用户。流失用户重新登录，则变为初期用户。此处，我们认为如果用户$N$天未来，对于app有些新功能没有使用过，需要引导教育。因此，该类用户被划分为初期用户。</p>\n<p>  <img src=\"/2018/02/08/user_profile/life_recycle_conversion.png\" alt=\"“用户生命周期转换”\"></p>\n<center>图4 &ensp;用户生命周期转换</center>\n\n\n<p>用户流失预测模型和兴趣画像构建类似，不同之处在于：标签易确定；不需要矩阵分解进行标签扩展；需要采用较多的时间序列相关特征。</p>\n<ul>\n<li><p>标签确定<br>对于用户流失预测模型，主要包括2个标签:预警用户和正常用户</p>\n</li>\n<li><p>模型训练和预测<br>模型的训练和预测和兴趣画像基本类似，主要是使用的特征不同、不需要进行标签扩展。</p>\n<ul>\n<li>特征提取<br>用户使用app频率、频率的变化趋势、使用功能等，都和用户是否将要流失有很大的关系。我们可以提取用户每个周期的登录频次、间隔的1-N个周期登录频次差值、每个周期使用的各个功能模块的频次、间隔1－N个周期各个功能模块使用频次的差值等，作为流失预测模型的特征。</li>\n<li>模型训练和预测<br>用户在平台上注册的时间不同，如有的用户注册只有不到一周，有的用户可能超过几年。针对这种情况，可以针对注册时间不同的用户，划分为多个类别，同一个类别的用户注册日期相近。这样，对于注册时间距离当前时间较近（如不到1周）的用户，可以采用其所有时间段的行为进行训练和预测。对于注册时间距离当前时间较长（如超过2个月）的用户，可以采用最近一段时间（如最近2个月）的行为进行训练和预测。</li>\n</ul>\n</li>\n<li><p>模型评估<br>对于训练好的模型，在测试集上评估模型的准确率、召回率和F值</p>\n</li>\n<li>模型应用<br>针对处于未流失用户，通过流失预测模型，可以有效地找出即将流失的用户，从而进行用户预警，通过相关运营手段防止其变为流失用户。</li>\n</ul>\n<h2 id=\"其它画像\"><a href=\"#其它画像\" class=\"headerlink\" title=\"其它画像\"></a>其它画像</h2><p>除了上述画像之外，还可以从很多的维度去描述用户，并为之建立画像。如人群画像、功能偏好画像等，p图相关画像（如贴纸风格画像、滤镜画像等）。针对社交网络，可以有消费兴趣画像、生产兴趣画像等。各类画像的挖掘方法很多，本文描述了自己学习过程中使用的一些方法，希望能对大家有用。</p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>[1] iamhere1, “矩阵分解模型的分布式求解”, 2018.01, https：//iamhere1.github.io/2018/01/03/mf/<br>[2] Hu J, Zeng H J, Li H, et al. “Demographic prediction based on user’s browsing behavior”, International Conference on World Wide Web, 2007.05, pp.151-160.</p>\n"},{"title":"spark mllib 决策树算法源码学习","date":"2016-12-06T16:00:00.000Z","toc":true,"description":"决策树算法源码学习，其中模型的训练部分以随机森林的训练过程进行说明，决策树相当于树的数量为1的随机森林","mathjax":true,"_content":"\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n该文章来自于2016年后半年整理的算法源码笔记，由于之前没有写博客的习惯，都直接以笔记的形式存在电脑上，分享起来非常不便，因此抽出时间，将其整理成博客的形式，和大家一起学习交流。\n\n# 决策树算法简要介绍\n\n决策树算法是一种常见的分类算法，也可以用于回归问题。相对于其他分类算法，决策树的优点在于简单,可解释性强；对特征尺度不敏感，不需要做太多的特征预处理工作;能够自动挖掘特征之间的关联关系。缺点是比较容易过拟合（通过随机森林可以避免过拟合）\n\n决策树是一个树形结构，其中叶子节点表示分类（或回归）结果，非叶子节点是属性判断判断节点，每个属性判断节点都选择样本的一个特征，并根据该特征的取值决定选择哪一个分支路径。在对样本进行预测时，从根节点开始直到叶子节点，对于路径上的每个分支节点，都根据其对应的属性取值选择下一个分支节点，直到叶子节点。整个完整的路径，表示对样本的预测过程。如图1所示，表示一个女孩在决定是否决定去相亲的一个过程，最终选择去或者不去，对应分类的结果，中间的各种条件对应相关的属性。\n\n<center>\n![“决策树样例”](/decision_tree/decision_tree_example.png)\n</center>\n<center>图1：决策树样例：对女孩决定是否参加相亲的问题进行决策树建模</center>\n \n\n## 决策树的训练\n\n从根节点开始，根据信息增益或其他条件，不断选择分裂的属性，直到生成叶子节点的过程。具体过程如下所示：\n* 对不同的属性，计算其信息增益，选择增益最大的特征对应根节点的最佳分裂。\n* 从根节点开始，对于不同的分支节点，分别选择信息增益最大的特征作为分支节点的最佳分裂。\n* 如果达到停止分裂的条件，则将该节点作为叶子节点：当前节点对应的样本都是一类样本，分类结果为对应的样本的类别；总样本数量小于一定值，或者树的高度达到最大值，或者信息增益小于一定值，或者已经用完所有的属性，选择占比最大的样本分类作为节点对应的分类结果。否则，根据步骤2进一步构造分裂节点。\n\n\n## 属性度量\n\n\n决策树构建的关键，在于不断地选择最佳分裂属性。属性的收益度量方法，常见的有信息增益（ID3算法）、信息增益率（C4.5算法），基尼系数(CART算法)等。\n\n**ID3算法:**\n\n熵：信息论中，用于描述信息的不确定性，定义如式1，其中$D$表示对样本的一个划分，$m$表示划分的类别数量，$p\\_i$表示第i个类别的样本数量比例。\n\n$info(D)=-\\sum\\_{i=1}^m p\\_ilog\\_2(p\\_i)\\;\\;\\;（式1）$\n\n假设按照属性A对样本D进行划分，$v$为属性$A$的划分数量。则$A$对$D$划分的期望熵如式2：\n\n$info\\_A(D)=\\sum\\_{j=1}^v\\frac{|D\\_j|}{|D|}info(D\\_j)\\;\\;\\;（式2）$\n\n信心增益为上述原始熵和属性A对D划分后期望熵的差值，可以看做是加入信息A后，不确定性的减少程度。信息增益的定义如式3所示：\n\n$gain(A)=info(D)-info\\_A(D)\\;\\;\\;（式3）$\n\nID3算法即在每次选择最佳分裂的属性时，根据信息增益进行选择。\n\n**C4.5算法:**\nID3算法容易使得选取值较多的属性。一种极端的情况是，对于ID类特征有很多的无意义的值的划分，ID3会选择该属性其作为最佳划分。C4.5算法通过采用信息增益率作为衡量特征有效性的指标，可以克服这个问题。\n\n首先定义分裂信息：\n$splitInfo\\_A(D)=-\\sum\\_{j=1}^v\\frac{|D\\_j|}{|D|}log\\_2(\\frac{|D\\_j|}{|D|})\\;\\;\\;（式4）$\n\n信息增益率：\n$gainRatio(A)=\\frac{gain(A)}{splitInfo\\_A(D)}\\;\\;\\;（式5）$\n\n**CART算法:**\n\n使用基尼系数作为不纯度的度量。\n基尼系数:表示在样本集合中一个随机选中的样本被分错的概率，Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。当所有样本属于一个类别时，基尼系数最小为0。所有类别以等概率出现时，基尼系数最大。\n$GINI(P)=\\sum\\_{k=1}^Kp\\_k(1-p\\_k)=1-\\sum\\_{k=1}^K p\\_k^2\\;\\;\\;（式6）$\n\n由于cart建立的树是个二叉树，所以K的取值为2。对于特征取值超过2的情况，以每个取值作为划分点，计算该划分下对应的基尼系数的期望。期望值最小的划分点，作为最佳分裂使用的特征划分。\n\n\n\n# spark 决策树源码分析\n\n为加深对ALS算法的理解，该部分主要分析spark mllib中决策树源码的实现。主要包括模型训练、模型预测2个部分\n\n##  模型训练\n\n### 决策树伴生类\n    \nDecisionTree伴随类，外部调用决策树模型进行训练的入口。通过外部传入数据和配置参数，调用DecisionTree中的run方法进行模型训练， 最终返回DecisionTreeModel类型对象。\n\n```scala\nobject DecisionTree extends Serializable with Logging {\n def train(\n      input: RDD[LabeledPoint], //训练数据，包括label和特征向量\n      algo: Algo,//决策树类型，分类树or回归树\n      impurity: Impurity,//衡量特征信息增益的标准，如信息增益、基尼、方差\n      maxDepth: Int,//树的深度\n      numClasses: Int,//待分类类别的数量\n      maxBins: Int,//用于特征分裂的bin的最大数量\n      quantileCalculationStrategy: QuantileStrategy,//计算分位数的算法\n      //离散特征存储，如n->k表示第n个特征有k个取值（0，1，..., k-1）\n      categoricalFeaturesInfo: Map[Int, Int]): DecisionTreeModel = { \n    //根据参数信息，生成决策树配置\n    val strategy = new Strategy(algo, impurity, maxDepth, numClasses, maxBins,\n      quantileCalculationStrategy, categoricalFeaturesInfo)\n    //调用DecisionTree对象的run方法，训练决策树模型\n    new DecisionTree(strategy).run(input)\n  }\n   //训练分类决策树\n   def trainClassifier(\n      input: RDD[LabeledPoint],\n      numClasses: Int,\n      categoricalFeaturesInfo: Map[Int, Int],\n      impurity: String,\n      maxDepth: Int,\n      maxBins: Int): DecisionTreeModel = {\n    val impurityType = Impurities.fromString(impurity)\n    train(input, Classification, impurityType, maxDepth, numClasses, maxBins, Sort,categoricalFeaturesInfo)\n  }\n    //训练回归决策树\n    def trainRegressor(\n      input: RDD[LabeledPoint],\n      categoricalFeaturesInfo: Map[Int, Int],\n      impurity: String,\n      maxDepth: Int,\n      maxBins: Int): DecisionTreeModel = {\n    val impurityType = Impurities.fromString(impurity) //基尼、熵、方差三种衡量标准\n    train(input, Regression, impurityType, maxDepth, 0, maxBins, Sort, categoricalFeaturesInfo)\n  }\n}\n```\n\n### 决策树类\n\n接受strategy参数初始化，并通过对run方法调用随机森林的run方法，通过设置特征集合为全集、树的个数为1，将随机森林训练后结果集中的第一棵树作为结果返回。\n\n```\nclass DecisionTree private[spark] (private val strategy: Strategy, private val seed: Int)\n  extends Serializable with Logging {\n  def run(input: RDD[LabeledPoint]): DecisionTreeModel = {\n    val rf = new RandomForest(strategy, numTrees = 1, featureSubsetStrategy = \"all\", seed = seed)\n    val rfModel = rf.run(input)\n    rfModel.trees(0)\n  }\n}\n```\n\n### RandomForest私有类run方法,通过run方法完成模型的训练\n\n**分布式训练思想：**\n\n*\t分布式存储样本\n*\t对于每次迭代，算法都会对一个node集合进行分裂。对于每个node，相关worker计算的的所有相关统计特征全部传递到某个worker进行汇总，并选择最好的特征分裂\n*\tfindSplitsBins方法可用于将连续特征离散化，在初始化阶段完成\n*\t迭代算法\n   每次都作用于树的边缘节点，如果是随机森林，则选择所有的树的边缘节点。具体迭代步骤如下：\n   1. Master 节点: 从node queue中选取节点，如果训练的是随机森林,且featureSubsetStrategy取值不是all，则对于每个节点选择随机特征子集。selectNodesToSplit用于选择待分裂的节点。\n   2. Worer节点: findBestSplits函数，对每个(tree, node, feature, split)，遍历所有本地所有样本计算相关特征，计算结果通过reduceByKey传递给某个节点，由该节点汇总数据，得到(feature, split)或者判断是否停止分裂\n   3. Master节点: 收集所有节点分裂信息，更新model, 并将新的model传递给各个worker节点 \n\n#### \n```\ndef run(\n      input: RDD[LabeledPoint],\n      strategy: OldStrategy,\n      numTrees: Int,\n      featureSubsetStrategy: String,\n      seed: Long,\n      instr: Option[Instrumentation[_]],\n      parentUID: Option[String] = None): Array[DecisionTreeModel] = {\n    val timer = new TimeTracker()\n    timer.start(\"total\")\n    timer.start(\"init\")\n    \n    val retaggedInput = input.retag(classOf[LabeledPoint])\n    //构建元数据\n    val metadata =\n      DecisionTreeMetadata.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)\n    instr match {\n      case Some(instrumentation) =>\n        instrumentation.logNumFeatures(metadata.numFeatures)\n        instrumentation.logNumClasses(metadata.numClasses)\n      case None =>\n        logInfo(\"numFeatures: \" + metadata.numFeatures)\n        logInfo(\"numClasses: \" + metadata.numClasses)\n    }\n\n\n    //每个特征对应的splits和bins\n    timer.start(\"findSplits\")\n    val splits = findSplits(retaggedInput, metadata, seed)\n    timer.stop(\"findSplits\")\n    logDebug(\"numBins: feature: number of bins\")\n    logDebug(Range(0, metadata.numFeatures).map { featureIndex =>\n      s\"\\t$featureIndex\\t${metadata.numBins(featureIndex)}\"\n    }.mkString(\"\\n\"))\n\n    // Bin feature values (TreePoint representation).\n    // Cache input RDD for speedup during multiple passes.\n    //输入\n    val treeInput = TreePoint.convertToTreeRDD(retaggedInput, splits, metadata)\n\n    val withReplacement = numTrees > 1\n\n    val baggedInput = BaggedPoint\n      .convertToBaggedRDD(treeInput, strategy.subsamplingRate, numTrees, withReplacement, seed)\n      .persist(StorageLevel.MEMORY_AND_DISK)\n\n    // depth of the decision tree\n    val maxDepth = strategy.maxDepth\n    require(maxDepth <= 30,\n      s\"DecisionTree currently only supports maxDepth <= 30, but was given maxDepth = $maxDepth.\")\n\n    // Max memory usage for aggregates\n    // TODO: Calculate memory usage more precisely.\n    val maxMemoryUsage: Long = strategy.maxMemoryInMB * 1024L * 1024L\n    logDebug(\"max memory usage for aggregates = \" + maxMemoryUsage + \" bytes.\")\n\n    /*\n     * The main idea here is to perform group-wise training of the decision tree nodes thus\n     * reducing the passes over the data from (# nodes) to (# nodes / maxNumberOfNodesPerGroup).\n     * Each data sample is handled by a particular node (or it reaches a leaf and is not used\n     * in lower levels).\n     */\n\n    // Create an RDD of node Id cache.\n    // At first, all the rows belong to the root nodes (node Id == 1).\n    val nodeIdCache = if (strategy.useNodeIdCache) {\n      Some(NodeIdCache.init(\n        data = baggedInput,\n        numTrees = numTrees,\n        checkpointInterval = strategy.checkpointInterval,\n        initVal = 1))\n    } else {\n      None\n    }\n\n    /*\n      Stack of nodes to train: (treeIndex, node)\n      The reason this is a stack is that we train many trees at once, but we want to focus on\n      completing trees, rather than training all simultaneously.  If we are splitting nodes from\n      1 tree, then the new nodes to split will be put at the top of this stack, so we will continue\n      training the same tree in the next iteration.  This focus allows us to send fewer trees to\n      workers on each iteration; see topNodesForGroup below.\n     */\n    val nodeStack = new mutable.Stack[(Int, LearningNode)]\n\n    val rng = new Random()\n    rng.setSeed(seed)\n\n    // Allocate and queue root nodes.\n    val topNodes = Array.fill[LearningNode](numTrees)(LearningNode.emptyNode(nodeIndex = 1))\n    Range(0, numTrees).foreach(treeIndex => nodeStack.push((treeIndex, topNodes(treeIndex))))\n\n    timer.stop(\"init\")\n\n    while (nodeStack.nonEmpty) {\n      // Collect some nodes to split, and choose features for each node (if subsampling).\n      // Each group of nodes may come from one or multiple trees, and at multiple levels.\n      val (nodesForGroup, treeToNodeToIndexInfo) =\n        RandomForest.selectNodesToSplit(nodeStack, maxMemoryUsage, metadata, rng)\n      // Sanity check (should never occur):\n      assert(nodesForGroup.nonEmpty,\n        s\"RandomForest selected empty nodesForGroup.  Error for unknown reason.\")\n\n      // Only send trees to worker if they contain nodes being split this iteration.\n      val topNodesForGroup: Map[Int, LearningNode] =\n        nodesForGroup.keys.map(treeIdx => treeIdx -> topNodes(treeIdx)).toMap\n\n      // Choose node splits, and enqueue new nodes as needed.\n      timer.start(\"findBestSplits\")\n      RandomForest.findBestSplits(baggedInput, metadata, topNodesForGroup, nodesForGroup,\n        treeToNodeToIndexInfo, splits, nodeStack, timer, nodeIdCache)\n      timer.stop(\"findBestSplits\")\n    }\n\n    baggedInput.unpersist()\n\n    timer.stop(\"total\")\n\n    logInfo(\"Internal timing for DecisionTree:\")\n    logInfo(s\"$timer\")\n\n    // Delete any remaining checkpoints used for node Id cache.\n    if (nodeIdCache.nonEmpty) {\n      try {\n        nodeIdCache.get.deleteAllCheckpoints()\n      } catch {\n        case e: IOException =>\n          logWarning(s\"delete all checkpoints failed. Error reason: ${e.getMessage}\")\n      }\n    }\n\n    val numFeatures = metadata.numFeatures\n\n    parentUID match {\n      case Some(uid) =>\n        if (strategy.algo == OldAlgo.Classification) {\n          topNodes.map { rootNode =>\n            new DecisionTreeClassificationModel(uid, rootNode.toNode, numFeatures,\n              strategy.getNumClasses)\n          }\n        } else {\n          topNodes.map { rootNode =>\n            new DecisionTreeRegressionModel(uid, rootNode.toNode, numFeatures)\n          }\n        }\n      case None =>\n        if (strategy.algo == OldAlgo.Classification) {\n          topNodes.map { rootNode =>\n            new DecisionTreeClassificationModel(rootNode.toNode, numFeatures,\n              strategy.getNumClasses)\n          }\n        } else {\n          topNodes.map(rootNode => new DecisionTreeRegressionModel(rootNode.toNode, numFeatures))\n        }\n    }\n  }\n```\n\n\n\n\n#### buildMetadata\n决策树训练的元数据构造。主要用于计算每个特征的bin数量，以及无序类特征集合, 每个节点使用的特征数量等。其中决策树一般使用所有特征、随机森林分类采用$sqrt(n)$个特征，随机森林回归采用$\\frac{n}{3}$个特征\n\n\n```\ndef buildMetadata(\n      input: RDD[LabeledPoint],\n      strategy: Strategy,\n      numTrees: Int,\n      featureSubsetStrategy: String): DecisionTreeMetadata = {\n    //特征数量\n    val numFeatures = input.map(_.features.size).take(1).headOption.getOrElse {\n      throw new IllegalArgumentException(s\"DecisionTree requires size of input RDD > 0, \" +\n        s\"but was given by empty one.\")\n    }\n    val numExamples = input.count() //样本数量\n    val numClasses = strategy.algo match {\n      case Classification => strategy.numClasses\n      case Regression => 0\n    }\n    //最大划分数量 \n    val maxPossibleBins = math.min(strategy.maxBins, numExamples).toInt\n    if (maxPossibleBins < strategy.maxBins) {\n      logWarning(s\"DecisionTree reducing maxBins from ${strategy.maxBins} to $maxPossibleBins\" +\n        s\" (= number of training instances)\")\n    }\n    //maxPossibleBins可能被numExamples修改过，导致小于刚开始设置的strategy.maxBins。\n    //需要进一步确保离散值的特征取值数量小于maxPossibleBins，\n    if (strategy.categoricalFeaturesInfo.nonEmpty) {\n      val maxCategoriesPerFeature = strategy.categoricalFeaturesInfo.values.max\n      val maxCategory =\n        strategy.categoricalFeaturesInfo.find(_._2 == maxCategoriesPerFeature).get._1\n      require(maxCategoriesPerFeature <= maxPossibleBins,\n        s\"DecisionTree requires maxBins (= $maxPossibleBins) to be at least as large as the \" +\n        s\"number of values in each categorical feature, but categorical feature $maxCategory \" +\n        s\"has $maxCategoriesPerFeature values. Considering remove this and other categorical \" +\n        \"features with a large number of values, or add more training examples.\")\n    }\n    //存储每个无序特征的索引\n    val unorderedFeatures = new mutable.HashSet[Int]()\n    //存储每个无序特征的bin数量\n    val numBins = Array.fill[Int](numFeatures)(maxPossibleBins)\n    if (numClasses > 2) { //多分类问题\n      //根据maxPossibleBins，计算每个无序特征对应的最大类别数量\n      val maxCategoriesForUnorderedFeature =\n        ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt\n      strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =>\n        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤\n          if (numCategories > 1) {\n          //判断离散特征是否可当做无序特征，需要保证\n          //bins的数量需要小于2 * ((1 << numCategories - 1) - 1)）\n          if (numCategories <= maxCategoriesForUnorderedFeature) {\n            unorderedFeatures.add(featureIndex)\n            //有numCategories个取值的的特征，对应bins数量为(1 << numCategories - 1) - 1\n            //此处刚开始有点疑惑，感觉应该是2 *（(1 << numCategories - 1) - 1）\n            //通过DecisionTreeMetadata中numSplits函数发现，此处的bin数量和split数量有一定对应关系，(featureIndex)\n           //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1\n            numBins(featureIndex) = numUnorderedBins(numCategories)\n          } else {\n            //对于其他离散特征，numBins数量为特征可能的取值数量\n            numBins(featureIndex) = numCategories\n          }\n        }\n      }\n    } else { //对于二值分类或回归问题\n      strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =>\n        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤\n        if (numCategories > 1) {\n          //numBins数量为特征可能的取值数量\n          numBins(featureIndex) = numCategories \n        }\n      }\n    }\n\n    //设置每个分支节点对应的特征数量\n    val _featureSubsetStrategy = featureSubsetStrategy match {\n      case \"auto\" =>\n        if (numTrees == 1) { //如果是树，使用所有特征n\n          \"all\"\n        } else {\n          if (strategy.algo == Classification) { //如果是用于分类的随机森林，使用sqrt(n)个特征\n            \"sqrt\"\n          } else {\n            \"onethird\"  //如果是用于回归的随机森林，使用n/3个特征\n          }\n        }\n      case _ => featureSubsetStrategy\n    }\n\n    val numFeaturesPerNode: Int = _featureSubsetStrategy match {\n      case \"all\" => numFeatures\n      case \"sqrt\" => math.sqrt(numFeatures).ceil.toInt\n      case \"log2\" => math.max(1, (math.log(numFeatures) / math.log(2)).ceil.toInt)\n      case \"onethird\" => (numFeatures / 3.0).ceil.toInt\n      case _ =>\n        Try(_featureSubsetStrategy.toInt).filter(_ > 0).toOption match {\n          case Some(value) => math.min(value, numFeatures)\n          case None =>\n            Try(_featureSubsetStrategy.toDouble).filter(_ > 0).filter(_ <= 1.0).toOption match {\n              case Some(value) => math.ceil(value * numFeatures).toInt\n              case _ => throw new IllegalArgumentException(s\"Supported values:\" +\n                s\" ${RandomForestParams.supportedFeatureSubsetStrategies.mkString(\", \")},\" +\n                s\" (0.0-1.0], [1-n].\")\n            }\n        }\n    }\n\n    new DecisionTreeMetadata(numFeatures, numExamples, numClasses, numBins.max,\n      strategy.categoricalFeaturesInfo, unorderedFeatures.toSet, numBins,\n      strategy.impurity, strategy.quantileCalculationStrategy, strategy.maxDepth,\n      strategy.minInstancesPerNode, strategy.minInfoGain, numTrees, numFeaturesPerNode)\n  }\n```\n\n#### DecisionTreeMetadata类\n```  \nprivate[spark] class DecisionTreeMetadata(\n    val numFeatures: Int,\n    val numExamples: Long,\n    val numClasses: Int,\n    val maxBins: Int,\n    val featureArity: Map[Int, Int],\n    val unorderedFeatures: Set[Int],\n    val numBins: Array[Int],\n    val impurity: Impurity,\n    val quantileStrategy: QuantileStrategy,\n    val maxDepth: Int,\n    val minInstancesPerNode: Int,\n    val minInfoGain: Double,\n    val numTrees: Int,\n    val numFeaturesPerNode: Int) extends Serializable {\n  //判断是否为无序特征\n  def isUnordered(featureIndex: Int): Boolean = unorderedFeatures.contains(featureIndex)\n  //判断是否用于分类的决策树（随机森林）\n  def isClassification: Boolean = numClasses >= 2\n  //判断是否用于多分类的决策树（随机森林）\n  def isMulticlass: Boolean = numClasses > 2\n  //判断是否拥有离散特征的多分类决策树（随机森林）\n  def isMulticlassWithCategoricalFeatures: Boolean = isMulticlass && (featureArity.size > 0)\n  //判断是否离散特征\n  def isCategorical(featureIndex: Int): Boolean = featureArity.contains(featureIndex)\n //判断是否连续特征\n  def isContinuous(featureIndex: Int): Boolean = !featureArity.contains(featureIndex)\n  //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1\n  def numSplits(featureIndex: Int): Int = if (isUnordered(featureIndex)) {\n    numBins(featureIndex)\n  } else {\n    numBins(featureIndex) - 1\n  }\n  //对于连续特征，根据划分数量设置bin数量为划分数量加1\n  def setNumSplits(featureIndex: Int, numSplits: Int) {\n    require(isContinuous(featureIndex),\n      s\"Only number of bin for a continuous feature can be set.\")\n    numBins(featureIndex) = numSplits + 1\n  }\n  //判断是否需要对特征进行采样\n  def subsamplingFeatures: Boolean = numFeatures != numFeaturesPerNode\n}    \n```\n\n#### findSplits\n通过使用采样的样本，寻找样本的划分splits和划分后的bins。\n\n**划分的思想：**对连续特征和离散特征，分别采用不同处理方式。对于每个连续特征，numBins - 1个splits, 代表每个树的节点的所有可能的二值化分；对于每个离散特征，无序离散特征（用于多分类的维度较大的feature）基于特征的子集进行划分。有序类特征（用于回归、二分类、多分类的维度较小的feature)的每个取值对应一个bin.\n\n```\nprotected[tree] def findSplits(\n      input: RDD[LabeledPoint],\n      metadata: DecisionTreeMetadata,\n      seed: Long): Array[Array[Split]] = {\n    logDebug(\"isMulticlass = \" + metadata.isMulticlass)\n    val numFeatures = metadata.numFeatures //特征的数量\n    // 得到所有连续特征索引\n    val continuousFeatures = Range(0, numFeatures).filter(metadata.isContinuous)\n    //当有连续特征的时候需要采样样本   \n    val sampledInput = if (continuousFeatures.nonEmpty) {\n      // 计算近似分位数计算需要的样本数\n      val requiredSamples = math.max(metadata.maxBins * metadata.maxBins, 10000)\n      // 计算需要的样本占总样本比例\n      val fraction = if (requiredSamples < metadata.numExamples) {\n        requiredSamples.toDouble / metadata.numExamples\n      } else {\n        1.0\n      }\n      logDebug(\"fraction of data used for calculating quantiles = \" + fraction)\n      input.sample(withReplacement = false, fraction, new XORShiftRandom(seed).nextInt())\n    } else {\n      input.sparkContext.emptyRDD[LabeledPoint]\n    }\n    //对每个连续特征和非有序类离散特征，通过排序的方式，寻找最佳的splits点\n    findSplitsBySorting(sampledInput, metadata, continuousFeatures)\n  }\n```\n\n```\n //对每个特征，通过排序的方式，寻找最佳的splits点\n private def findSplitsBySorting(\n      input: RDD[LabeledPoint],\n      metadata: DecisionTreeMetadata,\n      continuousFeatures: IndexedSeq[Int]): Array[Array[Split]] = {\n   \n    //寻找连续特征的划分阈值\n    val continuousSplits: scala.collection.Map[Int, Array[Split]] = {\n      //设置分区数量，如果连续特征的数量小于原始分区数，则进一步减少分区，防止无效的启动的task任务。\n      val numPartitions = math.min(continuousFeatures.length, input.partitions.length)\n\n      input\n        .flatMap(point => continuousFeatures.map(idx => (idx, point.features(idx))))\n        .groupByKey(numPartitions)\n        .map { case (idx, samples) =>\n          val thresholds = findSplitsForContinuousFeature(samples, metadata, idx)\n          val splits: Array[Split] = thresholds.map(thresh => new ContinuousSplit(idx, thresh))\n          logDebug(s\"featureIndex = $idx, numSplits = ${splits.length}\")\n          (idx, splits)\n        }.collectAsMap()\n    }\n    //特征数量\n    val numFeatures = metadata.numFeatures\n    //汇总所有特征的split(不包括无序离散特征)\n    val splits: Array[Array[Split]] = Array.tabulate(numFeatures) {\n      //如果是连续特征，返回该连续特征的split\n      case i if metadata.isContinuous(i) =>\n        val split = continuousSplits(i)\n        metadata.setNumSplits(i, split.length)\n        split\n      //如果是无序离散特征，则提取该特征的split， 具体是对于每个离散特征，其第k个split为其k对应二进制的所有位置为1的数值。\n      case i if metadata.isCategorical(i) && metadata.isUnordered(i) =>\n        // Unordered features\n        // 2^(maxFeatureValue - 1) - 1 combinations\n        //特征的取值数量\n        val featureArity = metadata.featureArity(i)\n        Array.tabulate[Split](metadata.numSplits(i)) { splitIndex =>\n          val categories = extractMultiClassCategories(splitIndex + 1, featureArity)\n          new CategoricalSplit(i, categories.toArray, featureArity)\n        }\n      //对于有序离散特征，暂时不求解split, 在训练阶段求解\n      case i if metadata.isCategorical(i) =>\n        // Ordered features\n        //   Splits are constructed as needed during training.\n        Array.empty[Split]\n    }\n    splits\n  }\n\n```\n\n```\n//将input这个数对应的二进制位置为1的位置加入到当前划分\nprivate[tree] def extractMultiClassCategories(\n      input: Int,\n      maxFeatureValue: Int): List[Double] = {\n    var categories = List[Double]()\n    var j = 0\n    var bitShiftedInput = input\n    while (j < maxFeatureValue) {\n      if (bitShiftedInput % 2 != 0) {\n        // updating the list of categories.\n        categories = j.toDouble :: categories\n      }\n      // Right shift by one\n      bitShiftedInput = bitShiftedInput >> 1\n      j += 1\n    }\n    categories\n  }\n```\n\n```\n//对于连续特征，找到其对应的splits分割点\nprivate[tree] def findSplitsForContinuousFeature(\n      featureSamples: Iterable[Double], \n      metadata: DecisionTreeMetadata, \n      featureIndex: Int): Array[Double] = {\n    //确保有连续特征\n    require(metadata.isContinuous(featureIndex),\n      \"findSplitsForContinuousFeature can only be used to find splits for a continuous feature.\")\n    //寻找splits分割点\n    val splits = if (featureSamples.isEmpty) {\n      Array.empty[Double]  //如果样本数为0， 返回空数组\n    } else {\n      //得到metadata里的split数量\n      val numSplits = metadata.numSplits(featureIndex) \n\n      //在采样得到的样本中，计算每个特征取值的计数、以及总样本数量\n      val (valueCountMap, numSamples) = featureSamples.foldLeft((Map.empty[Double, Int], 0)) {\n        case ((m, cnt), x) =>\n          (m + ((x, m.getOrElse(x, 0) + 1)), cnt + 1)\n      }\n      // 对于每个特征取值进行排序\n      val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n      //如果得到的possible splits数量小于metadata中该特征的的split数量，则直接以当前每个特征取值作为分割的阈值\n      val possibleSplits = valueCounts.length - 1\n      if (possibleSplits <= numSplits) { \n        valueCounts.map(_._1).init\n      } else {\n        //否则，根据总样本数量，计算平均每个区间对应的特征取值数量，假设为n。然后，对于n, 2*n, 3*n ...的位置分别设置标记。设置2个游标分别指向valueCounts内部连续的两个特征取值，从前向后遍历，当后面游标到标记的距离大于前面的游标时，将前面游标的位置对应的特征取值设置为一个split点。\n        //计算平均每个区间对应的特征取值数量\n        val stride: Double = numSamples.toDouble / (numSplits + 1)\n        logDebug(\"stride = \" + stride)\n        //splitsBuilder用于存储每个分割阈值\n        val splitsBuilder = mutable.ArrayBuilder.make[Double]\n        //特征取值从小到大的位置索引\n        var index = 1\n        //当前访问的所有特征取值数量之和\n        var currentCount = valueCounts(0)._2\n        //下一次的标记位置      \n        var targetCount = stride\n        while (index < valueCounts.length) {\n          val previousCount = currentCount\n          currentCount += valueCounts(index)._2\n          val previousGap = math.abs(previousCount - targetCount)\n          val currentGap = math.abs(currentCount - targetCount)\n          //使前面游标和后面游标的距离更小，且较小游标距离标记位置的距离最近\n          if (previousGap < currentGap) {\n            splitsBuilder += valueCounts(index - 1)._1\n            targetCount += stride\n          }\n          index += 1\n        }\n        splitsBuilder.result()\n      }\n    }\n    splits\n  }\n\n```\n\n#### TreePoint.convertToTreeRDD\n调用TreePoint类的convertToTreeRDD方法，RDD[LabeledPoint]转化为RDD[TreePoint]。\n\n```\n def convertToTreeRDD(\n      input: RDD[LabeledPoint],\n      splits: Array[Array[Split]],\n      metadata: DecisionTreeMetadata): RDD[TreePoint] = {\n    // 构建数组featureArity，存储每个特征对应的离散值个数，连续值对应的value为0\n    val featureArity: Array[Int] = new Array[Int](metadata.numFeatures)\n    var featureIndex = 0\n    while (featureIndex < metadata.numFeatures) {\n      featureArity(featureIndex) = metadata.featureArity.getOrElse(featureIndex, 0)\n      featureIndex += 1\n    }\n    //获得所有连续特征的分裂阈值，如果是离散特征，则数组对应空\n    val thresholds: Array[Array[Double]] = featureArity.zipWithIndex.map { case (arity, idx) =>\n      if (arity == 0) {\n        splits(idx).map(_.asInstanceOf[ContinuousSplit].threshold)\n      } else {\n        Array.empty[Double]\n      }\n    }\n    //将样本的每个原始特征，转化为对应的bin特征值，用于训练\n    input.map { x =>\n      TreePoint.labeledPointToTreePoint(x, thresholds, featureArity)\n    }\n  }\n```\n\n```\n  //将单个样本的原始特征，转化为对应的bin特征值，用于训练\n  private def labeledPointToTreePoint(\n      labeledPoint: LabeledPoint,\n      thresholds: Array[Array[Double]],\n      featureArity: Array[Int]): TreePoint = {\n    //特征数量\n    val numFeatures = labeledPoint.features.size\n    //为每个特征找到对应的bin特征值，存储在arr数组\n    val arr = new Array[Int](numFeatures)\n    var featureIndex = 0\n    while (featureIndex < numFeatures) {\n      //寻找数据点labeledPoint、当前特征featureIndex对应的bin特征值\n      arr(featureIndex) =\n        findBin(featureIndex, labeledPoint, featureArity(featureIndex), thresholds(featureIndex))\n      featureIndex += 1\n    }\n    new TreePoint(labeledPoint.label, arr)\n  }\n```\n\n```\nprivate def findBin(\n      featureIndex: Int,\n      labeledPoint: LabeledPoint,\n      featureArity: Int,\n      thresholds: Array[Double]): Int = {\n    //获取当前labeledPoint的第featureIndex个原始特征值\n    val featureValue = labeledPoint.features(featureIndex)\n    \n    if (featureArity == 0) { \n      //如果是连续特征，利用二分法得到当前特征值对应的离散区间下标\n      val idx = java.util.Arrays.binarySearch(thresholds, featureValue)\n      if (idx >= 0) {\n        idx\n      } else {\n        -idx - 1\n      }\n    } else {\n      //如果是离散值，则直接返回当前的特征值\n      if (featureValue < 0 || featureValue >= featureArity) {\n        throw new IllegalArgumentException(\n          s\"DecisionTree given invalid data:\" +\n            s\" Feature $featureIndex is categorical with values in {0,...,${featureArity - 1},\" +\n            s\" but a data point gives it value $featureValue.\\n\" +\n            \"  Bad data point: \" + labeledPoint.toString)\n      }\n      featureValue.toInt\n    }\n  }\n```\n\n\n```\n//LabeledPoint类\ncase class LabeledPoint(@Since(\"2.0.0\") label: Double, @Since(\"2.0.0\") features: Vector) {\n  override def toString: String = {\n    s\"($label,$features)\"\n  }\n}\n```\n```\n//TreePoint类\nprivate[spark] class TreePoint(val label: Double, val binnedFeatures: Array[Int])\n  extends Serializable {\n}\n```\n#### BaggedPoint.convertToBaggedRDD\nRDD[Datum]数据集转换成RDD[BaggedPoint[Datum]的表示类型，\n\n```\n  def convertToBaggedRDD[Datum] (\n      input: RDD[Datum], //输入数据集\n      subsamplingRate: Double, //采样率\n      numSubsamples: Int, //采样次数\n      withReplacement: Boolean, //是否有放回\n      //随机数种子\n      seed: Long = Utils.random.nextLong()): RDD[BaggedPoint[Datum]] = {\n    if (withReplacement) {//有放回采样，生成BaggedPoint结构表示\n      convertToBaggedRDDSamplingWithReplacement(input, subsamplingRate, numSubsamples, seed)\n    } else {\n      //当采样比为1，并且采样次数为1时，不采样，只生成BaggedPoint结构表示\n      if (numSubsamples == 1 && subsamplingRate == 1.0) {\n        convertToBaggedRDDWithoutSampling(input)\n      } else {\n        //无放回采样，生成BaggedPoint结构表示\n        convertToBaggedRDDSamplingWithoutReplacement(input, subsamplingRate, numSubsamples, seed)\n      }\n    }\n  }\n```\n```\n  //有放回采样，数据转换为RDD[BaggedPoint[Datum]]\n  private def convertToBaggedRDDSamplingWithReplacement[Datum] (\n      input: RDD[Datum],//输入数据集\n      subsample: Double,//采样率\n      numSubsamples: Int,//采样次数\n      //随机数种子\n      seed: Long): RDD[BaggedPoint[Datum]] = {\n    input.mapPartitionsWithIndex { (partitionIndex, instances) =>\n      //每个分区生成一个泊松采样器，通过采样率、随机种子、分区索引等初始化\n      val poisson = new PoissonDistribution(subsample)\n      poisson.reseedRandomGenerator(seed + partitionIndex + 1)\n      //将每个实例变换成BaggedPoint结构表示\n      instances.map { instance =>\n        val subsampleWeights = new Array[Double](numSubsamples)\n        var subsampleIndex = 0\n        //依次对每次采样，生成权重（即该实例在每次无放回采样出现的次数）\n        while (subsampleIndex < numSubsamples) {\n          subsampleWeights(subsampleIndex) = poisson.sample()\n          subsampleIndex += 1\n        }\n        //生成BaggedPoint结构表示\n        new BaggedPoint(instance, subsampleWeights) \n      }\n    }\n  }\n```\n```\n//BaggedPoint类，datum表示数据实例，subsampleWeights表示当前实例在每个采样中的权重。\n如(datum, [1, 0, 4])表示有3次采样，数据实例在3次采样中出现的次数分别为1，0，4\nprivate[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])\n  extends Serializable\n```\n```\n  //原始数据（不采样）直接转换为BaggedPoint结构表示\n  private def convertToBaggedRDDWithoutSampling[Datum] (\n      input: RDD[Datum]): RDD[BaggedPoint[Datum]] = {\n    input.map(datum => new BaggedPoint(datum, Array(1.0)))\n  }\n```\n\n```\n  //无放回采样，数据转换为RDD[BaggedPoint[Datum]]\n  private def convertToBaggedRDDSamplingWithoutReplacement[Datum] (\n      input: RDD[Datum],\n      subsamplingRate: Double,\n      numSubsamples: Int,\n      seed: Long): RDD[BaggedPoint[Datum]] = {\n    input.mapPartitionsWithIndex { (partitionIndex, instances) =>\n      //使用随机数种子，分区索引，构建随机数生成器\n      val rng = new XORShiftRandom\n      rng.setSeed(seed + partitionIndex + 1)\n      //将每个实例变换成BaggedPoint结构表示\n      instances.map { instance =>\n        val subsampleWeights = new Array[Double](numSubsamples)\n        var subsampleIndex = 0\n        //对于每次采样，生成0-1之间的随机数，如果小于采样比，则对应权重为1，否则为0\n        while (subsampleIndex < numSubsamples) {\n          val x = rng.nextDouble()\n          subsampleWeights(subsampleIndex) = {\n            if (x < subsamplingRate) 1.0 else 0.0\n          }\n          subsampleIndex += 1\n        }\n        //转换为BaggedPoint结构数据\n        new BaggedPoint(instance, subsampleWeights)\n      }\n    }\n  }\n```\n#### RandomForest.selectNodesToSplit\n选择当前迭代待分裂的节点，以及确定每个节点使用的特征。每次选择都根据内存限制、每个节点占用的内存（如果每个节点使用的是采样后的特征），自适应地确定节点个数。\n\n```\nprivate[tree] def selectNodesToSplit(\n      nodeStack: mutable.Stack[(Int, LearningNode)], //存储节点的栈结构\n      maxMemoryUsage: Long, //最大占用内存限制\n      metadata: DecisionTreeMetadata, //元数据\n      //随机数\n      rng: Random): \n      //返回值包括：（1）每个树对应的待分裂节点数组， \n      //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）\n      (Map[Int, Array[LearningNode]], Map[Int, Map[Int, NodeIndexInfo]]) = {\n      //nodesForGroup(treeIndex) 存储第treeIndex个树对应的待分裂节点数组\n      val mutableNodesForGroup = new mutable.HashMap[Int, mutable.ArrayBuffer[LearningNode]]()\n      //每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）\n      val mutableTreeToNodeToIndexInfo =\n      new mutable.HashMap[Int, mutable.HashMap[Int, NodeIndexInfo]]()\n      var memUsage: Long = 0L  //当前使用内存\n      var numNodesInGroup = 0  //当前分组的节点数量\n      // If maxMemoryInMB is set very small, we want to still try to split 1 node,\n      // so we allow one iteration if memUsage == 0.\n      //如果栈不空，并且（1）如果内存上限设置非常小，我们要去报至少能有1个节点用于分裂\n      //（2）当前使用内存小于内存上限值，则进一步选择节点用于分裂\n      while (nodeStack.nonEmpty && (memUsage < maxMemoryUsage || memUsage == 0)) {\n      val (treeIndex, node) = nodeStack.top //选择栈顶节点\n      // Choose subset of features for node (if subsampling).\n     \n      val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) {       //如果特征需要采样，则对所有特征进行无放回采样\n        Some(SamplingUtils.reservoirSampleAndCount(Range(0,\n          metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong())._1)\n      } else {//如果特征不需要采样，则返回None\n        None\n      }\n      //通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的内存\n      val nodeMemUsage = RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L\n      ////检查增加当前节点后，内存容量是是否超过限制\n      if (memUsage + nodeMemUsage <= maxMemoryUsage || memUsage == 0) {\n        //如果加入该节点后内存没有超过限制\n        nodeStack.pop() //当前节点出栈\n        //更新mutableNodesForGroup，将当前节点加入对应treeIndex的节点数组\n        mutableNodesForGroup.getOrElseUpdate(treeIndex, new mutable.ArrayBuffer[LearningNode]()) +=\n          node\n        //更新mutableTreeToNodeToIndexInfo，将当前节点的具体信息，加入对应treeindex的节点map\n        mutableTreeToNodeToIndexInfo\n          .getOrElseUpdate(treeIndex, new mutable.HashMap[Int, NodeIndexInfo]())(node.id)\n          = new NodeIndexInfo(numNodesInGroup, featureSubset)\n      }\n      numNodesInGroup += 1 //当前分组的节点数量加一\n      memUsage += nodeMemUsage //当前使用内存数量加一\n    }\n    if (memUsage > maxMemoryUsage) {\n      // If maxMemoryUsage is 0, we should still allow splitting 1 node.\n      logWarning(s\"Tree learning is using approximately $memUsage bytes per iteration, which\" +\n        s\" exceeds requested limit maxMemoryUsage=$maxMemoryUsage. This allows splitting\" +\n        s\" $numNodesInGroup nodes in this iteration.\")\n    }\n    //转换可变map为不可变map类型\n    val nodesForGroup: Map[Int, Array[LearningNode]] =\n      mutableNodesForGroup.mapValues(_.toArray).toMap\n    val treeToNodeToIndexInfo = mutableTreeToNodeToIndexInfo.mapValues(_.toMap).toMap\n    //返回（1）每个树对应的待分裂节点数组， \n    //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）\n    (nodesForGroup, treeToNodeToIndexInfo)\n  }\n```\n\n```\n//无放回采样\ndef reservoirSampleAndCount[T: ClassTag](\n      input: Iterator[T], //input输入的迭代器\n      k: Int, //采样的样本数\n      seed: Long = Random.nextLong()) //随机数种子\n    : (Array[T], Long) = {\n    val reservoir = new Array[T](k) //存储采样结果的数组\n    // 放置迭代器的前k个元素到结果数组\n    var i = 0\n    while (i < k && input.hasNext) {\n      val item = input.next()\n      reservoir(i) = item\n      i += 1\n    }\n\n\n    //如果输入元素个数小于k, 则这k个特征作为返回的结果\n    if (i < k) {\n      // If input size < k, trim the array to return only an array of input size.\n      val trimReservoir = new Array[T](i)\n      System.arraycopy(reservoir, 0, trimReservoir, 0, i)\n      (trimReservoir, i) //返回结果数组，以及原始数组的元素个数\n    } else { \n      //如果输入元素个数大于k, 继续采样过程，将后面元素以一定概率随机替换前面的某个元素\n      var l = i.toLong\n      val rand = new XORShiftRandom(seed)\n      while (input.hasNext) {\n        val item = input.next()\n        l += 1\n        //当前结果数组有k个元素，l为当前元素的序号。k/l为当前元素替换结果数组中某个元素的概率。\n        //在进行替换时，对结果数组的每个元素以相等概率发生替换\n        //具体方式是产生一个0到l-1之间的随机整数replacementIndex，\n        //如果小于k则对第replacementIndex这个元素进行替换\n        val replacementIndex = (rand.nextDouble() * l).toLong\n        if (replacementIndex < k) {\n          reservoir(replacementIndex.toInt) = item\n        }\n      }\n      (reservoir, l) //返回结果数组，以及原始数组的元素个数\n    }\n  }\n```\n\n```\n  //通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的字节数\n  private def aggregateSizeForNode(\n      metadata: DecisionTreeMetadata,\n      featureSubset: Option[Array[Int]]): Long = {\n    //得到所有使用的特征的bin的数量之后\n    val totalBins = if (featureSubset.nonEmpty) {\n      //如果使用采样特征，得到采样后的所有特征bin数量之和\n      featureSubset.get.map(featureIndex => metadata.numBins(featureIndex).toLong).sum\n    } else {//否则使用所有的特征的bin数量之和\n      metadata.numBins.map(_.toLong).sum\n    }\n    if (metadata.isClassification) {\n      //如果是分类问题，则返回bin数量之和*类别个数\n      metadata.numClasses * totalBins \n    } else {\n      //否则返回bin数量之和*3\n      3 * totalBins\n    }\n  }\n\n```\n####  RandomForest.findBestSplits\n\n给定selectNodesToSplit方法选择的一组节点，找到每个节点对应的最佳分类特征的分裂位置。**求解的主要思想如下：**\n\n**基于节点的分组进行并行训练：**对一组的节点同时进行每个bin的统计和计算，减少不必要的数据传输成本。这样每次迭代需要更多的计算和存储成本，但是可以大大减少迭代的次数\n\n**基于bin的最佳分割点计算：**基于bin的计算来寻找最佳分割点，计算的思想不是依次对每个样本计算其对每个孩子节点的增益贡献，而是先将所有样本的每个特征映射到对应的bin，通过聚合每个bin的数据，进一步计算对应每个特征每个分割的增益。\n\n**对每个partition进行聚合：**由于提取知道了每个特征对应的split个数，因此可以用一个数组存储所有的bin的聚合信息，通过使用RDD的聚合方法，大大减少通讯开销。\n\n```\n private[tree] def findBestSplits(\n      input: RDD[BaggedPoint[TreePoint]], //训练数据\n      metadata: DecisionTreeMetadata, //随机森林元数据信息\n      topNodesForGroup: Map[Int, LearningNode], //存储当前节点分组对应的每个树的根节点\n      nodesForGroup: Map[Int, Array[LearningNode]],//存储当前节点分组对应的每个树的节点数组\n      treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]],//存储当前节点分组对应的每个树索引、节点索引、及详细信息\n      splits: Array[Array[Split]], //存储每个特征的所有split信息\n      //存储节点的栈结构，初始化时为各个树的根节点\n      nodeStack: mutable.Stack[(Int, LearningNode)],\n      timer: TimeTracker = new TimeTracker,       \n      nodeIdCache: Option[NodeIdCache] = None): Unit = {\n\n    //存储当前分组的节点数量\n    val numNodes = nodesForGroup.values.map(_.length).sum\n    logDebug(\"numNodes = \" + numNodes)\n    logDebug(\"numFeatures = \" + metadata.numFeatures)\n    logDebug(\"numClasses = \" + metadata.numClasses)\n    logDebug(\"isMulticlass = \" + metadata.isMulticlass)\n    logDebug(\"isMulticlassWithCategoricalFeatures = \" +\n      metadata.isMulticlassWithCategoricalFeatures)\n    logDebug(\"using nodeIdCache = \" + nodeIdCache.nonEmpty.toString)\n\n  \n    //对于一个特定的树的特定节点，通过baggedPoint数据点，更新DTStatsAggregator聚合信息（更新相关的特征及bin的聚合类信息）\n    def nodeBinSeqOp(\n        treeIndex: Int, //树的索引\n        nodeInfo: NodeIndexInfo, //节点信息\n        agg: Array[DTStatsAggregator], //聚合信息，(node, feature, bin)\n        baggedPoint: BaggedPoint[TreePoint]): Unit = {//数据点\n      if (nodeInfo != null) {//如果节点信息不为空，表示该节点在当前计算的节点集合中\n        val aggNodeIndex = nodeInfo.nodeIndexInGroup //该节点在当前分组的编号\n        val featuresForNode = nodeInfo.featureSubset //该节点对应的特征集合\n        //该样本在该树上的采样次数，如果为n表示5个同样的数据点同时用于更新对应的聚合信息\n        val instanceWeight = baggedPoint.subsampleWeights(treeIndex) \n        if (metadata.unorderedFeatures.isEmpty) {\n          //如果不存在无序特征，根据有序特征进行更新\n          orderedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, instanceWeight, featuresForNode)\n        } else { //都是有序特征\n          mixedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, splits,\n            metadata.unorderedFeatures, instanceWeight, featuresForNode)\n        }\n        agg(aggNodeIndex).updateParent(baggedPoint.datum.label, instanceWeight)\n      }\n    }\n\n    //计算当前数据被划分到的树的节点，并更新在对应节点的聚合信息。对于每个特征的相关bin,更新其聚合信息。\n    def binSeqOp(\n        agg: Array[DTStatsAggregator],//agg数组存储聚合信息，数据结构为（node, feature, bin）\n        baggedPoint: BaggedPoint[TreePoint]): Array[DTStatsAggregator] = {\n      treeToNodeToIndexInfo.foreach { case (treeIndex, nodeIndexToInfo) =>\n        //得到要更新的节点编号\n        val nodeIndex = \n          topNodesForGroup(treeIndex).predictImpl(baggedPoint.datum.binnedFeatures, splits)\n        //对上步得到的节点，根据样本点更新其对应的bin的聚合信息\n        nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)\n      }\n      agg\n    }\n\n    /**\n     * Do the same thing as binSeqOp, but with nodeIdCache.\n     */\n    def binSeqOpWithNodeIdCache(\n        agg: Array[DTStatsAggregator],\n        dataPoint: (BaggedPoint[TreePoint], Array[Int])): Array[DTStatsAggregator] = {\n      treeToNodeToIndexInfo.foreach { case (treeIndex, nodeIndexToInfo) =>\n        val baggedPoint = dataPoint._1\n        val nodeIdCache = dataPoint._2\n        val nodeIndex = nodeIdCache(treeIndex)\n        nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)\n      }\n\n      agg\n    }\n    \n    //从treeToNodeToIndexInfo中获取每个节点对应的特征集合。key为节点在本组节点的编号，value为对应特征集合\n    def getNodeToFeatures(\n        treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]]): Option[Map[Int, Array[Int]]] = {\n      if (!metadata.subsamplingFeatures) { //如果定义为不进行特征采样\n        None\n      } else {\n        //定义为特征采样，从treeToNodeToIndexInfo中获取对应的节点编号和特征集合。\n        val mutableNodeToFeatures = new mutable.HashMap[Int, Array[Int]]()\n        treeToNodeToIndexInfo.values.foreach { nodeIdToNodeInfo =>\n          nodeIdToNodeInfo.values.foreach { nodeIndexInfo =>\n            assert(nodeIndexInfo.featureSubset.isDefined)\n            mutableNodeToFeatures(nodeIndexInfo.nodeIndexInGroup) = nodeIndexInfo.featureSubset.get\n          }\n        }\n        Some(mutableNodeToFeatures.toMap)\n      }\n    }\n    \n    //用于训练的节点数组\n    val nodes = new Array[LearningNode](numNodes)\n    //根据nodesForGroup，在nodes中存储本轮迭代的节点，存储到nodes中\n    nodesForGroup.foreach { case (treeIndex, nodesForTree) =>\n      nodesForTree.foreach { node =>\n        nodes(treeToNodeToIndexInfo(treeIndex)(node.id).nodeIndexInGroup) = node\n      }\n    }\n\n    //对于所有的节点，计算最佳特征及分割点\n    timer.start(\"chooseSplits\")\n    //对于每个分区，迭代所有的样本，计算每个节点的聚合信息，\n    //产出(nodeIndex, nodeAggregateStats)数据结构，\n    //通过reduceByKey操作，一个节点的所有信息会被shuffle到同一个分区，通过合并信息，\n    //计算每个节点的最佳分割，最后只有最佳的分割用于进一步构建决策树。\n    val nodeToFeatures = getNodeToFeatures(treeToNodeToIndexInfo)//\n    val nodeToFeaturesBc = input.sparkContext.broadcast(nodeToFeatures)\n\n    val partitionAggregates: RDD[(Int, DTStatsAggregator)] = if (nodeIdCache.nonEmpty) {\n      input.zip(nodeIdCache.get.nodeIdsForInstances).mapPartitions { points =>\n        // Construct a nodeStatsAggregators array to hold node aggregate stats,\n        // each node will have a nodeStatsAggregator\n        val nodeStatsAggregators = Array.tabulate(numNodes) { nodeIndex =>\n          val featuresForNode = nodeToFeaturesBc.value.map { nodeToFeatures =>\n            nodeToFeatures(nodeIndex)\n          }\n          new DTStatsAggregator(metadata, featuresForNode)\n        }\n        // iterator all instances in current partition and update aggregate stats\n        points.foreach(binSeqOpWithNodeIdCache(nodeStatsAggregators, _))\n        // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,\n        // which can be combined with other partition using `reduceByKey`\n        nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator\n      }\n    } else {\n      input.mapPartitions { points =>\n        // 在每个分区内，构建一个nodeStatsAggregators数组，其中每个元素对应一个node的DTStatsAggregator，该DTStatsAggregator包括了决策树元数据信息、以及该node对应的特征集合\n        val nodeStatsAggregators = Array.tabulate(numNodes) { nodeIndex =>\n          val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>\n            Some(nodeToFeatures(nodeIndex))\n          }\n          new DTStatsAggregator(metadata, featuresForNode)\n        }\n        //对当前分区，迭代所有样本，更新nodeStatsAggregators，即每个node对应的DTStatsAggregator\n        points.foreach(binSeqOp(nodeStatsAggregators, _))\n        //转化成(nodeIndex, nodeAggregateStats)格式，用于后续通过reduceByKey对多个分区的结果进行聚合。\n        nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator\n      }\n    }\n    //reduceByKey聚合多个partition的统计特征\n    val nodeToBestSplits = partitionAggregates.reduceByKey((a, b) => a.merge(b)).map {\n      case (nodeIndex, aggStats) =>\n        //得到节点对应的特征集合\n        val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>\n          Some(nodeToFeatures(nodeIndex))\n        }\n\n        // 找到最佳分裂特征和分裂位置，并返回度量的统计特征\n        val (split: Split, stats: ImpurityStats) =\n          binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))\n        (nodeIndex, (split, stats))\n    }.collectAsMap()\n\n    timer.stop(\"chooseSplits\")\n\n    val nodeIdUpdaters = if (nodeIdCache.nonEmpty) {\n      Array.fill[mutable.Map[Int, NodeIndexUpdater]](\n        metadata.numTrees)(mutable.Map[Int, NodeIndexUpdater]())\n    } else {\n      null\n    }\n    // Iterate over all nodes in this group.\n    //对于本组所有节点，更新节点本身信息，如果孩子节点是课分裂的叶子节点，则将其加入栈中\n    nodesForGroup.foreach { case (treeIndex, nodesForTree) =>\n      nodesForTree.foreach { node =>\n        val nodeIndex = node.id //节点id\n        val nodeInfo = treeToNodeToIndexInfo(treeIndex)(nodeIndex) //节点信息，包括节点在当前分组编号，节点特征等\n        val aggNodeIndex = nodeInfo.nodeIndexInGroup //节点在当前分组编号\n        //节点对应的最佳分裂，及最佳分裂对应的不纯度度量相关统计信息\n        val (split: Split, stats: ImpurityStats) =\n          nodeToBestSplits(aggNodeIndex) \n        logDebug(\"best split = \" + split)\n\n        //如果信息增益小于0，或者层次达到上限，则将当前节点设置为叶子节点\n        val isLeaf =\n          (stats.gain <= 0) || (LearningNode.indexToLevel(nodeIndex) == metadata.maxDepth)\n        node.isLeaf = isLeaf\n        node.stats = stats\n        logDebug(\"Node = \" + node)\n        \n        //当前节点非叶子节点，创建子节点\n        if (!isLeaf) {\n          node.split = Some(split) //设置节点split参数\n          //子节点层数是否达到最大值\n          val childIsLeaf = (LearningNode.indexToLevel(nodeIndex) + 1) == metadata.maxDepth\n          //左孩子节点层数达到最大值，或者不纯度度量等于0，则左孩子节点为叶子节点\n          val leftChildIsLeaf = childIsLeaf || (stats.leftImpurity == 0.0)\n          //右孩子节点层数达到最大值，或者不纯度度量等于0，则右孩子节点为叶子节点          \n          val rightChildIsLeaf = childIsLeaf || (stats.rightImpurity == 0.0)\n          //创建左孩子节点，getEmptyImpurityStats(stats.leftImpurityCalculator)为左孩子的不纯度度量，只有impurity、impurityCalculator两个属性\n          node.leftChild = Some(LearningNode(LearningNode.leftChildIndex(nodeIndex),\n            leftChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.leftImpurityCalculator)))\n          //创建右孩子节点\n          node.rightChild = Some(LearningNode(LearningNode.rightChildIndex(nodeIndex),\n            rightChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.rightImpurityCalculator)))\n\n          if (nodeIdCache.nonEmpty) {\n            val nodeIndexUpdater = NodeIndexUpdater(\n              split = split,\n              nodeIndex = nodeIndex)\n            nodeIdUpdaters(treeIndex).put(nodeIndex, nodeIndexUpdater)\n          }\n\n          // enqueue left child and right child if they are not leaves\n          //如果左孩子节点不是叶子节点，则将左孩子节点入栈\n          if (!leftChildIsLeaf) {\n            nodeStack.push((treeIndex, node.leftChild.get))\n          }\n          if (!rightChildIsLeaf) {\n            //如果右孩子节点不是叶子节点，则将右孩子节点入栈\n            nodeStack.push((treeIndex, node.rightChild.get))\n          }\n          logDebug(\"leftChildIndex = \" + node.leftChild.get.id +\n            \", impurity = \" + stats.leftImpurity)\n          logDebug(\"rightChildIndex = \" + node.rightChild.get.id +\n            \", impurity = \" + stats.rightImpurity)\n        }\n      }\n    }\n    if (nodeIdCache.nonEmpty) {\n      // Update the cache if needed.\n      nodeIdCache.get.updateNodeIndices(input, nodeIdUpdaters, splits)\n    }\n  }  \n```\n\n```\n  //得到当前数据点对应的node index输出,模仿对数据的预测过程，从根节点开始向下传播，\n  //直到一个叶子节点或者未进行分裂的节点终止，返回终止节点对应的索引。\n  def predictImpl(binnedFeatures: Array[Int], splits: Array[Array[Split]]): Int = {\n    if (this.isLeaf || this.split.isEmpty) {\n      this.id //如果当前节点是叶子节点或者未分裂的节点，返回当前节点索引\n    } else {\n      val split = this.split.get //当前节点的split\n      val featureIndex = split.featureIndex //当前节点split对应的特征索引\n      //根据数据点在featureIndex特征上的取值，以及featureIndex特征对应的分裂，判断当前数据点是否应该向左传递。\n      val splitLeft = split.shouldGoLeft(binnedFeatures(featureIndex), splits(featureIndex)) \n      if (this.leftChild.isEmpty) { //如果左孩子为空\n        // Not yet split. Return next layer of nodes to train\n        if (splitLeft) { //当前节点应该向左传递，得到左孩子节点索引值\n          LearningNode.leftChildIndex(this.id)\n        } else { //当前节点应该向右传递，得到右孩子节点索引值\n          LearningNode.rightChildIndex(this.id)\n        }\n      } else { //如果左孩子不为空，\n        if (splitLeft) { //当前节点应该向左传递，从左节点开始，递归计算最终节点的索引\n          this.leftChild.get.predictImpl(binnedFeatures, splits)\n        } else { //当前节点应该向右传递，从右节点开始，递归计算最终节点的索引\n          this.rightChild.get.predictImpl(binnedFeatures, splits)\n        }\n      }\n    }\n  }\n```\n\n```\n//对于排序类特征，根据数据点、权重，更新每个特征的每个bin信息        \nprivate def orderedBinSeqOp(\n      agg: DTStatsAggregator, //聚合信息，(feature, bin)\n      treePoint: TreePoint,\n      instanceWeight: Double,\n      featuresForNode: Option[Array[Int]]): Unit = {\n    val label = treePoint.label\n\n    // 如果是采样特征\n    if (featuresForNode.nonEmpty) {\n      // 使用采样的特征，对于每个特征的每个bin，进行更新\n      var featureIndexIdx = 0\n      while (featureIndexIdx < featuresForNode.get.length) {\n        val binIndex = treePoint.binnedFeatures(featuresForNode.get.apply(featureIndexIdx))\n        agg.update(featureIndexIdx, binIndex, label, instanceWeight)\n        featureIndexIdx += 1\n      }\n    } else {\n      // 如果是非采样特征，使用所有特征，对每个特征的每个bin，进行更新\n      val numFeatures = agg.metadata.numFeatures\n      var featureIndex = 0\n      while (featureIndex < numFeatures) {\n        val binIndex = treePoint.binnedFeatures(featureIndex)\n        agg.update(featureIndex, binIndex, label, instanceWeight)\n        featureIndex += 1\n      }\n    }\n  }\n```\n\n```\n//相对于orderedBinSeqOp函数，mixedBinSeqOp函数在同时包括排序和非排序特征情况下，更新聚合信息.\n//对于有序特征，对每个特征更新一个bin\n//对于无序特征，类别的子集对应的bin需要消息，每个子集的靠左bin或者靠右bin需要更新\nprivate def mixedBinSeqOp(\n      agg: DTStatsAggregator, //聚合信息，(feature, bin)\n      treePoint: TreePoint,\n      splits: Array[Array[Split]],\n      unorderedFeatures: Set[Int],\n      instanceWeight: Double,\n      featuresForNode: Option[Array[Int]]): Unit = {\n    val numFeaturesPerNode = if (featuresForNode.nonEmpty) {\n      // 如果特征需要采样，使用采样特征\n      featuresForNode.get.length\n    } else {\n      // 否则使用所有特征\n      agg.metadata.numFeatures\n    }\n    // 迭代每个特征，更新该节点对应的bin聚合信息.\n    var featureIndexIdx = 0\n    while (featureIndexIdx < numFeaturesPerNode) {\n      //得到特征对应的原始索引值\n      val featureIndex = if (featuresForNode.nonEmpty) {\n        featuresForNode.get.apply(featureIndexIdx)\n      } else {\n        featureIndexIdx\n      }\n      if (unorderedFeatures.contains(featureIndex)) {\n        //如果当前特征是无序特征\n        val featureValue = treePoint.binnedFeatures(featureIndex) //得到bin features\n        //得到当前特征偏移量\n        val leftNodeFeatureOffset = agg.getFeatureOffset(featureIndexIdx)\n        // Update the left or right bin for each split.\n        //得到当前特征的split数量\n        val numSplits = agg.metadata.numSplits(featureIndex)\n        //得到当前特征分裂信息\n        val featureSplits = splits(featureIndex)\n        var splitIndex = 0\n        while (splitIndex < numSplits) {\n          //根据当前特征值，判断是否应该向左传递，如果向左传递，则将节点对当前特征的当前区间聚合信息进行更新\n          if (featureSplits(splitIndex).shouldGoLeft(featureValue, featureSplits)) {\n            agg.featureUpdate(leftNodeFeatureOffset, splitIndex, treePoint.label, instanceWeight)\n          }\n          splitIndex += 1\n        }\n      } else {\n        // 如果是有序特征，则直接更新对应特征的对应bin信息\n        val binIndex = treePoint.binnedFeatures(featureIndex)\n        agg.update(featureIndexIdx, binIndex, treePoint.label, instanceWeight)\n      }\n      featureIndexIdx += 1\n    }\n  }\n```\n\n```\n//寻找最佳分裂特征和分裂位置\nprivate[tree] def binsToBestSplit(\n      binAggregates: DTStatsAggregator, //所有feature的bin的统计信息\n      splits: Array[Array[Split]],//所有feature的所有split\n      featuresForNode: Option[Array[Int]],//node对应的feature子集\n      //当前node\n      node: LearningNode): (Split, ImpurityStats) = { //返回值为最佳分裂，及对应的不纯度相关度量\n\n    // Calculate InformationGain and ImpurityStats if current node is top node\n    // 当前节点对应的树的层次\n    val level = LearningNode.indexToLevel(node.id)\n    // 如果是根节点，不纯度度量为0\n    var gainAndImpurityStats: ImpurityStats = if (level == 0) {\n      null\n    } else {\n      //否则为当前节点对应的相关度量stats\n      node.stats\n    }\n    //获得合法的特征分裂\n    val validFeatureSplits =\n      Range(0, binAggregates.metadata.numFeaturesPerNode).view.map { \n      //得到原始特征对应的feature index\n      featureIndexIdx =>\n        featuresForNode.map(features => (featureIndexIdx, features(featureIndexIdx)))\n          .getOrElse((featureIndexIdx, featureIndexIdx))\n      }.withFilter { case (_, featureIndex) => //过滤对应split数量为0的特征\n        binAggregates.metadata.numSplits(featureIndex) != 0\n      }\n\n    //对每个(feature,split), 计算增益，并选择增益最大的(feature,split)\n    val (bestSplit, bestSplitStats) =\n      validFeatureSplits.map { case (featureIndexIdx, featureIndex) =>\n        //得到索引为featureIndex的特征对应的split数量\n        val numSplits = binAggregates.metadata.numSplits(featureIndex)\n        if (binAggregates.metadata.isContinuous(featureIndex)) {\n          //如果是连续特征\n          //计算每个bin的累积统计信息（包括第一个bin到当前bin之间的所有bin对应的统计信息）\n          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)\n          var splitIndex = 0\n          while (splitIndex < numSplits) {\n            binAggregates.mergeForFeature(nodeFeatureOffset, splitIndex + 1, splitIndex)\n            splitIndex += 1\n          }\n          //找到最好的split\n          val (bestFeatureSplitIndex, bestFeatureGainStats) =\n            Range(0, numSplits).map { case splitIdx =>\n              //得到当前split左孩子对应的统计信息\n              val leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, splitIdx)\n              //得到当前split右孩子对应的统计信息， 为得到右孩子对应的统计信息，需要所有的统计信息减去左孩子的统计信息\n              val rightChildStats =\n                binAggregates.getImpurityCalculator(nodeFeatureOffset, numSplits)\n              //所有的统计信息减去左孩子的统计信息\n              rightChildStats.subtract(leftChildStats)\n              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,\n                leftChildStats, rightChildStats, binAggregates.metadata)\n              (splitIdx, gainAndImpurityStats)//分裂索引，不纯度度量信息\n            }.maxBy(_._2.gain)//取信息增益最大的分裂\n          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)\n        } else if (binAggregates.metadata.isUnordered(featureIndex)) {\n          //无序离散特征\n          val leftChildOffset = binAggregates.getFeatureOffset(featureIndexIdx)\n          val (bestFeatureSplitIndex, bestFeatureGainStats) =\n            Range(0, numSplits).map { splitIndex =>\n              //得到左孩子聚合信息\n              val leftChildStats = binAggregates.getImpurityCalculator(leftChildOffset, splitIndex)\n              //得到右孩子聚合信息\n              val rightChildStats = binAggregates.getParentImpurityCalculator()\n                .subtract(leftChildStats)\n              //计算不纯度度量相关统计信息\n              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,\n                leftChildStats, rightChildStats, binAggregates.metadata)\n              (splitIndex, gainAndImpurityStats) //分裂索引，不纯度度量信息\n            }.maxBy(_._2.gain)//取信息增益最大的分裂\n          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)\n        } else {\n          // 对于排序离散特征\n          //得到聚合信息的其实地址\n          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)\n          //得到类别数量\n          val numCategories = binAggregates.metadata.numBins(featureIndex)\n\n          //每个bin是一个特征值，根据质心对这些特征值排序，共K个特征值，对应生成K-1个划分\n          val centroidForCategories = Range(0, numCategories).map { case featureValue =>\n            //得到不纯度度量的统计信息\n            val categoryStats =\n              binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)\n            val centroid = if (categoryStats.count != 0) {//如果对应样本数量不为0，\n              if (binAggregates.metadata.isMulticlass) {\n                //如果是多分类决策树，则将对应多标签的不纯度度量作为质心\n                categoryStats.calculate()\n              } else if (binAggregates.metadata.isClassification) {\n                //如果是二分类问题，则将对应的正样本数量作为质心\n                categoryStats.stats(1)\n              } else {\n                //如果是回归问题，则将对应的预测值作为质心\n                categoryStats.predict\n              }\n            } else {\n              Double.MaxValue //如果对应样本数量为0，则质心为Double.MaxValue\n            }\n            (featureValue, centroid) //返回每个特征值对应的样本质心\n          }\n\n          logDebug(\"Centroids for categorical variable: \" + centroidForCategories.mkString(\",\"))\n\n          // 根据质心，将特征对应的bin排序（即对应的离散特征值排序）\n          val categoriesSortedByCentroid = centroidForCategories.toList.sortBy(_._2)\n\n          logDebug(\"Sorted centroids for categorical variable = \" +\n            categoriesSortedByCentroid.mkString(\",\"))\n\n          // 从左到右，依次计算每个category对应的从第一个category到当前categofy的统计信息聚合结果\n          var splitIndex = 0\n          while (splitIndex < numSplits) {\n            val currentCategory = categoriesSortedByCentroid(splitIndex)._1\n            val nextCategory = categoriesSortedByCentroid(splitIndex + 1)._1\n            binAggregates.mergeForFeature(nodeFeatureOffset, nextCategory, currentCategory)\n            splitIndex += 1\n          }\n          \n          //所有特征值的聚合结果对应的category索引\n          val lastCategory = categoriesSortedByCentroid.last._1\n          //找到最佳的分裂\n          val (bestFeatureSplitIndex, bestFeatureGainStats) =\n            Range(0, numSplits).map { splitIndex =>\n              //得到当前索引的特征值\n              val featureValue = categoriesSortedByCentroid(splitIndex)._1\n              //得到左孩子对应的聚合信息\n              val leftChildStats =\n                binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)\n              //得到右孩子对应的聚合信息\n              val rightChildStats =\n                binAggregates.getImpurityCalculator(nodeFeatureOffset, lastCategory)\n              rightChildStats.subtract(leftChildStats)\n              //得到不纯度度量的相关统计信息\n              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,\n                leftChildStats, rightChildStats, binAggregates.metadata)\n              (splitIndex, gainAndImpurityStats)\n            }.maxBy(_._2.gain)//根据信息增益进行排序，得到信息增益最大的split索引及增益\n          \n          //得到最佳分裂边界\n          val categoriesForSplit =\n            categoriesSortedByCentroid.map(_._1.toDouble).slice(0, bestFeatureSplitIndex + 1)\n          //得到最佳分裂，包括特征索引、划分边界、类别数量等\n          val bestFeatureSplit =\n            new CategoricalSplit(featureIndex, categoriesForSplit.toArray, numCategories)\n           //返回最佳分裂，及对应的增益统计信息\n          (bestFeatureSplit, bestFeatureGainStats)\n        }\n      }.maxBy(_._2.gain)//针对所有特征，按照信息增益进行排序，取增益最大的特征\n\n    (bestSplit, bestSplitStats)//返回最佳分裂，及对应的增益统计信息\n  }\n```\n\n```\n根据分裂对应的左孩子聚合信息，右孩子聚合信息，计算当前节点不纯度度量的相关统计信息\nprivate def calculateImpurityStats(\n      stats: ImpurityStats,\n      leftImpurityCalculator: ImpurityCalculator,\n      rightImpurityCalculator: ImpurityCalculator,\n      metadata: DecisionTreeMetadata): ImpurityStats = {\n    //得到父节点的聚合信息\n    val parentImpurityCalculator: ImpurityCalculator = if (stats == null) {\n      leftImpurityCalculator.copy.add(rightImpurityCalculator)\n    } else {\n      stats.impurityCalculator\n    }\n    //得到父节点不纯度度量\n    val impurity: Double = if (stats == null) {\n      parentImpurityCalculator.calculate()\n    } else {\n      stats.impurity\n    }\n   \n    val leftCount = leftImpurityCalculator.count //根据当前分裂得到的左孩子对应样本数量\n    val rightCount = rightImpurityCalculator.count //根据当前分裂得到的右孩子对应样本数量\n\n    val totalCount = leftCount + rightCount  //当前分裂对应的总样本数量\n\n    // If left child or right child doesn't satisfy minimum instances per node,\n    // then this split is invalid, return invalid information gain stats.\n    //如果左孩子或者右孩子样本数量小于下限值，返回不合法的不纯度度量信息\n    if ((leftCount < metadata.minInstancesPerNode) ||\n      (rightCount < metadata.minInstancesPerNode)) {\n      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)\n    }\n    //左孩子对应的不纯度度量\n    val leftImpurity = leftImpurityCalculator.calculate() // Note: This equals 0 if count = 0\n    //右孩子对应的不纯度度量\n    val rightImpurity = rightImpurityCalculator.calculate()\n    //左孩子权重\n    val leftWeight = leftCount / totalCount.toDouble\n    //右孩子权重\n    val rightWeight = rightCount / totalCount.toDouble\n    //信息增益\n    val gain = impurity - leftWeight * leftImpurity - rightWeight * rightImpurity\n    //信息增益小于下限值，则返回不合法的不纯度度量信息\n      if (gain < metadata.minInfoGain) {\n      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)\n    }\n    //返回不纯度度量信息\n    new ImpurityStats(gain, impurity, parentImpurityCalculator,\n      leftImpurityCalculator, rightImpurityCalculator)\n  }\n```\n\n## 模型预测\n\n通过模型训练生成决策树（随机森林）模型RandomForestModel，随机森林模型继承了树的组合模型TreeEnsembleModel，进一步通过predictBySumming函数，对传进的样本点进行预测。\n\n\n```\n  //对样本点features进行预测\n  private def predictBySumming(features: Vector): Double = {\n    //对每棵决策树进行预测，然后自后结果为每个决策树结果的加权求和\n    val treePredictions = trees.map(_.predict(features))\n    blas.ddot(numTrees, treePredictions, 1, treeWeights, 1)\n  }\n  \n```\n\n```\n  //DecisionTreeModel.predict方法\n  def predict(features: Vector): Double = {\n    //根据头部节点预测lable\n    topNode.predict(features)\n  }\n```\n\n```\n  //Node. predict方法\n  def predict(features: Vector): Double = {\n    if (isLeaf) {\n      predict.predict //如果是叶子节点，直接输出\n    } else {\n      if (split.get.featureType == Continuous) { \n        //如果是连续特征，根据分裂阈值，决定走左孩子节点还是右孩子节点\n        if (features(split.get.feature) <= split.get.threshold) {\n          leftNode.get.predict(features)\n        } else {\n          rightNode.get.predict(features)\n        }\n      } else {\n        //如果是离散特征，根据特征是否被当前节点对应的特征集合包含，决定走左孩子节点还是右孩子节点\n        if (split.get.categories.contains(features(split.get.feature))) {\n          leftNode.get.predict(features)\n        } else {\n          rightNode.get.predict(features)\n        }\n      }\n    }\n  }\n\n```\n\n\n# 参考资料\n\n【1】http://spark.apache.org/mllib/ \n【2】http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html\n","source":"_posts/decision_tree.md","raw":"---\ntitle: spark mllib 决策树算法源码学习\ndate: 2016-12-07\ntoc: true\ncategories: 模型与算法\ntags: [决策树,spark mlilib源码]\ndescription:  决策树算法源码学习，其中模型的训练部分以随机森林的训练过程进行说明，决策树相当于树的数量为1的随机森林\nmathjax: true\n---\n\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n该文章来自于2016年后半年整理的算法源码笔记，由于之前没有写博客的习惯，都直接以笔记的形式存在电脑上，分享起来非常不便，因此抽出时间，将其整理成博客的形式，和大家一起学习交流。\n\n# 决策树算法简要介绍\n\n决策树算法是一种常见的分类算法，也可以用于回归问题。相对于其他分类算法，决策树的优点在于简单,可解释性强；对特征尺度不敏感，不需要做太多的特征预处理工作;能够自动挖掘特征之间的关联关系。缺点是比较容易过拟合（通过随机森林可以避免过拟合）\n\n决策树是一个树形结构，其中叶子节点表示分类（或回归）结果，非叶子节点是属性判断判断节点，每个属性判断节点都选择样本的一个特征，并根据该特征的取值决定选择哪一个分支路径。在对样本进行预测时，从根节点开始直到叶子节点，对于路径上的每个分支节点，都根据其对应的属性取值选择下一个分支节点，直到叶子节点。整个完整的路径，表示对样本的预测过程。如图1所示，表示一个女孩在决定是否决定去相亲的一个过程，最终选择去或者不去，对应分类的结果，中间的各种条件对应相关的属性。\n\n<center>\n![“决策树样例”](/decision_tree/decision_tree_example.png)\n</center>\n<center>图1：决策树样例：对女孩决定是否参加相亲的问题进行决策树建模</center>\n \n\n## 决策树的训练\n\n从根节点开始，根据信息增益或其他条件，不断选择分裂的属性，直到生成叶子节点的过程。具体过程如下所示：\n* 对不同的属性，计算其信息增益，选择增益最大的特征对应根节点的最佳分裂。\n* 从根节点开始，对于不同的分支节点，分别选择信息增益最大的特征作为分支节点的最佳分裂。\n* 如果达到停止分裂的条件，则将该节点作为叶子节点：当前节点对应的样本都是一类样本，分类结果为对应的样本的类别；总样本数量小于一定值，或者树的高度达到最大值，或者信息增益小于一定值，或者已经用完所有的属性，选择占比最大的样本分类作为节点对应的分类结果。否则，根据步骤2进一步构造分裂节点。\n\n\n## 属性度量\n\n\n决策树构建的关键，在于不断地选择最佳分裂属性。属性的收益度量方法，常见的有信息增益（ID3算法）、信息增益率（C4.5算法），基尼系数(CART算法)等。\n\n**ID3算法:**\n\n熵：信息论中，用于描述信息的不确定性，定义如式1，其中$D$表示对样本的一个划分，$m$表示划分的类别数量，$p\\_i$表示第i个类别的样本数量比例。\n\n$info(D)=-\\sum\\_{i=1}^m p\\_ilog\\_2(p\\_i)\\;\\;\\;（式1）$\n\n假设按照属性A对样本D进行划分，$v$为属性$A$的划分数量。则$A$对$D$划分的期望熵如式2：\n\n$info\\_A(D)=\\sum\\_{j=1}^v\\frac{|D\\_j|}{|D|}info(D\\_j)\\;\\;\\;（式2）$\n\n信心增益为上述原始熵和属性A对D划分后期望熵的差值，可以看做是加入信息A后，不确定性的减少程度。信息增益的定义如式3所示：\n\n$gain(A)=info(D)-info\\_A(D)\\;\\;\\;（式3）$\n\nID3算法即在每次选择最佳分裂的属性时，根据信息增益进行选择。\n\n**C4.5算法:**\nID3算法容易使得选取值较多的属性。一种极端的情况是，对于ID类特征有很多的无意义的值的划分，ID3会选择该属性其作为最佳划分。C4.5算法通过采用信息增益率作为衡量特征有效性的指标，可以克服这个问题。\n\n首先定义分裂信息：\n$splitInfo\\_A(D)=-\\sum\\_{j=1}^v\\frac{|D\\_j|}{|D|}log\\_2(\\frac{|D\\_j|}{|D|})\\;\\;\\;（式4）$\n\n信息增益率：\n$gainRatio(A)=\\frac{gain(A)}{splitInfo\\_A(D)}\\;\\;\\;（式5）$\n\n**CART算法:**\n\n使用基尼系数作为不纯度的度量。\n基尼系数:表示在样本集合中一个随机选中的样本被分错的概率，Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。当所有样本属于一个类别时，基尼系数最小为0。所有类别以等概率出现时，基尼系数最大。\n$GINI(P)=\\sum\\_{k=1}^Kp\\_k(1-p\\_k)=1-\\sum\\_{k=1}^K p\\_k^2\\;\\;\\;（式6）$\n\n由于cart建立的树是个二叉树，所以K的取值为2。对于特征取值超过2的情况，以每个取值作为划分点，计算该划分下对应的基尼系数的期望。期望值最小的划分点，作为最佳分裂使用的特征划分。\n\n\n\n# spark 决策树源码分析\n\n为加深对ALS算法的理解，该部分主要分析spark mllib中决策树源码的实现。主要包括模型训练、模型预测2个部分\n\n##  模型训练\n\n### 决策树伴生类\n    \nDecisionTree伴随类，外部调用决策树模型进行训练的入口。通过外部传入数据和配置参数，调用DecisionTree中的run方法进行模型训练， 最终返回DecisionTreeModel类型对象。\n\n```scala\nobject DecisionTree extends Serializable with Logging {\n def train(\n      input: RDD[LabeledPoint], //训练数据，包括label和特征向量\n      algo: Algo,//决策树类型，分类树or回归树\n      impurity: Impurity,//衡量特征信息增益的标准，如信息增益、基尼、方差\n      maxDepth: Int,//树的深度\n      numClasses: Int,//待分类类别的数量\n      maxBins: Int,//用于特征分裂的bin的最大数量\n      quantileCalculationStrategy: QuantileStrategy,//计算分位数的算法\n      //离散特征存储，如n->k表示第n个特征有k个取值（0，1，..., k-1）\n      categoricalFeaturesInfo: Map[Int, Int]): DecisionTreeModel = { \n    //根据参数信息，生成决策树配置\n    val strategy = new Strategy(algo, impurity, maxDepth, numClasses, maxBins,\n      quantileCalculationStrategy, categoricalFeaturesInfo)\n    //调用DecisionTree对象的run方法，训练决策树模型\n    new DecisionTree(strategy).run(input)\n  }\n   //训练分类决策树\n   def trainClassifier(\n      input: RDD[LabeledPoint],\n      numClasses: Int,\n      categoricalFeaturesInfo: Map[Int, Int],\n      impurity: String,\n      maxDepth: Int,\n      maxBins: Int): DecisionTreeModel = {\n    val impurityType = Impurities.fromString(impurity)\n    train(input, Classification, impurityType, maxDepth, numClasses, maxBins, Sort,categoricalFeaturesInfo)\n  }\n    //训练回归决策树\n    def trainRegressor(\n      input: RDD[LabeledPoint],\n      categoricalFeaturesInfo: Map[Int, Int],\n      impurity: String,\n      maxDepth: Int,\n      maxBins: Int): DecisionTreeModel = {\n    val impurityType = Impurities.fromString(impurity) //基尼、熵、方差三种衡量标准\n    train(input, Regression, impurityType, maxDepth, 0, maxBins, Sort, categoricalFeaturesInfo)\n  }\n}\n```\n\n### 决策树类\n\n接受strategy参数初始化，并通过对run方法调用随机森林的run方法，通过设置特征集合为全集、树的个数为1，将随机森林训练后结果集中的第一棵树作为结果返回。\n\n```\nclass DecisionTree private[spark] (private val strategy: Strategy, private val seed: Int)\n  extends Serializable with Logging {\n  def run(input: RDD[LabeledPoint]): DecisionTreeModel = {\n    val rf = new RandomForest(strategy, numTrees = 1, featureSubsetStrategy = \"all\", seed = seed)\n    val rfModel = rf.run(input)\n    rfModel.trees(0)\n  }\n}\n```\n\n### RandomForest私有类run方法,通过run方法完成模型的训练\n\n**分布式训练思想：**\n\n*\t分布式存储样本\n*\t对于每次迭代，算法都会对一个node集合进行分裂。对于每个node，相关worker计算的的所有相关统计特征全部传递到某个worker进行汇总，并选择最好的特征分裂\n*\tfindSplitsBins方法可用于将连续特征离散化，在初始化阶段完成\n*\t迭代算法\n   每次都作用于树的边缘节点，如果是随机森林，则选择所有的树的边缘节点。具体迭代步骤如下：\n   1. Master 节点: 从node queue中选取节点，如果训练的是随机森林,且featureSubsetStrategy取值不是all，则对于每个节点选择随机特征子集。selectNodesToSplit用于选择待分裂的节点。\n   2. Worer节点: findBestSplits函数，对每个(tree, node, feature, split)，遍历所有本地所有样本计算相关特征，计算结果通过reduceByKey传递给某个节点，由该节点汇总数据，得到(feature, split)或者判断是否停止分裂\n   3. Master节点: 收集所有节点分裂信息，更新model, 并将新的model传递给各个worker节点 \n\n#### \n```\ndef run(\n      input: RDD[LabeledPoint],\n      strategy: OldStrategy,\n      numTrees: Int,\n      featureSubsetStrategy: String,\n      seed: Long,\n      instr: Option[Instrumentation[_]],\n      parentUID: Option[String] = None): Array[DecisionTreeModel] = {\n    val timer = new TimeTracker()\n    timer.start(\"total\")\n    timer.start(\"init\")\n    \n    val retaggedInput = input.retag(classOf[LabeledPoint])\n    //构建元数据\n    val metadata =\n      DecisionTreeMetadata.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)\n    instr match {\n      case Some(instrumentation) =>\n        instrumentation.logNumFeatures(metadata.numFeatures)\n        instrumentation.logNumClasses(metadata.numClasses)\n      case None =>\n        logInfo(\"numFeatures: \" + metadata.numFeatures)\n        logInfo(\"numClasses: \" + metadata.numClasses)\n    }\n\n\n    //每个特征对应的splits和bins\n    timer.start(\"findSplits\")\n    val splits = findSplits(retaggedInput, metadata, seed)\n    timer.stop(\"findSplits\")\n    logDebug(\"numBins: feature: number of bins\")\n    logDebug(Range(0, metadata.numFeatures).map { featureIndex =>\n      s\"\\t$featureIndex\\t${metadata.numBins(featureIndex)}\"\n    }.mkString(\"\\n\"))\n\n    // Bin feature values (TreePoint representation).\n    // Cache input RDD for speedup during multiple passes.\n    //输入\n    val treeInput = TreePoint.convertToTreeRDD(retaggedInput, splits, metadata)\n\n    val withReplacement = numTrees > 1\n\n    val baggedInput = BaggedPoint\n      .convertToBaggedRDD(treeInput, strategy.subsamplingRate, numTrees, withReplacement, seed)\n      .persist(StorageLevel.MEMORY_AND_DISK)\n\n    // depth of the decision tree\n    val maxDepth = strategy.maxDepth\n    require(maxDepth <= 30,\n      s\"DecisionTree currently only supports maxDepth <= 30, but was given maxDepth = $maxDepth.\")\n\n    // Max memory usage for aggregates\n    // TODO: Calculate memory usage more precisely.\n    val maxMemoryUsage: Long = strategy.maxMemoryInMB * 1024L * 1024L\n    logDebug(\"max memory usage for aggregates = \" + maxMemoryUsage + \" bytes.\")\n\n    /*\n     * The main idea here is to perform group-wise training of the decision tree nodes thus\n     * reducing the passes over the data from (# nodes) to (# nodes / maxNumberOfNodesPerGroup).\n     * Each data sample is handled by a particular node (or it reaches a leaf and is not used\n     * in lower levels).\n     */\n\n    // Create an RDD of node Id cache.\n    // At first, all the rows belong to the root nodes (node Id == 1).\n    val nodeIdCache = if (strategy.useNodeIdCache) {\n      Some(NodeIdCache.init(\n        data = baggedInput,\n        numTrees = numTrees,\n        checkpointInterval = strategy.checkpointInterval,\n        initVal = 1))\n    } else {\n      None\n    }\n\n    /*\n      Stack of nodes to train: (treeIndex, node)\n      The reason this is a stack is that we train many trees at once, but we want to focus on\n      completing trees, rather than training all simultaneously.  If we are splitting nodes from\n      1 tree, then the new nodes to split will be put at the top of this stack, so we will continue\n      training the same tree in the next iteration.  This focus allows us to send fewer trees to\n      workers on each iteration; see topNodesForGroup below.\n     */\n    val nodeStack = new mutable.Stack[(Int, LearningNode)]\n\n    val rng = new Random()\n    rng.setSeed(seed)\n\n    // Allocate and queue root nodes.\n    val topNodes = Array.fill[LearningNode](numTrees)(LearningNode.emptyNode(nodeIndex = 1))\n    Range(0, numTrees).foreach(treeIndex => nodeStack.push((treeIndex, topNodes(treeIndex))))\n\n    timer.stop(\"init\")\n\n    while (nodeStack.nonEmpty) {\n      // Collect some nodes to split, and choose features for each node (if subsampling).\n      // Each group of nodes may come from one or multiple trees, and at multiple levels.\n      val (nodesForGroup, treeToNodeToIndexInfo) =\n        RandomForest.selectNodesToSplit(nodeStack, maxMemoryUsage, metadata, rng)\n      // Sanity check (should never occur):\n      assert(nodesForGroup.nonEmpty,\n        s\"RandomForest selected empty nodesForGroup.  Error for unknown reason.\")\n\n      // Only send trees to worker if they contain nodes being split this iteration.\n      val topNodesForGroup: Map[Int, LearningNode] =\n        nodesForGroup.keys.map(treeIdx => treeIdx -> topNodes(treeIdx)).toMap\n\n      // Choose node splits, and enqueue new nodes as needed.\n      timer.start(\"findBestSplits\")\n      RandomForest.findBestSplits(baggedInput, metadata, topNodesForGroup, nodesForGroup,\n        treeToNodeToIndexInfo, splits, nodeStack, timer, nodeIdCache)\n      timer.stop(\"findBestSplits\")\n    }\n\n    baggedInput.unpersist()\n\n    timer.stop(\"total\")\n\n    logInfo(\"Internal timing for DecisionTree:\")\n    logInfo(s\"$timer\")\n\n    // Delete any remaining checkpoints used for node Id cache.\n    if (nodeIdCache.nonEmpty) {\n      try {\n        nodeIdCache.get.deleteAllCheckpoints()\n      } catch {\n        case e: IOException =>\n          logWarning(s\"delete all checkpoints failed. Error reason: ${e.getMessage}\")\n      }\n    }\n\n    val numFeatures = metadata.numFeatures\n\n    parentUID match {\n      case Some(uid) =>\n        if (strategy.algo == OldAlgo.Classification) {\n          topNodes.map { rootNode =>\n            new DecisionTreeClassificationModel(uid, rootNode.toNode, numFeatures,\n              strategy.getNumClasses)\n          }\n        } else {\n          topNodes.map { rootNode =>\n            new DecisionTreeRegressionModel(uid, rootNode.toNode, numFeatures)\n          }\n        }\n      case None =>\n        if (strategy.algo == OldAlgo.Classification) {\n          topNodes.map { rootNode =>\n            new DecisionTreeClassificationModel(rootNode.toNode, numFeatures,\n              strategy.getNumClasses)\n          }\n        } else {\n          topNodes.map(rootNode => new DecisionTreeRegressionModel(rootNode.toNode, numFeatures))\n        }\n    }\n  }\n```\n\n\n\n\n#### buildMetadata\n决策树训练的元数据构造。主要用于计算每个特征的bin数量，以及无序类特征集合, 每个节点使用的特征数量等。其中决策树一般使用所有特征、随机森林分类采用$sqrt(n)$个特征，随机森林回归采用$\\frac{n}{3}$个特征\n\n\n```\ndef buildMetadata(\n      input: RDD[LabeledPoint],\n      strategy: Strategy,\n      numTrees: Int,\n      featureSubsetStrategy: String): DecisionTreeMetadata = {\n    //特征数量\n    val numFeatures = input.map(_.features.size).take(1).headOption.getOrElse {\n      throw new IllegalArgumentException(s\"DecisionTree requires size of input RDD > 0, \" +\n        s\"but was given by empty one.\")\n    }\n    val numExamples = input.count() //样本数量\n    val numClasses = strategy.algo match {\n      case Classification => strategy.numClasses\n      case Regression => 0\n    }\n    //最大划分数量 \n    val maxPossibleBins = math.min(strategy.maxBins, numExamples).toInt\n    if (maxPossibleBins < strategy.maxBins) {\n      logWarning(s\"DecisionTree reducing maxBins from ${strategy.maxBins} to $maxPossibleBins\" +\n        s\" (= number of training instances)\")\n    }\n    //maxPossibleBins可能被numExamples修改过，导致小于刚开始设置的strategy.maxBins。\n    //需要进一步确保离散值的特征取值数量小于maxPossibleBins，\n    if (strategy.categoricalFeaturesInfo.nonEmpty) {\n      val maxCategoriesPerFeature = strategy.categoricalFeaturesInfo.values.max\n      val maxCategory =\n        strategy.categoricalFeaturesInfo.find(_._2 == maxCategoriesPerFeature).get._1\n      require(maxCategoriesPerFeature <= maxPossibleBins,\n        s\"DecisionTree requires maxBins (= $maxPossibleBins) to be at least as large as the \" +\n        s\"number of values in each categorical feature, but categorical feature $maxCategory \" +\n        s\"has $maxCategoriesPerFeature values. Considering remove this and other categorical \" +\n        \"features with a large number of values, or add more training examples.\")\n    }\n    //存储每个无序特征的索引\n    val unorderedFeatures = new mutable.HashSet[Int]()\n    //存储每个无序特征的bin数量\n    val numBins = Array.fill[Int](numFeatures)(maxPossibleBins)\n    if (numClasses > 2) { //多分类问题\n      //根据maxPossibleBins，计算每个无序特征对应的最大类别数量\n      val maxCategoriesForUnorderedFeature =\n        ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt\n      strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =>\n        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤\n          if (numCategories > 1) {\n          //判断离散特征是否可当做无序特征，需要保证\n          //bins的数量需要小于2 * ((1 << numCategories - 1) - 1)）\n          if (numCategories <= maxCategoriesForUnorderedFeature) {\n            unorderedFeatures.add(featureIndex)\n            //有numCategories个取值的的特征，对应bins数量为(1 << numCategories - 1) - 1\n            //此处刚开始有点疑惑，感觉应该是2 *（(1 << numCategories - 1) - 1）\n            //通过DecisionTreeMetadata中numSplits函数发现，此处的bin数量和split数量有一定对应关系，(featureIndex)\n           //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1\n            numBins(featureIndex) = numUnorderedBins(numCategories)\n          } else {\n            //对于其他离散特征，numBins数量为特征可能的取值数量\n            numBins(featureIndex) = numCategories\n          }\n        }\n      }\n    } else { //对于二值分类或回归问题\n      strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =>\n        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤\n        if (numCategories > 1) {\n          //numBins数量为特征可能的取值数量\n          numBins(featureIndex) = numCategories \n        }\n      }\n    }\n\n    //设置每个分支节点对应的特征数量\n    val _featureSubsetStrategy = featureSubsetStrategy match {\n      case \"auto\" =>\n        if (numTrees == 1) { //如果是树，使用所有特征n\n          \"all\"\n        } else {\n          if (strategy.algo == Classification) { //如果是用于分类的随机森林，使用sqrt(n)个特征\n            \"sqrt\"\n          } else {\n            \"onethird\"  //如果是用于回归的随机森林，使用n/3个特征\n          }\n        }\n      case _ => featureSubsetStrategy\n    }\n\n    val numFeaturesPerNode: Int = _featureSubsetStrategy match {\n      case \"all\" => numFeatures\n      case \"sqrt\" => math.sqrt(numFeatures).ceil.toInt\n      case \"log2\" => math.max(1, (math.log(numFeatures) / math.log(2)).ceil.toInt)\n      case \"onethird\" => (numFeatures / 3.0).ceil.toInt\n      case _ =>\n        Try(_featureSubsetStrategy.toInt).filter(_ > 0).toOption match {\n          case Some(value) => math.min(value, numFeatures)\n          case None =>\n            Try(_featureSubsetStrategy.toDouble).filter(_ > 0).filter(_ <= 1.0).toOption match {\n              case Some(value) => math.ceil(value * numFeatures).toInt\n              case _ => throw new IllegalArgumentException(s\"Supported values:\" +\n                s\" ${RandomForestParams.supportedFeatureSubsetStrategies.mkString(\", \")},\" +\n                s\" (0.0-1.0], [1-n].\")\n            }\n        }\n    }\n\n    new DecisionTreeMetadata(numFeatures, numExamples, numClasses, numBins.max,\n      strategy.categoricalFeaturesInfo, unorderedFeatures.toSet, numBins,\n      strategy.impurity, strategy.quantileCalculationStrategy, strategy.maxDepth,\n      strategy.minInstancesPerNode, strategy.minInfoGain, numTrees, numFeaturesPerNode)\n  }\n```\n\n#### DecisionTreeMetadata类\n```  \nprivate[spark] class DecisionTreeMetadata(\n    val numFeatures: Int,\n    val numExamples: Long,\n    val numClasses: Int,\n    val maxBins: Int,\n    val featureArity: Map[Int, Int],\n    val unorderedFeatures: Set[Int],\n    val numBins: Array[Int],\n    val impurity: Impurity,\n    val quantileStrategy: QuantileStrategy,\n    val maxDepth: Int,\n    val minInstancesPerNode: Int,\n    val minInfoGain: Double,\n    val numTrees: Int,\n    val numFeaturesPerNode: Int) extends Serializable {\n  //判断是否为无序特征\n  def isUnordered(featureIndex: Int): Boolean = unorderedFeatures.contains(featureIndex)\n  //判断是否用于分类的决策树（随机森林）\n  def isClassification: Boolean = numClasses >= 2\n  //判断是否用于多分类的决策树（随机森林）\n  def isMulticlass: Boolean = numClasses > 2\n  //判断是否拥有离散特征的多分类决策树（随机森林）\n  def isMulticlassWithCategoricalFeatures: Boolean = isMulticlass && (featureArity.size > 0)\n  //判断是否离散特征\n  def isCategorical(featureIndex: Int): Boolean = featureArity.contains(featureIndex)\n //判断是否连续特征\n  def isContinuous(featureIndex: Int): Boolean = !featureArity.contains(featureIndex)\n  //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1\n  def numSplits(featureIndex: Int): Int = if (isUnordered(featureIndex)) {\n    numBins(featureIndex)\n  } else {\n    numBins(featureIndex) - 1\n  }\n  //对于连续特征，根据划分数量设置bin数量为划分数量加1\n  def setNumSplits(featureIndex: Int, numSplits: Int) {\n    require(isContinuous(featureIndex),\n      s\"Only number of bin for a continuous feature can be set.\")\n    numBins(featureIndex) = numSplits + 1\n  }\n  //判断是否需要对特征进行采样\n  def subsamplingFeatures: Boolean = numFeatures != numFeaturesPerNode\n}    \n```\n\n#### findSplits\n通过使用采样的样本，寻找样本的划分splits和划分后的bins。\n\n**划分的思想：**对连续特征和离散特征，分别采用不同处理方式。对于每个连续特征，numBins - 1个splits, 代表每个树的节点的所有可能的二值化分；对于每个离散特征，无序离散特征（用于多分类的维度较大的feature）基于特征的子集进行划分。有序类特征（用于回归、二分类、多分类的维度较小的feature)的每个取值对应一个bin.\n\n```\nprotected[tree] def findSplits(\n      input: RDD[LabeledPoint],\n      metadata: DecisionTreeMetadata,\n      seed: Long): Array[Array[Split]] = {\n    logDebug(\"isMulticlass = \" + metadata.isMulticlass)\n    val numFeatures = metadata.numFeatures //特征的数量\n    // 得到所有连续特征索引\n    val continuousFeatures = Range(0, numFeatures).filter(metadata.isContinuous)\n    //当有连续特征的时候需要采样样本   \n    val sampledInput = if (continuousFeatures.nonEmpty) {\n      // 计算近似分位数计算需要的样本数\n      val requiredSamples = math.max(metadata.maxBins * metadata.maxBins, 10000)\n      // 计算需要的样本占总样本比例\n      val fraction = if (requiredSamples < metadata.numExamples) {\n        requiredSamples.toDouble / metadata.numExamples\n      } else {\n        1.0\n      }\n      logDebug(\"fraction of data used for calculating quantiles = \" + fraction)\n      input.sample(withReplacement = false, fraction, new XORShiftRandom(seed).nextInt())\n    } else {\n      input.sparkContext.emptyRDD[LabeledPoint]\n    }\n    //对每个连续特征和非有序类离散特征，通过排序的方式，寻找最佳的splits点\n    findSplitsBySorting(sampledInput, metadata, continuousFeatures)\n  }\n```\n\n```\n //对每个特征，通过排序的方式，寻找最佳的splits点\n private def findSplitsBySorting(\n      input: RDD[LabeledPoint],\n      metadata: DecisionTreeMetadata,\n      continuousFeatures: IndexedSeq[Int]): Array[Array[Split]] = {\n   \n    //寻找连续特征的划分阈值\n    val continuousSplits: scala.collection.Map[Int, Array[Split]] = {\n      //设置分区数量，如果连续特征的数量小于原始分区数，则进一步减少分区，防止无效的启动的task任务。\n      val numPartitions = math.min(continuousFeatures.length, input.partitions.length)\n\n      input\n        .flatMap(point => continuousFeatures.map(idx => (idx, point.features(idx))))\n        .groupByKey(numPartitions)\n        .map { case (idx, samples) =>\n          val thresholds = findSplitsForContinuousFeature(samples, metadata, idx)\n          val splits: Array[Split] = thresholds.map(thresh => new ContinuousSplit(idx, thresh))\n          logDebug(s\"featureIndex = $idx, numSplits = ${splits.length}\")\n          (idx, splits)\n        }.collectAsMap()\n    }\n    //特征数量\n    val numFeatures = metadata.numFeatures\n    //汇总所有特征的split(不包括无序离散特征)\n    val splits: Array[Array[Split]] = Array.tabulate(numFeatures) {\n      //如果是连续特征，返回该连续特征的split\n      case i if metadata.isContinuous(i) =>\n        val split = continuousSplits(i)\n        metadata.setNumSplits(i, split.length)\n        split\n      //如果是无序离散特征，则提取该特征的split， 具体是对于每个离散特征，其第k个split为其k对应二进制的所有位置为1的数值。\n      case i if metadata.isCategorical(i) && metadata.isUnordered(i) =>\n        // Unordered features\n        // 2^(maxFeatureValue - 1) - 1 combinations\n        //特征的取值数量\n        val featureArity = metadata.featureArity(i)\n        Array.tabulate[Split](metadata.numSplits(i)) { splitIndex =>\n          val categories = extractMultiClassCategories(splitIndex + 1, featureArity)\n          new CategoricalSplit(i, categories.toArray, featureArity)\n        }\n      //对于有序离散特征，暂时不求解split, 在训练阶段求解\n      case i if metadata.isCategorical(i) =>\n        // Ordered features\n        //   Splits are constructed as needed during training.\n        Array.empty[Split]\n    }\n    splits\n  }\n\n```\n\n```\n//将input这个数对应的二进制位置为1的位置加入到当前划分\nprivate[tree] def extractMultiClassCategories(\n      input: Int,\n      maxFeatureValue: Int): List[Double] = {\n    var categories = List[Double]()\n    var j = 0\n    var bitShiftedInput = input\n    while (j < maxFeatureValue) {\n      if (bitShiftedInput % 2 != 0) {\n        // updating the list of categories.\n        categories = j.toDouble :: categories\n      }\n      // Right shift by one\n      bitShiftedInput = bitShiftedInput >> 1\n      j += 1\n    }\n    categories\n  }\n```\n\n```\n//对于连续特征，找到其对应的splits分割点\nprivate[tree] def findSplitsForContinuousFeature(\n      featureSamples: Iterable[Double], \n      metadata: DecisionTreeMetadata, \n      featureIndex: Int): Array[Double] = {\n    //确保有连续特征\n    require(metadata.isContinuous(featureIndex),\n      \"findSplitsForContinuousFeature can only be used to find splits for a continuous feature.\")\n    //寻找splits分割点\n    val splits = if (featureSamples.isEmpty) {\n      Array.empty[Double]  //如果样本数为0， 返回空数组\n    } else {\n      //得到metadata里的split数量\n      val numSplits = metadata.numSplits(featureIndex) \n\n      //在采样得到的样本中，计算每个特征取值的计数、以及总样本数量\n      val (valueCountMap, numSamples) = featureSamples.foldLeft((Map.empty[Double, Int], 0)) {\n        case ((m, cnt), x) =>\n          (m + ((x, m.getOrElse(x, 0) + 1)), cnt + 1)\n      }\n      // 对于每个特征取值进行排序\n      val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n      //如果得到的possible splits数量小于metadata中该特征的的split数量，则直接以当前每个特征取值作为分割的阈值\n      val possibleSplits = valueCounts.length - 1\n      if (possibleSplits <= numSplits) { \n        valueCounts.map(_._1).init\n      } else {\n        //否则，根据总样本数量，计算平均每个区间对应的特征取值数量，假设为n。然后，对于n, 2*n, 3*n ...的位置分别设置标记。设置2个游标分别指向valueCounts内部连续的两个特征取值，从前向后遍历，当后面游标到标记的距离大于前面的游标时，将前面游标的位置对应的特征取值设置为一个split点。\n        //计算平均每个区间对应的特征取值数量\n        val stride: Double = numSamples.toDouble / (numSplits + 1)\n        logDebug(\"stride = \" + stride)\n        //splitsBuilder用于存储每个分割阈值\n        val splitsBuilder = mutable.ArrayBuilder.make[Double]\n        //特征取值从小到大的位置索引\n        var index = 1\n        //当前访问的所有特征取值数量之和\n        var currentCount = valueCounts(0)._2\n        //下一次的标记位置      \n        var targetCount = stride\n        while (index < valueCounts.length) {\n          val previousCount = currentCount\n          currentCount += valueCounts(index)._2\n          val previousGap = math.abs(previousCount - targetCount)\n          val currentGap = math.abs(currentCount - targetCount)\n          //使前面游标和后面游标的距离更小，且较小游标距离标记位置的距离最近\n          if (previousGap < currentGap) {\n            splitsBuilder += valueCounts(index - 1)._1\n            targetCount += stride\n          }\n          index += 1\n        }\n        splitsBuilder.result()\n      }\n    }\n    splits\n  }\n\n```\n\n#### TreePoint.convertToTreeRDD\n调用TreePoint类的convertToTreeRDD方法，RDD[LabeledPoint]转化为RDD[TreePoint]。\n\n```\n def convertToTreeRDD(\n      input: RDD[LabeledPoint],\n      splits: Array[Array[Split]],\n      metadata: DecisionTreeMetadata): RDD[TreePoint] = {\n    // 构建数组featureArity，存储每个特征对应的离散值个数，连续值对应的value为0\n    val featureArity: Array[Int] = new Array[Int](metadata.numFeatures)\n    var featureIndex = 0\n    while (featureIndex < metadata.numFeatures) {\n      featureArity(featureIndex) = metadata.featureArity.getOrElse(featureIndex, 0)\n      featureIndex += 1\n    }\n    //获得所有连续特征的分裂阈值，如果是离散特征，则数组对应空\n    val thresholds: Array[Array[Double]] = featureArity.zipWithIndex.map { case (arity, idx) =>\n      if (arity == 0) {\n        splits(idx).map(_.asInstanceOf[ContinuousSplit].threshold)\n      } else {\n        Array.empty[Double]\n      }\n    }\n    //将样本的每个原始特征，转化为对应的bin特征值，用于训练\n    input.map { x =>\n      TreePoint.labeledPointToTreePoint(x, thresholds, featureArity)\n    }\n  }\n```\n\n```\n  //将单个样本的原始特征，转化为对应的bin特征值，用于训练\n  private def labeledPointToTreePoint(\n      labeledPoint: LabeledPoint,\n      thresholds: Array[Array[Double]],\n      featureArity: Array[Int]): TreePoint = {\n    //特征数量\n    val numFeatures = labeledPoint.features.size\n    //为每个特征找到对应的bin特征值，存储在arr数组\n    val arr = new Array[Int](numFeatures)\n    var featureIndex = 0\n    while (featureIndex < numFeatures) {\n      //寻找数据点labeledPoint、当前特征featureIndex对应的bin特征值\n      arr(featureIndex) =\n        findBin(featureIndex, labeledPoint, featureArity(featureIndex), thresholds(featureIndex))\n      featureIndex += 1\n    }\n    new TreePoint(labeledPoint.label, arr)\n  }\n```\n\n```\nprivate def findBin(\n      featureIndex: Int,\n      labeledPoint: LabeledPoint,\n      featureArity: Int,\n      thresholds: Array[Double]): Int = {\n    //获取当前labeledPoint的第featureIndex个原始特征值\n    val featureValue = labeledPoint.features(featureIndex)\n    \n    if (featureArity == 0) { \n      //如果是连续特征，利用二分法得到当前特征值对应的离散区间下标\n      val idx = java.util.Arrays.binarySearch(thresholds, featureValue)\n      if (idx >= 0) {\n        idx\n      } else {\n        -idx - 1\n      }\n    } else {\n      //如果是离散值，则直接返回当前的特征值\n      if (featureValue < 0 || featureValue >= featureArity) {\n        throw new IllegalArgumentException(\n          s\"DecisionTree given invalid data:\" +\n            s\" Feature $featureIndex is categorical with values in {0,...,${featureArity - 1},\" +\n            s\" but a data point gives it value $featureValue.\\n\" +\n            \"  Bad data point: \" + labeledPoint.toString)\n      }\n      featureValue.toInt\n    }\n  }\n```\n\n\n```\n//LabeledPoint类\ncase class LabeledPoint(@Since(\"2.0.0\") label: Double, @Since(\"2.0.0\") features: Vector) {\n  override def toString: String = {\n    s\"($label,$features)\"\n  }\n}\n```\n```\n//TreePoint类\nprivate[spark] class TreePoint(val label: Double, val binnedFeatures: Array[Int])\n  extends Serializable {\n}\n```\n#### BaggedPoint.convertToBaggedRDD\nRDD[Datum]数据集转换成RDD[BaggedPoint[Datum]的表示类型，\n\n```\n  def convertToBaggedRDD[Datum] (\n      input: RDD[Datum], //输入数据集\n      subsamplingRate: Double, //采样率\n      numSubsamples: Int, //采样次数\n      withReplacement: Boolean, //是否有放回\n      //随机数种子\n      seed: Long = Utils.random.nextLong()): RDD[BaggedPoint[Datum]] = {\n    if (withReplacement) {//有放回采样，生成BaggedPoint结构表示\n      convertToBaggedRDDSamplingWithReplacement(input, subsamplingRate, numSubsamples, seed)\n    } else {\n      //当采样比为1，并且采样次数为1时，不采样，只生成BaggedPoint结构表示\n      if (numSubsamples == 1 && subsamplingRate == 1.0) {\n        convertToBaggedRDDWithoutSampling(input)\n      } else {\n        //无放回采样，生成BaggedPoint结构表示\n        convertToBaggedRDDSamplingWithoutReplacement(input, subsamplingRate, numSubsamples, seed)\n      }\n    }\n  }\n```\n```\n  //有放回采样，数据转换为RDD[BaggedPoint[Datum]]\n  private def convertToBaggedRDDSamplingWithReplacement[Datum] (\n      input: RDD[Datum],//输入数据集\n      subsample: Double,//采样率\n      numSubsamples: Int,//采样次数\n      //随机数种子\n      seed: Long): RDD[BaggedPoint[Datum]] = {\n    input.mapPartitionsWithIndex { (partitionIndex, instances) =>\n      //每个分区生成一个泊松采样器，通过采样率、随机种子、分区索引等初始化\n      val poisson = new PoissonDistribution(subsample)\n      poisson.reseedRandomGenerator(seed + partitionIndex + 1)\n      //将每个实例变换成BaggedPoint结构表示\n      instances.map { instance =>\n        val subsampleWeights = new Array[Double](numSubsamples)\n        var subsampleIndex = 0\n        //依次对每次采样，生成权重（即该实例在每次无放回采样出现的次数）\n        while (subsampleIndex < numSubsamples) {\n          subsampleWeights(subsampleIndex) = poisson.sample()\n          subsampleIndex += 1\n        }\n        //生成BaggedPoint结构表示\n        new BaggedPoint(instance, subsampleWeights) \n      }\n    }\n  }\n```\n```\n//BaggedPoint类，datum表示数据实例，subsampleWeights表示当前实例在每个采样中的权重。\n如(datum, [1, 0, 4])表示有3次采样，数据实例在3次采样中出现的次数分别为1，0，4\nprivate[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])\n  extends Serializable\n```\n```\n  //原始数据（不采样）直接转换为BaggedPoint结构表示\n  private def convertToBaggedRDDWithoutSampling[Datum] (\n      input: RDD[Datum]): RDD[BaggedPoint[Datum]] = {\n    input.map(datum => new BaggedPoint(datum, Array(1.0)))\n  }\n```\n\n```\n  //无放回采样，数据转换为RDD[BaggedPoint[Datum]]\n  private def convertToBaggedRDDSamplingWithoutReplacement[Datum] (\n      input: RDD[Datum],\n      subsamplingRate: Double,\n      numSubsamples: Int,\n      seed: Long): RDD[BaggedPoint[Datum]] = {\n    input.mapPartitionsWithIndex { (partitionIndex, instances) =>\n      //使用随机数种子，分区索引，构建随机数生成器\n      val rng = new XORShiftRandom\n      rng.setSeed(seed + partitionIndex + 1)\n      //将每个实例变换成BaggedPoint结构表示\n      instances.map { instance =>\n        val subsampleWeights = new Array[Double](numSubsamples)\n        var subsampleIndex = 0\n        //对于每次采样，生成0-1之间的随机数，如果小于采样比，则对应权重为1，否则为0\n        while (subsampleIndex < numSubsamples) {\n          val x = rng.nextDouble()\n          subsampleWeights(subsampleIndex) = {\n            if (x < subsamplingRate) 1.0 else 0.0\n          }\n          subsampleIndex += 1\n        }\n        //转换为BaggedPoint结构数据\n        new BaggedPoint(instance, subsampleWeights)\n      }\n    }\n  }\n```\n#### RandomForest.selectNodesToSplit\n选择当前迭代待分裂的节点，以及确定每个节点使用的特征。每次选择都根据内存限制、每个节点占用的内存（如果每个节点使用的是采样后的特征），自适应地确定节点个数。\n\n```\nprivate[tree] def selectNodesToSplit(\n      nodeStack: mutable.Stack[(Int, LearningNode)], //存储节点的栈结构\n      maxMemoryUsage: Long, //最大占用内存限制\n      metadata: DecisionTreeMetadata, //元数据\n      //随机数\n      rng: Random): \n      //返回值包括：（1）每个树对应的待分裂节点数组， \n      //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）\n      (Map[Int, Array[LearningNode]], Map[Int, Map[Int, NodeIndexInfo]]) = {\n      //nodesForGroup(treeIndex) 存储第treeIndex个树对应的待分裂节点数组\n      val mutableNodesForGroup = new mutable.HashMap[Int, mutable.ArrayBuffer[LearningNode]]()\n      //每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）\n      val mutableTreeToNodeToIndexInfo =\n      new mutable.HashMap[Int, mutable.HashMap[Int, NodeIndexInfo]]()\n      var memUsage: Long = 0L  //当前使用内存\n      var numNodesInGroup = 0  //当前分组的节点数量\n      // If maxMemoryInMB is set very small, we want to still try to split 1 node,\n      // so we allow one iteration if memUsage == 0.\n      //如果栈不空，并且（1）如果内存上限设置非常小，我们要去报至少能有1个节点用于分裂\n      //（2）当前使用内存小于内存上限值，则进一步选择节点用于分裂\n      while (nodeStack.nonEmpty && (memUsage < maxMemoryUsage || memUsage == 0)) {\n      val (treeIndex, node) = nodeStack.top //选择栈顶节点\n      // Choose subset of features for node (if subsampling).\n     \n      val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) {       //如果特征需要采样，则对所有特征进行无放回采样\n        Some(SamplingUtils.reservoirSampleAndCount(Range(0,\n          metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong())._1)\n      } else {//如果特征不需要采样，则返回None\n        None\n      }\n      //通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的内存\n      val nodeMemUsage = RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L\n      ////检查增加当前节点后，内存容量是是否超过限制\n      if (memUsage + nodeMemUsage <= maxMemoryUsage || memUsage == 0) {\n        //如果加入该节点后内存没有超过限制\n        nodeStack.pop() //当前节点出栈\n        //更新mutableNodesForGroup，将当前节点加入对应treeIndex的节点数组\n        mutableNodesForGroup.getOrElseUpdate(treeIndex, new mutable.ArrayBuffer[LearningNode]()) +=\n          node\n        //更新mutableTreeToNodeToIndexInfo，将当前节点的具体信息，加入对应treeindex的节点map\n        mutableTreeToNodeToIndexInfo\n          .getOrElseUpdate(treeIndex, new mutable.HashMap[Int, NodeIndexInfo]())(node.id)\n          = new NodeIndexInfo(numNodesInGroup, featureSubset)\n      }\n      numNodesInGroup += 1 //当前分组的节点数量加一\n      memUsage += nodeMemUsage //当前使用内存数量加一\n    }\n    if (memUsage > maxMemoryUsage) {\n      // If maxMemoryUsage is 0, we should still allow splitting 1 node.\n      logWarning(s\"Tree learning is using approximately $memUsage bytes per iteration, which\" +\n        s\" exceeds requested limit maxMemoryUsage=$maxMemoryUsage. This allows splitting\" +\n        s\" $numNodesInGroup nodes in this iteration.\")\n    }\n    //转换可变map为不可变map类型\n    val nodesForGroup: Map[Int, Array[LearningNode]] =\n      mutableNodesForGroup.mapValues(_.toArray).toMap\n    val treeToNodeToIndexInfo = mutableTreeToNodeToIndexInfo.mapValues(_.toMap).toMap\n    //返回（1）每个树对应的待分裂节点数组， \n    //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）\n    (nodesForGroup, treeToNodeToIndexInfo)\n  }\n```\n\n```\n//无放回采样\ndef reservoirSampleAndCount[T: ClassTag](\n      input: Iterator[T], //input输入的迭代器\n      k: Int, //采样的样本数\n      seed: Long = Random.nextLong()) //随机数种子\n    : (Array[T], Long) = {\n    val reservoir = new Array[T](k) //存储采样结果的数组\n    // 放置迭代器的前k个元素到结果数组\n    var i = 0\n    while (i < k && input.hasNext) {\n      val item = input.next()\n      reservoir(i) = item\n      i += 1\n    }\n\n\n    //如果输入元素个数小于k, 则这k个特征作为返回的结果\n    if (i < k) {\n      // If input size < k, trim the array to return only an array of input size.\n      val trimReservoir = new Array[T](i)\n      System.arraycopy(reservoir, 0, trimReservoir, 0, i)\n      (trimReservoir, i) //返回结果数组，以及原始数组的元素个数\n    } else { \n      //如果输入元素个数大于k, 继续采样过程，将后面元素以一定概率随机替换前面的某个元素\n      var l = i.toLong\n      val rand = new XORShiftRandom(seed)\n      while (input.hasNext) {\n        val item = input.next()\n        l += 1\n        //当前结果数组有k个元素，l为当前元素的序号。k/l为当前元素替换结果数组中某个元素的概率。\n        //在进行替换时，对结果数组的每个元素以相等概率发生替换\n        //具体方式是产生一个0到l-1之间的随机整数replacementIndex，\n        //如果小于k则对第replacementIndex这个元素进行替换\n        val replacementIndex = (rand.nextDouble() * l).toLong\n        if (replacementIndex < k) {\n          reservoir(replacementIndex.toInt) = item\n        }\n      }\n      (reservoir, l) //返回结果数组，以及原始数组的元素个数\n    }\n  }\n```\n\n```\n  //通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的字节数\n  private def aggregateSizeForNode(\n      metadata: DecisionTreeMetadata,\n      featureSubset: Option[Array[Int]]): Long = {\n    //得到所有使用的特征的bin的数量之后\n    val totalBins = if (featureSubset.nonEmpty) {\n      //如果使用采样特征，得到采样后的所有特征bin数量之和\n      featureSubset.get.map(featureIndex => metadata.numBins(featureIndex).toLong).sum\n    } else {//否则使用所有的特征的bin数量之和\n      metadata.numBins.map(_.toLong).sum\n    }\n    if (metadata.isClassification) {\n      //如果是分类问题，则返回bin数量之和*类别个数\n      metadata.numClasses * totalBins \n    } else {\n      //否则返回bin数量之和*3\n      3 * totalBins\n    }\n  }\n\n```\n####  RandomForest.findBestSplits\n\n给定selectNodesToSplit方法选择的一组节点，找到每个节点对应的最佳分类特征的分裂位置。**求解的主要思想如下：**\n\n**基于节点的分组进行并行训练：**对一组的节点同时进行每个bin的统计和计算，减少不必要的数据传输成本。这样每次迭代需要更多的计算和存储成本，但是可以大大减少迭代的次数\n\n**基于bin的最佳分割点计算：**基于bin的计算来寻找最佳分割点，计算的思想不是依次对每个样本计算其对每个孩子节点的增益贡献，而是先将所有样本的每个特征映射到对应的bin，通过聚合每个bin的数据，进一步计算对应每个特征每个分割的增益。\n\n**对每个partition进行聚合：**由于提取知道了每个特征对应的split个数，因此可以用一个数组存储所有的bin的聚合信息，通过使用RDD的聚合方法，大大减少通讯开销。\n\n```\n private[tree] def findBestSplits(\n      input: RDD[BaggedPoint[TreePoint]], //训练数据\n      metadata: DecisionTreeMetadata, //随机森林元数据信息\n      topNodesForGroup: Map[Int, LearningNode], //存储当前节点分组对应的每个树的根节点\n      nodesForGroup: Map[Int, Array[LearningNode]],//存储当前节点分组对应的每个树的节点数组\n      treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]],//存储当前节点分组对应的每个树索引、节点索引、及详细信息\n      splits: Array[Array[Split]], //存储每个特征的所有split信息\n      //存储节点的栈结构，初始化时为各个树的根节点\n      nodeStack: mutable.Stack[(Int, LearningNode)],\n      timer: TimeTracker = new TimeTracker,       \n      nodeIdCache: Option[NodeIdCache] = None): Unit = {\n\n    //存储当前分组的节点数量\n    val numNodes = nodesForGroup.values.map(_.length).sum\n    logDebug(\"numNodes = \" + numNodes)\n    logDebug(\"numFeatures = \" + metadata.numFeatures)\n    logDebug(\"numClasses = \" + metadata.numClasses)\n    logDebug(\"isMulticlass = \" + metadata.isMulticlass)\n    logDebug(\"isMulticlassWithCategoricalFeatures = \" +\n      metadata.isMulticlassWithCategoricalFeatures)\n    logDebug(\"using nodeIdCache = \" + nodeIdCache.nonEmpty.toString)\n\n  \n    //对于一个特定的树的特定节点，通过baggedPoint数据点，更新DTStatsAggregator聚合信息（更新相关的特征及bin的聚合类信息）\n    def nodeBinSeqOp(\n        treeIndex: Int, //树的索引\n        nodeInfo: NodeIndexInfo, //节点信息\n        agg: Array[DTStatsAggregator], //聚合信息，(node, feature, bin)\n        baggedPoint: BaggedPoint[TreePoint]): Unit = {//数据点\n      if (nodeInfo != null) {//如果节点信息不为空，表示该节点在当前计算的节点集合中\n        val aggNodeIndex = nodeInfo.nodeIndexInGroup //该节点在当前分组的编号\n        val featuresForNode = nodeInfo.featureSubset //该节点对应的特征集合\n        //该样本在该树上的采样次数，如果为n表示5个同样的数据点同时用于更新对应的聚合信息\n        val instanceWeight = baggedPoint.subsampleWeights(treeIndex) \n        if (metadata.unorderedFeatures.isEmpty) {\n          //如果不存在无序特征，根据有序特征进行更新\n          orderedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, instanceWeight, featuresForNode)\n        } else { //都是有序特征\n          mixedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, splits,\n            metadata.unorderedFeatures, instanceWeight, featuresForNode)\n        }\n        agg(aggNodeIndex).updateParent(baggedPoint.datum.label, instanceWeight)\n      }\n    }\n\n    //计算当前数据被划分到的树的节点，并更新在对应节点的聚合信息。对于每个特征的相关bin,更新其聚合信息。\n    def binSeqOp(\n        agg: Array[DTStatsAggregator],//agg数组存储聚合信息，数据结构为（node, feature, bin）\n        baggedPoint: BaggedPoint[TreePoint]): Array[DTStatsAggregator] = {\n      treeToNodeToIndexInfo.foreach { case (treeIndex, nodeIndexToInfo) =>\n        //得到要更新的节点编号\n        val nodeIndex = \n          topNodesForGroup(treeIndex).predictImpl(baggedPoint.datum.binnedFeatures, splits)\n        //对上步得到的节点，根据样本点更新其对应的bin的聚合信息\n        nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)\n      }\n      agg\n    }\n\n    /**\n     * Do the same thing as binSeqOp, but with nodeIdCache.\n     */\n    def binSeqOpWithNodeIdCache(\n        agg: Array[DTStatsAggregator],\n        dataPoint: (BaggedPoint[TreePoint], Array[Int])): Array[DTStatsAggregator] = {\n      treeToNodeToIndexInfo.foreach { case (treeIndex, nodeIndexToInfo) =>\n        val baggedPoint = dataPoint._1\n        val nodeIdCache = dataPoint._2\n        val nodeIndex = nodeIdCache(treeIndex)\n        nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)\n      }\n\n      agg\n    }\n    \n    //从treeToNodeToIndexInfo中获取每个节点对应的特征集合。key为节点在本组节点的编号，value为对应特征集合\n    def getNodeToFeatures(\n        treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]]): Option[Map[Int, Array[Int]]] = {\n      if (!metadata.subsamplingFeatures) { //如果定义为不进行特征采样\n        None\n      } else {\n        //定义为特征采样，从treeToNodeToIndexInfo中获取对应的节点编号和特征集合。\n        val mutableNodeToFeatures = new mutable.HashMap[Int, Array[Int]]()\n        treeToNodeToIndexInfo.values.foreach { nodeIdToNodeInfo =>\n          nodeIdToNodeInfo.values.foreach { nodeIndexInfo =>\n            assert(nodeIndexInfo.featureSubset.isDefined)\n            mutableNodeToFeatures(nodeIndexInfo.nodeIndexInGroup) = nodeIndexInfo.featureSubset.get\n          }\n        }\n        Some(mutableNodeToFeatures.toMap)\n      }\n    }\n    \n    //用于训练的节点数组\n    val nodes = new Array[LearningNode](numNodes)\n    //根据nodesForGroup，在nodes中存储本轮迭代的节点，存储到nodes中\n    nodesForGroup.foreach { case (treeIndex, nodesForTree) =>\n      nodesForTree.foreach { node =>\n        nodes(treeToNodeToIndexInfo(treeIndex)(node.id).nodeIndexInGroup) = node\n      }\n    }\n\n    //对于所有的节点，计算最佳特征及分割点\n    timer.start(\"chooseSplits\")\n    //对于每个分区，迭代所有的样本，计算每个节点的聚合信息，\n    //产出(nodeIndex, nodeAggregateStats)数据结构，\n    //通过reduceByKey操作，一个节点的所有信息会被shuffle到同一个分区，通过合并信息，\n    //计算每个节点的最佳分割，最后只有最佳的分割用于进一步构建决策树。\n    val nodeToFeatures = getNodeToFeatures(treeToNodeToIndexInfo)//\n    val nodeToFeaturesBc = input.sparkContext.broadcast(nodeToFeatures)\n\n    val partitionAggregates: RDD[(Int, DTStatsAggregator)] = if (nodeIdCache.nonEmpty) {\n      input.zip(nodeIdCache.get.nodeIdsForInstances).mapPartitions { points =>\n        // Construct a nodeStatsAggregators array to hold node aggregate stats,\n        // each node will have a nodeStatsAggregator\n        val nodeStatsAggregators = Array.tabulate(numNodes) { nodeIndex =>\n          val featuresForNode = nodeToFeaturesBc.value.map { nodeToFeatures =>\n            nodeToFeatures(nodeIndex)\n          }\n          new DTStatsAggregator(metadata, featuresForNode)\n        }\n        // iterator all instances in current partition and update aggregate stats\n        points.foreach(binSeqOpWithNodeIdCache(nodeStatsAggregators, _))\n        // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,\n        // which can be combined with other partition using `reduceByKey`\n        nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator\n      }\n    } else {\n      input.mapPartitions { points =>\n        // 在每个分区内，构建一个nodeStatsAggregators数组，其中每个元素对应一个node的DTStatsAggregator，该DTStatsAggregator包括了决策树元数据信息、以及该node对应的特征集合\n        val nodeStatsAggregators = Array.tabulate(numNodes) { nodeIndex =>\n          val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>\n            Some(nodeToFeatures(nodeIndex))\n          }\n          new DTStatsAggregator(metadata, featuresForNode)\n        }\n        //对当前分区，迭代所有样本，更新nodeStatsAggregators，即每个node对应的DTStatsAggregator\n        points.foreach(binSeqOp(nodeStatsAggregators, _))\n        //转化成(nodeIndex, nodeAggregateStats)格式，用于后续通过reduceByKey对多个分区的结果进行聚合。\n        nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator\n      }\n    }\n    //reduceByKey聚合多个partition的统计特征\n    val nodeToBestSplits = partitionAggregates.reduceByKey((a, b) => a.merge(b)).map {\n      case (nodeIndex, aggStats) =>\n        //得到节点对应的特征集合\n        val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>\n          Some(nodeToFeatures(nodeIndex))\n        }\n\n        // 找到最佳分裂特征和分裂位置，并返回度量的统计特征\n        val (split: Split, stats: ImpurityStats) =\n          binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))\n        (nodeIndex, (split, stats))\n    }.collectAsMap()\n\n    timer.stop(\"chooseSplits\")\n\n    val nodeIdUpdaters = if (nodeIdCache.nonEmpty) {\n      Array.fill[mutable.Map[Int, NodeIndexUpdater]](\n        metadata.numTrees)(mutable.Map[Int, NodeIndexUpdater]())\n    } else {\n      null\n    }\n    // Iterate over all nodes in this group.\n    //对于本组所有节点，更新节点本身信息，如果孩子节点是课分裂的叶子节点，则将其加入栈中\n    nodesForGroup.foreach { case (treeIndex, nodesForTree) =>\n      nodesForTree.foreach { node =>\n        val nodeIndex = node.id //节点id\n        val nodeInfo = treeToNodeToIndexInfo(treeIndex)(nodeIndex) //节点信息，包括节点在当前分组编号，节点特征等\n        val aggNodeIndex = nodeInfo.nodeIndexInGroup //节点在当前分组编号\n        //节点对应的最佳分裂，及最佳分裂对应的不纯度度量相关统计信息\n        val (split: Split, stats: ImpurityStats) =\n          nodeToBestSplits(aggNodeIndex) \n        logDebug(\"best split = \" + split)\n\n        //如果信息增益小于0，或者层次达到上限，则将当前节点设置为叶子节点\n        val isLeaf =\n          (stats.gain <= 0) || (LearningNode.indexToLevel(nodeIndex) == metadata.maxDepth)\n        node.isLeaf = isLeaf\n        node.stats = stats\n        logDebug(\"Node = \" + node)\n        \n        //当前节点非叶子节点，创建子节点\n        if (!isLeaf) {\n          node.split = Some(split) //设置节点split参数\n          //子节点层数是否达到最大值\n          val childIsLeaf = (LearningNode.indexToLevel(nodeIndex) + 1) == metadata.maxDepth\n          //左孩子节点层数达到最大值，或者不纯度度量等于0，则左孩子节点为叶子节点\n          val leftChildIsLeaf = childIsLeaf || (stats.leftImpurity == 0.0)\n          //右孩子节点层数达到最大值，或者不纯度度量等于0，则右孩子节点为叶子节点          \n          val rightChildIsLeaf = childIsLeaf || (stats.rightImpurity == 0.0)\n          //创建左孩子节点，getEmptyImpurityStats(stats.leftImpurityCalculator)为左孩子的不纯度度量，只有impurity、impurityCalculator两个属性\n          node.leftChild = Some(LearningNode(LearningNode.leftChildIndex(nodeIndex),\n            leftChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.leftImpurityCalculator)))\n          //创建右孩子节点\n          node.rightChild = Some(LearningNode(LearningNode.rightChildIndex(nodeIndex),\n            rightChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.rightImpurityCalculator)))\n\n          if (nodeIdCache.nonEmpty) {\n            val nodeIndexUpdater = NodeIndexUpdater(\n              split = split,\n              nodeIndex = nodeIndex)\n            nodeIdUpdaters(treeIndex).put(nodeIndex, nodeIndexUpdater)\n          }\n\n          // enqueue left child and right child if they are not leaves\n          //如果左孩子节点不是叶子节点，则将左孩子节点入栈\n          if (!leftChildIsLeaf) {\n            nodeStack.push((treeIndex, node.leftChild.get))\n          }\n          if (!rightChildIsLeaf) {\n            //如果右孩子节点不是叶子节点，则将右孩子节点入栈\n            nodeStack.push((treeIndex, node.rightChild.get))\n          }\n          logDebug(\"leftChildIndex = \" + node.leftChild.get.id +\n            \", impurity = \" + stats.leftImpurity)\n          logDebug(\"rightChildIndex = \" + node.rightChild.get.id +\n            \", impurity = \" + stats.rightImpurity)\n        }\n      }\n    }\n    if (nodeIdCache.nonEmpty) {\n      // Update the cache if needed.\n      nodeIdCache.get.updateNodeIndices(input, nodeIdUpdaters, splits)\n    }\n  }  \n```\n\n```\n  //得到当前数据点对应的node index输出,模仿对数据的预测过程，从根节点开始向下传播，\n  //直到一个叶子节点或者未进行分裂的节点终止，返回终止节点对应的索引。\n  def predictImpl(binnedFeatures: Array[Int], splits: Array[Array[Split]]): Int = {\n    if (this.isLeaf || this.split.isEmpty) {\n      this.id //如果当前节点是叶子节点或者未分裂的节点，返回当前节点索引\n    } else {\n      val split = this.split.get //当前节点的split\n      val featureIndex = split.featureIndex //当前节点split对应的特征索引\n      //根据数据点在featureIndex特征上的取值，以及featureIndex特征对应的分裂，判断当前数据点是否应该向左传递。\n      val splitLeft = split.shouldGoLeft(binnedFeatures(featureIndex), splits(featureIndex)) \n      if (this.leftChild.isEmpty) { //如果左孩子为空\n        // Not yet split. Return next layer of nodes to train\n        if (splitLeft) { //当前节点应该向左传递，得到左孩子节点索引值\n          LearningNode.leftChildIndex(this.id)\n        } else { //当前节点应该向右传递，得到右孩子节点索引值\n          LearningNode.rightChildIndex(this.id)\n        }\n      } else { //如果左孩子不为空，\n        if (splitLeft) { //当前节点应该向左传递，从左节点开始，递归计算最终节点的索引\n          this.leftChild.get.predictImpl(binnedFeatures, splits)\n        } else { //当前节点应该向右传递，从右节点开始，递归计算最终节点的索引\n          this.rightChild.get.predictImpl(binnedFeatures, splits)\n        }\n      }\n    }\n  }\n```\n\n```\n//对于排序类特征，根据数据点、权重，更新每个特征的每个bin信息        \nprivate def orderedBinSeqOp(\n      agg: DTStatsAggregator, //聚合信息，(feature, bin)\n      treePoint: TreePoint,\n      instanceWeight: Double,\n      featuresForNode: Option[Array[Int]]): Unit = {\n    val label = treePoint.label\n\n    // 如果是采样特征\n    if (featuresForNode.nonEmpty) {\n      // 使用采样的特征，对于每个特征的每个bin，进行更新\n      var featureIndexIdx = 0\n      while (featureIndexIdx < featuresForNode.get.length) {\n        val binIndex = treePoint.binnedFeatures(featuresForNode.get.apply(featureIndexIdx))\n        agg.update(featureIndexIdx, binIndex, label, instanceWeight)\n        featureIndexIdx += 1\n      }\n    } else {\n      // 如果是非采样特征，使用所有特征，对每个特征的每个bin，进行更新\n      val numFeatures = agg.metadata.numFeatures\n      var featureIndex = 0\n      while (featureIndex < numFeatures) {\n        val binIndex = treePoint.binnedFeatures(featureIndex)\n        agg.update(featureIndex, binIndex, label, instanceWeight)\n        featureIndex += 1\n      }\n    }\n  }\n```\n\n```\n//相对于orderedBinSeqOp函数，mixedBinSeqOp函数在同时包括排序和非排序特征情况下，更新聚合信息.\n//对于有序特征，对每个特征更新一个bin\n//对于无序特征，类别的子集对应的bin需要消息，每个子集的靠左bin或者靠右bin需要更新\nprivate def mixedBinSeqOp(\n      agg: DTStatsAggregator, //聚合信息，(feature, bin)\n      treePoint: TreePoint,\n      splits: Array[Array[Split]],\n      unorderedFeatures: Set[Int],\n      instanceWeight: Double,\n      featuresForNode: Option[Array[Int]]): Unit = {\n    val numFeaturesPerNode = if (featuresForNode.nonEmpty) {\n      // 如果特征需要采样，使用采样特征\n      featuresForNode.get.length\n    } else {\n      // 否则使用所有特征\n      agg.metadata.numFeatures\n    }\n    // 迭代每个特征，更新该节点对应的bin聚合信息.\n    var featureIndexIdx = 0\n    while (featureIndexIdx < numFeaturesPerNode) {\n      //得到特征对应的原始索引值\n      val featureIndex = if (featuresForNode.nonEmpty) {\n        featuresForNode.get.apply(featureIndexIdx)\n      } else {\n        featureIndexIdx\n      }\n      if (unorderedFeatures.contains(featureIndex)) {\n        //如果当前特征是无序特征\n        val featureValue = treePoint.binnedFeatures(featureIndex) //得到bin features\n        //得到当前特征偏移量\n        val leftNodeFeatureOffset = agg.getFeatureOffset(featureIndexIdx)\n        // Update the left or right bin for each split.\n        //得到当前特征的split数量\n        val numSplits = agg.metadata.numSplits(featureIndex)\n        //得到当前特征分裂信息\n        val featureSplits = splits(featureIndex)\n        var splitIndex = 0\n        while (splitIndex < numSplits) {\n          //根据当前特征值，判断是否应该向左传递，如果向左传递，则将节点对当前特征的当前区间聚合信息进行更新\n          if (featureSplits(splitIndex).shouldGoLeft(featureValue, featureSplits)) {\n            agg.featureUpdate(leftNodeFeatureOffset, splitIndex, treePoint.label, instanceWeight)\n          }\n          splitIndex += 1\n        }\n      } else {\n        // 如果是有序特征，则直接更新对应特征的对应bin信息\n        val binIndex = treePoint.binnedFeatures(featureIndex)\n        agg.update(featureIndexIdx, binIndex, treePoint.label, instanceWeight)\n      }\n      featureIndexIdx += 1\n    }\n  }\n```\n\n```\n//寻找最佳分裂特征和分裂位置\nprivate[tree] def binsToBestSplit(\n      binAggregates: DTStatsAggregator, //所有feature的bin的统计信息\n      splits: Array[Array[Split]],//所有feature的所有split\n      featuresForNode: Option[Array[Int]],//node对应的feature子集\n      //当前node\n      node: LearningNode): (Split, ImpurityStats) = { //返回值为最佳分裂，及对应的不纯度相关度量\n\n    // Calculate InformationGain and ImpurityStats if current node is top node\n    // 当前节点对应的树的层次\n    val level = LearningNode.indexToLevel(node.id)\n    // 如果是根节点，不纯度度量为0\n    var gainAndImpurityStats: ImpurityStats = if (level == 0) {\n      null\n    } else {\n      //否则为当前节点对应的相关度量stats\n      node.stats\n    }\n    //获得合法的特征分裂\n    val validFeatureSplits =\n      Range(0, binAggregates.metadata.numFeaturesPerNode).view.map { \n      //得到原始特征对应的feature index\n      featureIndexIdx =>\n        featuresForNode.map(features => (featureIndexIdx, features(featureIndexIdx)))\n          .getOrElse((featureIndexIdx, featureIndexIdx))\n      }.withFilter { case (_, featureIndex) => //过滤对应split数量为0的特征\n        binAggregates.metadata.numSplits(featureIndex) != 0\n      }\n\n    //对每个(feature,split), 计算增益，并选择增益最大的(feature,split)\n    val (bestSplit, bestSplitStats) =\n      validFeatureSplits.map { case (featureIndexIdx, featureIndex) =>\n        //得到索引为featureIndex的特征对应的split数量\n        val numSplits = binAggregates.metadata.numSplits(featureIndex)\n        if (binAggregates.metadata.isContinuous(featureIndex)) {\n          //如果是连续特征\n          //计算每个bin的累积统计信息（包括第一个bin到当前bin之间的所有bin对应的统计信息）\n          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)\n          var splitIndex = 0\n          while (splitIndex < numSplits) {\n            binAggregates.mergeForFeature(nodeFeatureOffset, splitIndex + 1, splitIndex)\n            splitIndex += 1\n          }\n          //找到最好的split\n          val (bestFeatureSplitIndex, bestFeatureGainStats) =\n            Range(0, numSplits).map { case splitIdx =>\n              //得到当前split左孩子对应的统计信息\n              val leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, splitIdx)\n              //得到当前split右孩子对应的统计信息， 为得到右孩子对应的统计信息，需要所有的统计信息减去左孩子的统计信息\n              val rightChildStats =\n                binAggregates.getImpurityCalculator(nodeFeatureOffset, numSplits)\n              //所有的统计信息减去左孩子的统计信息\n              rightChildStats.subtract(leftChildStats)\n              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,\n                leftChildStats, rightChildStats, binAggregates.metadata)\n              (splitIdx, gainAndImpurityStats)//分裂索引，不纯度度量信息\n            }.maxBy(_._2.gain)//取信息增益最大的分裂\n          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)\n        } else if (binAggregates.metadata.isUnordered(featureIndex)) {\n          //无序离散特征\n          val leftChildOffset = binAggregates.getFeatureOffset(featureIndexIdx)\n          val (bestFeatureSplitIndex, bestFeatureGainStats) =\n            Range(0, numSplits).map { splitIndex =>\n              //得到左孩子聚合信息\n              val leftChildStats = binAggregates.getImpurityCalculator(leftChildOffset, splitIndex)\n              //得到右孩子聚合信息\n              val rightChildStats = binAggregates.getParentImpurityCalculator()\n                .subtract(leftChildStats)\n              //计算不纯度度量相关统计信息\n              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,\n                leftChildStats, rightChildStats, binAggregates.metadata)\n              (splitIndex, gainAndImpurityStats) //分裂索引，不纯度度量信息\n            }.maxBy(_._2.gain)//取信息增益最大的分裂\n          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)\n        } else {\n          // 对于排序离散特征\n          //得到聚合信息的其实地址\n          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)\n          //得到类别数量\n          val numCategories = binAggregates.metadata.numBins(featureIndex)\n\n          //每个bin是一个特征值，根据质心对这些特征值排序，共K个特征值，对应生成K-1个划分\n          val centroidForCategories = Range(0, numCategories).map { case featureValue =>\n            //得到不纯度度量的统计信息\n            val categoryStats =\n              binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)\n            val centroid = if (categoryStats.count != 0) {//如果对应样本数量不为0，\n              if (binAggregates.metadata.isMulticlass) {\n                //如果是多分类决策树，则将对应多标签的不纯度度量作为质心\n                categoryStats.calculate()\n              } else if (binAggregates.metadata.isClassification) {\n                //如果是二分类问题，则将对应的正样本数量作为质心\n                categoryStats.stats(1)\n              } else {\n                //如果是回归问题，则将对应的预测值作为质心\n                categoryStats.predict\n              }\n            } else {\n              Double.MaxValue //如果对应样本数量为0，则质心为Double.MaxValue\n            }\n            (featureValue, centroid) //返回每个特征值对应的样本质心\n          }\n\n          logDebug(\"Centroids for categorical variable: \" + centroidForCategories.mkString(\",\"))\n\n          // 根据质心，将特征对应的bin排序（即对应的离散特征值排序）\n          val categoriesSortedByCentroid = centroidForCategories.toList.sortBy(_._2)\n\n          logDebug(\"Sorted centroids for categorical variable = \" +\n            categoriesSortedByCentroid.mkString(\",\"))\n\n          // 从左到右，依次计算每个category对应的从第一个category到当前categofy的统计信息聚合结果\n          var splitIndex = 0\n          while (splitIndex < numSplits) {\n            val currentCategory = categoriesSortedByCentroid(splitIndex)._1\n            val nextCategory = categoriesSortedByCentroid(splitIndex + 1)._1\n            binAggregates.mergeForFeature(nodeFeatureOffset, nextCategory, currentCategory)\n            splitIndex += 1\n          }\n          \n          //所有特征值的聚合结果对应的category索引\n          val lastCategory = categoriesSortedByCentroid.last._1\n          //找到最佳的分裂\n          val (bestFeatureSplitIndex, bestFeatureGainStats) =\n            Range(0, numSplits).map { splitIndex =>\n              //得到当前索引的特征值\n              val featureValue = categoriesSortedByCentroid(splitIndex)._1\n              //得到左孩子对应的聚合信息\n              val leftChildStats =\n                binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)\n              //得到右孩子对应的聚合信息\n              val rightChildStats =\n                binAggregates.getImpurityCalculator(nodeFeatureOffset, lastCategory)\n              rightChildStats.subtract(leftChildStats)\n              //得到不纯度度量的相关统计信息\n              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,\n                leftChildStats, rightChildStats, binAggregates.metadata)\n              (splitIndex, gainAndImpurityStats)\n            }.maxBy(_._2.gain)//根据信息增益进行排序，得到信息增益最大的split索引及增益\n          \n          //得到最佳分裂边界\n          val categoriesForSplit =\n            categoriesSortedByCentroid.map(_._1.toDouble).slice(0, bestFeatureSplitIndex + 1)\n          //得到最佳分裂，包括特征索引、划分边界、类别数量等\n          val bestFeatureSplit =\n            new CategoricalSplit(featureIndex, categoriesForSplit.toArray, numCategories)\n           //返回最佳分裂，及对应的增益统计信息\n          (bestFeatureSplit, bestFeatureGainStats)\n        }\n      }.maxBy(_._2.gain)//针对所有特征，按照信息增益进行排序，取增益最大的特征\n\n    (bestSplit, bestSplitStats)//返回最佳分裂，及对应的增益统计信息\n  }\n```\n\n```\n根据分裂对应的左孩子聚合信息，右孩子聚合信息，计算当前节点不纯度度量的相关统计信息\nprivate def calculateImpurityStats(\n      stats: ImpurityStats,\n      leftImpurityCalculator: ImpurityCalculator,\n      rightImpurityCalculator: ImpurityCalculator,\n      metadata: DecisionTreeMetadata): ImpurityStats = {\n    //得到父节点的聚合信息\n    val parentImpurityCalculator: ImpurityCalculator = if (stats == null) {\n      leftImpurityCalculator.copy.add(rightImpurityCalculator)\n    } else {\n      stats.impurityCalculator\n    }\n    //得到父节点不纯度度量\n    val impurity: Double = if (stats == null) {\n      parentImpurityCalculator.calculate()\n    } else {\n      stats.impurity\n    }\n   \n    val leftCount = leftImpurityCalculator.count //根据当前分裂得到的左孩子对应样本数量\n    val rightCount = rightImpurityCalculator.count //根据当前分裂得到的右孩子对应样本数量\n\n    val totalCount = leftCount + rightCount  //当前分裂对应的总样本数量\n\n    // If left child or right child doesn't satisfy minimum instances per node,\n    // then this split is invalid, return invalid information gain stats.\n    //如果左孩子或者右孩子样本数量小于下限值，返回不合法的不纯度度量信息\n    if ((leftCount < metadata.minInstancesPerNode) ||\n      (rightCount < metadata.minInstancesPerNode)) {\n      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)\n    }\n    //左孩子对应的不纯度度量\n    val leftImpurity = leftImpurityCalculator.calculate() // Note: This equals 0 if count = 0\n    //右孩子对应的不纯度度量\n    val rightImpurity = rightImpurityCalculator.calculate()\n    //左孩子权重\n    val leftWeight = leftCount / totalCount.toDouble\n    //右孩子权重\n    val rightWeight = rightCount / totalCount.toDouble\n    //信息增益\n    val gain = impurity - leftWeight * leftImpurity - rightWeight * rightImpurity\n    //信息增益小于下限值，则返回不合法的不纯度度量信息\n      if (gain < metadata.minInfoGain) {\n      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)\n    }\n    //返回不纯度度量信息\n    new ImpurityStats(gain, impurity, parentImpurityCalculator,\n      leftImpurityCalculator, rightImpurityCalculator)\n  }\n```\n\n## 模型预测\n\n通过模型训练生成决策树（随机森林）模型RandomForestModel，随机森林模型继承了树的组合模型TreeEnsembleModel，进一步通过predictBySumming函数，对传进的样本点进行预测。\n\n\n```\n  //对样本点features进行预测\n  private def predictBySumming(features: Vector): Double = {\n    //对每棵决策树进行预测，然后自后结果为每个决策树结果的加权求和\n    val treePredictions = trees.map(_.predict(features))\n    blas.ddot(numTrees, treePredictions, 1, treeWeights, 1)\n  }\n  \n```\n\n```\n  //DecisionTreeModel.predict方法\n  def predict(features: Vector): Double = {\n    //根据头部节点预测lable\n    topNode.predict(features)\n  }\n```\n\n```\n  //Node. predict方法\n  def predict(features: Vector): Double = {\n    if (isLeaf) {\n      predict.predict //如果是叶子节点，直接输出\n    } else {\n      if (split.get.featureType == Continuous) { \n        //如果是连续特征，根据分裂阈值，决定走左孩子节点还是右孩子节点\n        if (features(split.get.feature) <= split.get.threshold) {\n          leftNode.get.predict(features)\n        } else {\n          rightNode.get.predict(features)\n        }\n      } else {\n        //如果是离散特征，根据特征是否被当前节点对应的特征集合包含，决定走左孩子节点还是右孩子节点\n        if (split.get.categories.contains(features(split.get.feature))) {\n          leftNode.get.predict(features)\n        } else {\n          rightNode.get.predict(features)\n        }\n      }\n    }\n  }\n\n```\n\n\n# 参考资料\n\n【1】http://spark.apache.org/mllib/ \n【2】http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html\n","slug":"decision_tree","published":1,"updated":"2018-02-11T08:33:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjdikguhr001sga01rt9gx0j9","content":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n<p>该文章来自于2016年后半年整理的算法源码笔记，由于之前没有写博客的习惯，都直接以笔记的形式存在电脑上，分享起来非常不便，因此抽出时间，将其整理成博客的形式，和大家一起学习交流。</p>\n<h1 id=\"决策树算法简要介绍\"><a href=\"#决策树算法简要介绍\" class=\"headerlink\" title=\"决策树算法简要介绍\"></a>决策树算法简要介绍</h1><p>决策树算法是一种常见的分类算法，也可以用于回归问题。相对于其他分类算法，决策树的优点在于简单,可解释性强；对特征尺度不敏感，不需要做太多的特征预处理工作;能够自动挖掘特征之间的关联关系。缺点是比较容易过拟合（通过随机森林可以避免过拟合）</p>\n<p>决策树是一个树形结构，其中叶子节点表示分类（或回归）结果，非叶子节点是属性判断判断节点，每个属性判断节点都选择样本的一个特征，并根据该特征的取值决定选择哪一个分支路径。在对样本进行预测时，从根节点开始直到叶子节点，对于路径上的每个分支节点，都根据其对应的属性取值选择下一个分支节点，直到叶子节点。整个完整的路径，表示对样本的预测过程。如图1所示，表示一个女孩在决定是否决定去相亲的一个过程，最终选择去或者不去，对应分类的结果，中间的各种条件对应相关的属性。</p>\n<center><br><img src=\"/decision_tree/decision_tree_example.png\" alt=\"“决策树样例”\"><br></center><br><center>图1：决策树样例：对女孩决定是否参加相亲的问题进行决策树建模</center>\n\n\n<h2 id=\"决策树的训练\"><a href=\"#决策树的训练\" class=\"headerlink\" title=\"决策树的训练\"></a>决策树的训练</h2><p>从根节点开始，根据信息增益或其他条件，不断选择分裂的属性，直到生成叶子节点的过程。具体过程如下所示：</p>\n<ul>\n<li>对不同的属性，计算其信息增益，选择增益最大的特征对应根节点的最佳分裂。</li>\n<li>从根节点开始，对于不同的分支节点，分别选择信息增益最大的特征作为分支节点的最佳分裂。</li>\n<li>如果达到停止分裂的条件，则将该节点作为叶子节点：当前节点对应的样本都是一类样本，分类结果为对应的样本的类别；总样本数量小于一定值，或者树的高度达到最大值，或者信息增益小于一定值，或者已经用完所有的属性，选择占比最大的样本分类作为节点对应的分类结果。否则，根据步骤2进一步构造分裂节点。</li>\n</ul>\n<h2 id=\"属性度量\"><a href=\"#属性度量\" class=\"headerlink\" title=\"属性度量\"></a>属性度量</h2><p>决策树构建的关键，在于不断地选择最佳分裂属性。属性的收益度量方法，常见的有信息增益（ID3算法）、信息增益率（C4.5算法），基尼系数(CART算法)等。</p>\n<p><strong>ID3算法:</strong></p>\n<p>熵：信息论中，用于描述信息的不确定性，定义如式1，其中$D$表示对样本的一个划分，$m$表示划分的类别数量，$p_i$表示第i个类别的样本数量比例。</p>\n<p>$info(D)=-\\sum_{i=1}^m p_ilog_2(p_i)\\;\\;\\;（式1）$</p>\n<p>假设按照属性A对样本D进行划分，$v$为属性$A$的划分数量。则$A$对$D$划分的期望熵如式2：</p>\n<p>$info_A(D)=\\sum_{j=1}^v\\frac{|D_j|}{|D|}info(D_j)\\;\\;\\;（式2）$</p>\n<p>信心增益为上述原始熵和属性A对D划分后期望熵的差值，可以看做是加入信息A后，不确定性的减少程度。信息增益的定义如式3所示：</p>\n<p>$gain(A)=info(D)-info_A(D)\\;\\;\\;（式3）$</p>\n<p>ID3算法即在每次选择最佳分裂的属性时，根据信息增益进行选择。</p>\n<p><strong>C4.5算法:</strong><br>ID3算法容易使得选取值较多的属性。一种极端的情况是，对于ID类特征有很多的无意义的值的划分，ID3会选择该属性其作为最佳划分。C4.5算法通过采用信息增益率作为衡量特征有效性的指标，可以克服这个问题。</p>\n<p>首先定义分裂信息：<br>$splitInfo_A(D)=-\\sum_{j=1}^v\\frac{|D_j|}{|D|}log_2(\\frac{|D_j|}{|D|})\\;\\;\\;（式4）$</p>\n<p>信息增益率：<br>$gainRatio(A)=\\frac{gain(A)}{splitInfo_A(D)}\\;\\;\\;（式5）$</p>\n<p><strong>CART算法:</strong></p>\n<p>使用基尼系数作为不纯度的度量。<br>基尼系数:表示在样本集合中一个随机选中的样本被分错的概率，Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。当所有样本属于一个类别时，基尼系数最小为0。所有类别以等概率出现时，基尼系数最大。<br>$GINI(P)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^K p_k^2\\;\\;\\;（式6）$</p>\n<p>由于cart建立的树是个二叉树，所以K的取值为2。对于特征取值超过2的情况，以每个取值作为划分点，计算该划分下对应的基尼系数的期望。期望值最小的划分点，作为最佳分裂使用的特征划分。</p>\n<h1 id=\"spark-决策树源码分析\"><a href=\"#spark-决策树源码分析\" class=\"headerlink\" title=\"spark 决策树源码分析\"></a>spark 决策树源码分析</h1><p>为加深对ALS算法的理解，该部分主要分析spark mllib中决策树源码的实现。主要包括模型训练、模型预测2个部分</p>\n<h2 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h2><h3 id=\"决策树伴生类\"><a href=\"#决策树伴生类\" class=\"headerlink\" title=\"决策树伴生类\"></a>决策树伴生类</h3><p>DecisionTree伴随类，外部调用决策树模型进行训练的入口。通过外部传入数据和配置参数，调用DecisionTree中的run方法进行模型训练， 最终返回DecisionTreeModel类型对象。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">DecisionTree</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> <span class=\"keyword\">with</span> <span class=\"title\">Logging</span> </span>&#123;</span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span></span>(</span><br><span class=\"line\">      input: <span class=\"type\">RDD</span>[<span class=\"type\">LabeledPoint</span>], <span class=\"comment\">//训练数据，包括label和特征向量</span></span><br><span class=\"line\">      algo: <span class=\"type\">Algo</span>,<span class=\"comment\">//决策树类型，分类树or回归树</span></span><br><span class=\"line\">      impurity: <span class=\"type\">Impurity</span>,<span class=\"comment\">//衡量特征信息增益的标准，如信息增益、基尼、方差</span></span><br><span class=\"line\">      maxDepth: <span class=\"type\">Int</span>,<span class=\"comment\">//树的深度</span></span><br><span class=\"line\">      numClasses: <span class=\"type\">Int</span>,<span class=\"comment\">//待分类类别的数量</span></span><br><span class=\"line\">      maxBins: <span class=\"type\">Int</span>,<span class=\"comment\">//用于特征分裂的bin的最大数量</span></span><br><span class=\"line\">      quantileCalculationStrategy: <span class=\"type\">QuantileStrategy</span>,<span class=\"comment\">//计算分位数的算法</span></span><br><span class=\"line\">      <span class=\"comment\">//离散特征存储，如n-&gt;k表示第n个特征有k个取值（0，1，..., k-1）</span></span><br><span class=\"line\">      categoricalFeaturesInfo: <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Int</span>]): <span class=\"type\">DecisionTreeModel</span> = &#123; </span><br><span class=\"line\">    <span class=\"comment\">//根据参数信息，生成决策树配置</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> strategy = <span class=\"keyword\">new</span> <span class=\"type\">Strategy</span>(algo, impurity, maxDepth, numClasses, maxBins,</span><br><span class=\"line\">      quantileCalculationStrategy, categoricalFeaturesInfo)</span><br><span class=\"line\">    <span class=\"comment\">//调用DecisionTree对象的run方法，训练决策树模型</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">DecisionTree</span>(strategy).run(input)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   <span class=\"comment\">//训练分类决策树</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trainClassifier</span></span>(</span><br><span class=\"line\">      input: <span class=\"type\">RDD</span>[<span class=\"type\">LabeledPoint</span>],</span><br><span class=\"line\">      numClasses: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      categoricalFeaturesInfo: <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Int</span>],</span><br><span class=\"line\">      impurity: <span class=\"type\">String</span>,</span><br><span class=\"line\">      maxDepth: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      maxBins: <span class=\"type\">Int</span>): <span class=\"type\">DecisionTreeModel</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> impurityType = <span class=\"type\">Impurities</span>.fromString(impurity)</span><br><span class=\"line\">    train(input, <span class=\"type\">Classification</span>, impurityType, maxDepth, numClasses, maxBins, <span class=\"type\">Sort</span>,categoricalFeaturesInfo)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">    <span class=\"comment\">//训练回归决策树</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trainRegressor</span></span>(</span><br><span class=\"line\">      input: <span class=\"type\">RDD</span>[<span class=\"type\">LabeledPoint</span>],</span><br><span class=\"line\">      categoricalFeaturesInfo: <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Int</span>],</span><br><span class=\"line\">      impurity: <span class=\"type\">String</span>,</span><br><span class=\"line\">      maxDepth: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      maxBins: <span class=\"type\">Int</span>): <span class=\"type\">DecisionTreeModel</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> impurityType = <span class=\"type\">Impurities</span>.fromString(impurity) <span class=\"comment\">//基尼、熵、方差三种衡量标准</span></span><br><span class=\"line\">    train(input, <span class=\"type\">Regression</span>, impurityType, maxDepth, <span class=\"number\">0</span>, maxBins, <span class=\"type\">Sort</span>, categoricalFeaturesInfo)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"决策树类\"><a href=\"#决策树类\" class=\"headerlink\" title=\"决策树类\"></a>决策树类</h3><p>接受strategy参数初始化，并通过对run方法调用随机森林的run方法，通过设置特征集合为全集、树的个数为1，将随机森林训练后结果集中的第一棵树作为结果返回。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class DecisionTree private[spark] (private val strategy: Strategy, private val seed: Int)</span><br><span class=\"line\">  extends Serializable with Logging &#123;</span><br><span class=\"line\">  def run(input: RDD[LabeledPoint]): DecisionTreeModel = &#123;</span><br><span class=\"line\">    val rf = new RandomForest(strategy, numTrees = 1, featureSubsetStrategy = &quot;all&quot;, seed = seed)</span><br><span class=\"line\">    val rfModel = rf.run(input)</span><br><span class=\"line\">    rfModel.trees(0)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"RandomForest私有类run方法-通过run方法完成模型的训练\"><a href=\"#RandomForest私有类run方法-通过run方法完成模型的训练\" class=\"headerlink\" title=\"RandomForest私有类run方法,通过run方法完成模型的训练\"></a>RandomForest私有类run方法,通过run方法完成模型的训练</h3><p><strong>分布式训练思想：</strong></p>\n<ul>\n<li>分布式存储样本</li>\n<li>对于每次迭代，算法都会对一个node集合进行分裂。对于每个node，相关worker计算的的所有相关统计特征全部传递到某个worker进行汇总，并选择最好的特征分裂</li>\n<li>findSplitsBins方法可用于将连续特征离散化，在初始化阶段完成</li>\n<li>迭代算法<br>每次都作用于树的边缘节点，如果是随机森林，则选择所有的树的边缘节点。具体迭代步骤如下：<ol>\n<li>Master 节点: 从node queue中选取节点，如果训练的是随机森林,且featureSubsetStrategy取值不是all，则对于每个节点选择随机特征子集。selectNodesToSplit用于选择待分裂的节点。</li>\n<li>Worer节点: findBestSplits函数，对每个(tree, node, feature, split)，遍历所有本地所有样本计算相关特征，计算结果通过reduceByKey传递给某个节点，由该节点汇总数据，得到(feature, split)或者判断是否停止分裂</li>\n<li>Master节点: 收集所有节点分裂信息，更新model, 并将新的model传递给各个worker节点 </li>\n</ol>\n</li>\n</ul>\n<p>####<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def run(</span><br><span class=\"line\">      input: RDD[LabeledPoint],</span><br><span class=\"line\">      strategy: OldStrategy,</span><br><span class=\"line\">      numTrees: Int,</span><br><span class=\"line\">      featureSubsetStrategy: String,</span><br><span class=\"line\">      seed: Long,</span><br><span class=\"line\">      instr: Option[Instrumentation[_]],</span><br><span class=\"line\">      parentUID: Option[String] = None): Array[DecisionTreeModel] = &#123;</span><br><span class=\"line\">    val timer = new TimeTracker()</span><br><span class=\"line\">    timer.start(&quot;total&quot;)</span><br><span class=\"line\">    timer.start(&quot;init&quot;)</span><br><span class=\"line\">    </span><br><span class=\"line\">    val retaggedInput = input.retag(classOf[LabeledPoint])</span><br><span class=\"line\">    //构建元数据</span><br><span class=\"line\">    val metadata =</span><br><span class=\"line\">      DecisionTreeMetadata.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)</span><br><span class=\"line\">    instr match &#123;</span><br><span class=\"line\">      case Some(instrumentation) =&gt;</span><br><span class=\"line\">        instrumentation.logNumFeatures(metadata.numFeatures)</span><br><span class=\"line\">        instrumentation.logNumClasses(metadata.numClasses)</span><br><span class=\"line\">      case None =&gt;</span><br><span class=\"line\">        logInfo(&quot;numFeatures: &quot; + metadata.numFeatures)</span><br><span class=\"line\">        logInfo(&quot;numClasses: &quot; + metadata.numClasses)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    //每个特征对应的splits和bins</span><br><span class=\"line\">    timer.start(&quot;findSplits&quot;)</span><br><span class=\"line\">    val splits = findSplits(retaggedInput, metadata, seed)</span><br><span class=\"line\">    timer.stop(&quot;findSplits&quot;)</span><br><span class=\"line\">    logDebug(&quot;numBins: feature: number of bins&quot;)</span><br><span class=\"line\">    logDebug(Range(0, metadata.numFeatures).map &#123; featureIndex =&gt;</span><br><span class=\"line\">      s&quot;\\t$featureIndex\\t$&#123;metadata.numBins(featureIndex)&#125;&quot;</span><br><span class=\"line\">    &#125;.mkString(&quot;\\n&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">    // Bin feature values (TreePoint representation).</span><br><span class=\"line\">    // Cache input RDD for speedup during multiple passes.</span><br><span class=\"line\">    //输入</span><br><span class=\"line\">    val treeInput = TreePoint.convertToTreeRDD(retaggedInput, splits, metadata)</span><br><span class=\"line\"></span><br><span class=\"line\">    val withReplacement = numTrees &gt; 1</span><br><span class=\"line\"></span><br><span class=\"line\">    val baggedInput = BaggedPoint</span><br><span class=\"line\">      .convertToBaggedRDD(treeInput, strategy.subsamplingRate, numTrees, withReplacement, seed)</span><br><span class=\"line\">      .persist(StorageLevel.MEMORY_AND_DISK)</span><br><span class=\"line\"></span><br><span class=\"line\">    // depth of the decision tree</span><br><span class=\"line\">    val maxDepth = strategy.maxDepth</span><br><span class=\"line\">    require(maxDepth &lt;= 30,</span><br><span class=\"line\">      s&quot;DecisionTree currently only supports maxDepth &lt;= 30, but was given maxDepth = $maxDepth.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // Max memory usage for aggregates</span><br><span class=\"line\">    // TODO: Calculate memory usage more precisely.</span><br><span class=\"line\">    val maxMemoryUsage: Long = strategy.maxMemoryInMB * 1024L * 1024L</span><br><span class=\"line\">    logDebug(&quot;max memory usage for aggregates = &quot; + maxMemoryUsage + &quot; bytes.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    /*</span><br><span class=\"line\">     * The main idea here is to perform group-wise training of the decision tree nodes thus</span><br><span class=\"line\">     * reducing the passes over the data from (# nodes) to (# nodes / maxNumberOfNodesPerGroup).</span><br><span class=\"line\">     * Each data sample is handled by a particular node (or it reaches a leaf and is not used</span><br><span class=\"line\">     * in lower levels).</span><br><span class=\"line\">     */</span><br><span class=\"line\"></span><br><span class=\"line\">    // Create an RDD of node Id cache.</span><br><span class=\"line\">    // At first, all the rows belong to the root nodes (node Id == 1).</span><br><span class=\"line\">    val nodeIdCache = if (strategy.useNodeIdCache) &#123;</span><br><span class=\"line\">      Some(NodeIdCache.init(</span><br><span class=\"line\">        data = baggedInput,</span><br><span class=\"line\">        numTrees = numTrees,</span><br><span class=\"line\">        checkpointInterval = strategy.checkpointInterval,</span><br><span class=\"line\">        initVal = 1))</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      None</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    /*</span><br><span class=\"line\">      Stack of nodes to train: (treeIndex, node)</span><br><span class=\"line\">      The reason this is a stack is that we train many trees at once, but we want to focus on</span><br><span class=\"line\">      completing trees, rather than training all simultaneously.  If we are splitting nodes from</span><br><span class=\"line\">      1 tree, then the new nodes to split will be put at the top of this stack, so we will continue</span><br><span class=\"line\">      training the same tree in the next iteration.  This focus allows us to send fewer trees to</span><br><span class=\"line\">      workers on each iteration; see topNodesForGroup below.</span><br><span class=\"line\">     */</span><br><span class=\"line\">    val nodeStack = new mutable.Stack[(Int, LearningNode)]</span><br><span class=\"line\"></span><br><span class=\"line\">    val rng = new Random()</span><br><span class=\"line\">    rng.setSeed(seed)</span><br><span class=\"line\"></span><br><span class=\"line\">    // Allocate and queue root nodes.</span><br><span class=\"line\">    val topNodes = Array.fill[LearningNode](numTrees)(LearningNode.emptyNode(nodeIndex = 1))</span><br><span class=\"line\">    Range(0, numTrees).foreach(treeIndex =&gt; nodeStack.push((treeIndex, topNodes(treeIndex))))</span><br><span class=\"line\"></span><br><span class=\"line\">    timer.stop(&quot;init&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    while (nodeStack.nonEmpty) &#123;</span><br><span class=\"line\">      // Collect some nodes to split, and choose features for each node (if subsampling).</span><br><span class=\"line\">      // Each group of nodes may come from one or multiple trees, and at multiple levels.</span><br><span class=\"line\">      val (nodesForGroup, treeToNodeToIndexInfo) =</span><br><span class=\"line\">        RandomForest.selectNodesToSplit(nodeStack, maxMemoryUsage, metadata, rng)</span><br><span class=\"line\">      // Sanity check (should never occur):</span><br><span class=\"line\">      assert(nodesForGroup.nonEmpty,</span><br><span class=\"line\">        s&quot;RandomForest selected empty nodesForGroup.  Error for unknown reason.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">      // Only send trees to worker if they contain nodes being split this iteration.</span><br><span class=\"line\">      val topNodesForGroup: Map[Int, LearningNode] =</span><br><span class=\"line\">        nodesForGroup.keys.map(treeIdx =&gt; treeIdx -&gt; topNodes(treeIdx)).toMap</span><br><span class=\"line\"></span><br><span class=\"line\">      // Choose node splits, and enqueue new nodes as needed.</span><br><span class=\"line\">      timer.start(&quot;findBestSplits&quot;)</span><br><span class=\"line\">      RandomForest.findBestSplits(baggedInput, metadata, topNodesForGroup, nodesForGroup,</span><br><span class=\"line\">        treeToNodeToIndexInfo, splits, nodeStack, timer, nodeIdCache)</span><br><span class=\"line\">      timer.stop(&quot;findBestSplits&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    baggedInput.unpersist()</span><br><span class=\"line\"></span><br><span class=\"line\">    timer.stop(&quot;total&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    logInfo(&quot;Internal timing for DecisionTree:&quot;)</span><br><span class=\"line\">    logInfo(s&quot;$timer&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // Delete any remaining checkpoints used for node Id cache.</span><br><span class=\"line\">    if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">        nodeIdCache.get.deleteAllCheckpoints()</span><br><span class=\"line\">      &#125; catch &#123;</span><br><span class=\"line\">        case e: IOException =&gt;</span><br><span class=\"line\">          logWarning(s&quot;delete all checkpoints failed. Error reason: $&#123;e.getMessage&#125;&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    val numFeatures = metadata.numFeatures</span><br><span class=\"line\"></span><br><span class=\"line\">    parentUID match &#123;</span><br><span class=\"line\">      case Some(uid) =&gt;</span><br><span class=\"line\">        if (strategy.algo == OldAlgo.Classification) &#123;</span><br><span class=\"line\">          topNodes.map &#123; rootNode =&gt;</span><br><span class=\"line\">            new DecisionTreeClassificationModel(uid, rootNode.toNode, numFeatures,</span><br><span class=\"line\">              strategy.getNumClasses)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          topNodes.map &#123; rootNode =&gt;</span><br><span class=\"line\">            new DecisionTreeRegressionModel(uid, rootNode.toNode, numFeatures)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      case None =&gt;</span><br><span class=\"line\">        if (strategy.algo == OldAlgo.Classification) &#123;</span><br><span class=\"line\">          topNodes.map &#123; rootNode =&gt;</span><br><span class=\"line\">            new DecisionTreeClassificationModel(rootNode.toNode, numFeatures,</span><br><span class=\"line\">              strategy.getNumClasses)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          topNodes.map(rootNode =&gt; new DecisionTreeRegressionModel(rootNode.toNode, numFeatures))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"buildMetadata\"><a href=\"#buildMetadata\" class=\"headerlink\" title=\"buildMetadata\"></a>buildMetadata</h4><p>决策树训练的元数据构造。主要用于计算每个特征的bin数量，以及无序类特征集合, 每个节点使用的特征数量等。其中决策树一般使用所有特征、随机森林分类采用$sqrt(n)$个特征，随机森林回归采用$\\frac{n}{3}$个特征</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def buildMetadata(</span><br><span class=\"line\">      input: RDD[LabeledPoint],</span><br><span class=\"line\">      strategy: Strategy,</span><br><span class=\"line\">      numTrees: Int,</span><br><span class=\"line\">      featureSubsetStrategy: String): DecisionTreeMetadata = &#123;</span><br><span class=\"line\">    //特征数量</span><br><span class=\"line\">    val numFeatures = input.map(_.features.size).take(1).headOption.getOrElse &#123;</span><br><span class=\"line\">      throw new IllegalArgumentException(s&quot;DecisionTree requires size of input RDD &gt; 0, &quot; +</span><br><span class=\"line\">        s&quot;but was given by empty one.&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    val numExamples = input.count() //样本数量</span><br><span class=\"line\">    val numClasses = strategy.algo match &#123;</span><br><span class=\"line\">      case Classification =&gt; strategy.numClasses</span><br><span class=\"line\">      case Regression =&gt; 0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //最大划分数量 </span><br><span class=\"line\">    val maxPossibleBins = math.min(strategy.maxBins, numExamples).toInt</span><br><span class=\"line\">    if (maxPossibleBins &lt; strategy.maxBins) &#123;</span><br><span class=\"line\">      logWarning(s&quot;DecisionTree reducing maxBins from $&#123;strategy.maxBins&#125; to $maxPossibleBins&quot; +</span><br><span class=\"line\">        s&quot; (= number of training instances)&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //maxPossibleBins可能被numExamples修改过，导致小于刚开始设置的strategy.maxBins。</span><br><span class=\"line\">    //需要进一步确保离散值的特征取值数量小于maxPossibleBins，</span><br><span class=\"line\">    if (strategy.categoricalFeaturesInfo.nonEmpty) &#123;</span><br><span class=\"line\">      val maxCategoriesPerFeature = strategy.categoricalFeaturesInfo.values.max</span><br><span class=\"line\">      val maxCategory =</span><br><span class=\"line\">        strategy.categoricalFeaturesInfo.find(_._2 == maxCategoriesPerFeature).get._1</span><br><span class=\"line\">      require(maxCategoriesPerFeature &lt;= maxPossibleBins,</span><br><span class=\"line\">        s&quot;DecisionTree requires maxBins (= $maxPossibleBins) to be at least as large as the &quot; +</span><br><span class=\"line\">        s&quot;number of values in each categorical feature, but categorical feature $maxCategory &quot; +</span><br><span class=\"line\">        s&quot;has $maxCategoriesPerFeature values. Considering remove this and other categorical &quot; +</span><br><span class=\"line\">        &quot;features with a large number of values, or add more training examples.&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //存储每个无序特征的索引</span><br><span class=\"line\">    val unorderedFeatures = new mutable.HashSet[Int]()</span><br><span class=\"line\">    //存储每个无序特征的bin数量</span><br><span class=\"line\">    val numBins = Array.fill[Int](numFeatures)(maxPossibleBins)</span><br><span class=\"line\">    if (numClasses &gt; 2) &#123; //多分类问题</span><br><span class=\"line\">      //根据maxPossibleBins，计算每个无序特征对应的最大类别数量</span><br><span class=\"line\">      val maxCategoriesForUnorderedFeature =</span><br><span class=\"line\">        ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt</span><br><span class=\"line\">      strategy.categoricalFeaturesInfo.foreach &#123; case (featureIndex, numCategories) =&gt;</span><br><span class=\"line\">        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤</span><br><span class=\"line\">          if (numCategories &gt; 1) &#123;</span><br><span class=\"line\">          //判断离散特征是否可当做无序特征，需要保证</span><br><span class=\"line\">          //bins的数量需要小于2 * ((1 &lt;&lt; numCategories - 1) - 1)）</span><br><span class=\"line\">          if (numCategories &lt;= maxCategoriesForUnorderedFeature) &#123;</span><br><span class=\"line\">            unorderedFeatures.add(featureIndex)</span><br><span class=\"line\">            //有numCategories个取值的的特征，对应bins数量为(1 &lt;&lt; numCategories - 1) - 1</span><br><span class=\"line\">            //此处刚开始有点疑惑，感觉应该是2 *（(1 &lt;&lt; numCategories - 1) - 1）</span><br><span class=\"line\">            //通过DecisionTreeMetadata中numSplits函数发现，此处的bin数量和split数量有一定对应关系，(featureIndex)</span><br><span class=\"line\">           //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1</span><br><span class=\"line\">            numBins(featureIndex) = numUnorderedBins(numCategories)</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">            //对于其他离散特征，numBins数量为特征可能的取值数量</span><br><span class=\"line\">            numBins(featureIndex) = numCategories</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123; //对于二值分类或回归问题</span><br><span class=\"line\">      strategy.categoricalFeaturesInfo.foreach &#123; case (featureIndex, numCategories) =&gt;</span><br><span class=\"line\">        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤</span><br><span class=\"line\">        if (numCategories &gt; 1) &#123;</span><br><span class=\"line\">          //numBins数量为特征可能的取值数量</span><br><span class=\"line\">          numBins(featureIndex) = numCategories </span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //设置每个分支节点对应的特征数量</span><br><span class=\"line\">    val _featureSubsetStrategy = featureSubsetStrategy match &#123;</span><br><span class=\"line\">      case &quot;auto&quot; =&gt;</span><br><span class=\"line\">        if (numTrees == 1) &#123; //如果是树，使用所有特征n</span><br><span class=\"line\">          &quot;all&quot;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          if (strategy.algo == Classification) &#123; //如果是用于分类的随机森林，使用sqrt(n)个特征</span><br><span class=\"line\">            &quot;sqrt&quot;</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">            &quot;onethird&quot;  //如果是用于回归的随机森林，使用n/3个特征</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      case _ =&gt; featureSubsetStrategy</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    val numFeaturesPerNode: Int = _featureSubsetStrategy match &#123;</span><br><span class=\"line\">      case &quot;all&quot; =&gt; numFeatures</span><br><span class=\"line\">      case &quot;sqrt&quot; =&gt; math.sqrt(numFeatures).ceil.toInt</span><br><span class=\"line\">      case &quot;log2&quot; =&gt; math.max(1, (math.log(numFeatures) / math.log(2)).ceil.toInt)</span><br><span class=\"line\">      case &quot;onethird&quot; =&gt; (numFeatures / 3.0).ceil.toInt</span><br><span class=\"line\">      case _ =&gt;</span><br><span class=\"line\">        Try(_featureSubsetStrategy.toInt).filter(_ &gt; 0).toOption match &#123;</span><br><span class=\"line\">          case Some(value) =&gt; math.min(value, numFeatures)</span><br><span class=\"line\">          case None =&gt;</span><br><span class=\"line\">            Try(_featureSubsetStrategy.toDouble).filter(_ &gt; 0).filter(_ &lt;= 1.0).toOption match &#123;</span><br><span class=\"line\">              case Some(value) =&gt; math.ceil(value * numFeatures).toInt</span><br><span class=\"line\">              case _ =&gt; throw new IllegalArgumentException(s&quot;Supported values:&quot; +</span><br><span class=\"line\">                s&quot; $&#123;RandomForestParams.supportedFeatureSubsetStrategies.mkString(&quot;, &quot;)&#125;,&quot; +</span><br><span class=\"line\">                s&quot; (0.0-1.0], [1-n].&quot;)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    new DecisionTreeMetadata(numFeatures, numExamples, numClasses, numBins.max,</span><br><span class=\"line\">      strategy.categoricalFeaturesInfo, unorderedFeatures.toSet, numBins,</span><br><span class=\"line\">      strategy.impurity, strategy.quantileCalculationStrategy, strategy.maxDepth,</span><br><span class=\"line\">      strategy.minInstancesPerNode, strategy.minInfoGain, numTrees, numFeaturesPerNode)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"DecisionTreeMetadata类\"><a href=\"#DecisionTreeMetadata类\" class=\"headerlink\" title=\"DecisionTreeMetadata类\"></a>DecisionTreeMetadata类</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private[spark] class DecisionTreeMetadata(</span><br><span class=\"line\">    val numFeatures: Int,</span><br><span class=\"line\">    val numExamples: Long,</span><br><span class=\"line\">    val numClasses: Int,</span><br><span class=\"line\">    val maxBins: Int,</span><br><span class=\"line\">    val featureArity: Map[Int, Int],</span><br><span class=\"line\">    val unorderedFeatures: Set[Int],</span><br><span class=\"line\">    val numBins: Array[Int],</span><br><span class=\"line\">    val impurity: Impurity,</span><br><span class=\"line\">    val quantileStrategy: QuantileStrategy,</span><br><span class=\"line\">    val maxDepth: Int,</span><br><span class=\"line\">    val minInstancesPerNode: Int,</span><br><span class=\"line\">    val minInfoGain: Double,</span><br><span class=\"line\">    val numTrees: Int,</span><br><span class=\"line\">    val numFeaturesPerNode: Int) extends Serializable &#123;</span><br><span class=\"line\">  //判断是否为无序特征</span><br><span class=\"line\">  def isUnordered(featureIndex: Int): Boolean = unorderedFeatures.contains(featureIndex)</span><br><span class=\"line\">  //判断是否用于分类的决策树（随机森林）</span><br><span class=\"line\">  def isClassification: Boolean = numClasses &gt;= 2</span><br><span class=\"line\">  //判断是否用于多分类的决策树（随机森林）</span><br><span class=\"line\">  def isMulticlass: Boolean = numClasses &gt; 2</span><br><span class=\"line\">  //判断是否拥有离散特征的多分类决策树（随机森林）</span><br><span class=\"line\">  def isMulticlassWithCategoricalFeatures: Boolean = isMulticlass &amp;&amp; (featureArity.size &gt; 0)</span><br><span class=\"line\">  //判断是否离散特征</span><br><span class=\"line\">  def isCategorical(featureIndex: Int): Boolean = featureArity.contains(featureIndex)</span><br><span class=\"line\"> //判断是否连续特征</span><br><span class=\"line\">  def isContinuous(featureIndex: Int): Boolean = !featureArity.contains(featureIndex)</span><br><span class=\"line\">  //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1</span><br><span class=\"line\">  def numSplits(featureIndex: Int): Int = if (isUnordered(featureIndex)) &#123;</span><br><span class=\"line\">    numBins(featureIndex)</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    numBins(featureIndex) - 1</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  //对于连续特征，根据划分数量设置bin数量为划分数量加1</span><br><span class=\"line\">  def setNumSplits(featureIndex: Int, numSplits: Int) &#123;</span><br><span class=\"line\">    require(isContinuous(featureIndex),</span><br><span class=\"line\">      s&quot;Only number of bin for a continuous feature can be set.&quot;)</span><br><span class=\"line\">    numBins(featureIndex) = numSplits + 1</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  //判断是否需要对特征进行采样</span><br><span class=\"line\">  def subsamplingFeatures: Boolean = numFeatures != numFeaturesPerNode</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"findSplits\"><a href=\"#findSplits\" class=\"headerlink\" title=\"findSplits\"></a>findSplits</h4><p>通过使用采样的样本，寻找样本的划分splits和划分后的bins。</p>\n<p><strong>划分的思想：</strong>对连续特征和离散特征，分别采用不同处理方式。对于每个连续特征，numBins - 1个splits, 代表每个树的节点的所有可能的二值化分；对于每个离散特征，无序离散特征（用于多分类的维度较大的feature）基于特征的子集进行划分。有序类特征（用于回归、二分类、多分类的维度较小的feature)的每个取值对应一个bin.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected[tree] def findSplits(</span><br><span class=\"line\">      input: RDD[LabeledPoint],</span><br><span class=\"line\">      metadata: DecisionTreeMetadata,</span><br><span class=\"line\">      seed: Long): Array[Array[Split]] = &#123;</span><br><span class=\"line\">    logDebug(&quot;isMulticlass = &quot; + metadata.isMulticlass)</span><br><span class=\"line\">    val numFeatures = metadata.numFeatures //特征的数量</span><br><span class=\"line\">    // 得到所有连续特征索引</span><br><span class=\"line\">    val continuousFeatures = Range(0, numFeatures).filter(metadata.isContinuous)</span><br><span class=\"line\">    //当有连续特征的时候需要采样样本   </span><br><span class=\"line\">    val sampledInput = if (continuousFeatures.nonEmpty) &#123;</span><br><span class=\"line\">      // 计算近似分位数计算需要的样本数</span><br><span class=\"line\">      val requiredSamples = math.max(metadata.maxBins * metadata.maxBins, 10000)</span><br><span class=\"line\">      // 计算需要的样本占总样本比例</span><br><span class=\"line\">      val fraction = if (requiredSamples &lt; metadata.numExamples) &#123;</span><br><span class=\"line\">        requiredSamples.toDouble / metadata.numExamples</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        1.0</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      logDebug(&quot;fraction of data used for calculating quantiles = &quot; + fraction)</span><br><span class=\"line\">      input.sample(withReplacement = false, fraction, new XORShiftRandom(seed).nextInt())</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      input.sparkContext.emptyRDD[LabeledPoint]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //对每个连续特征和非有序类离散特征，通过排序的方式，寻找最佳的splits点</span><br><span class=\"line\">    findSplitsBySorting(sampledInput, metadata, continuousFeatures)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//对每个特征，通过排序的方式，寻找最佳的splits点</span><br><span class=\"line\">private def findSplitsBySorting(</span><br><span class=\"line\">     input: RDD[LabeledPoint],</span><br><span class=\"line\">     metadata: DecisionTreeMetadata,</span><br><span class=\"line\">     continuousFeatures: IndexedSeq[Int]): Array[Array[Split]] = &#123;</span><br><span class=\"line\">  </span><br><span class=\"line\">   //寻找连续特征的划分阈值</span><br><span class=\"line\">   val continuousSplits: scala.collection.Map[Int, Array[Split]] = &#123;</span><br><span class=\"line\">     //设置分区数量，如果连续特征的数量小于原始分区数，则进一步减少分区，防止无效的启动的task任务。</span><br><span class=\"line\">     val numPartitions = math.min(continuousFeatures.length, input.partitions.length)</span><br><span class=\"line\"></span><br><span class=\"line\">     input</span><br><span class=\"line\">       .flatMap(point =&gt; continuousFeatures.map(idx =&gt; (idx, point.features(idx))))</span><br><span class=\"line\">       .groupByKey(numPartitions)</span><br><span class=\"line\">       .map &#123; case (idx, samples) =&gt;</span><br><span class=\"line\">         val thresholds = findSplitsForContinuousFeature(samples, metadata, idx)</span><br><span class=\"line\">         val splits: Array[Split] = thresholds.map(thresh =&gt; new ContinuousSplit(idx, thresh))</span><br><span class=\"line\">         logDebug(s&quot;featureIndex = $idx, numSplits = $&#123;splits.length&#125;&quot;)</span><br><span class=\"line\">         (idx, splits)</span><br><span class=\"line\">       &#125;.collectAsMap()</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   //特征数量</span><br><span class=\"line\">   val numFeatures = metadata.numFeatures</span><br><span class=\"line\">   //汇总所有特征的split(不包括无序离散特征)</span><br><span class=\"line\">   val splits: Array[Array[Split]] = Array.tabulate(numFeatures) &#123;</span><br><span class=\"line\">     //如果是连续特征，返回该连续特征的split</span><br><span class=\"line\">     case i if metadata.isContinuous(i) =&gt;</span><br><span class=\"line\">       val split = continuousSplits(i)</span><br><span class=\"line\">       metadata.setNumSplits(i, split.length)</span><br><span class=\"line\">       split</span><br><span class=\"line\">     //如果是无序离散特征，则提取该特征的split， 具体是对于每个离散特征，其第k个split为其k对应二进制的所有位置为1的数值。</span><br><span class=\"line\">     case i if metadata.isCategorical(i) &amp;&amp; metadata.isUnordered(i) =&gt;</span><br><span class=\"line\">       // Unordered features</span><br><span class=\"line\">       // 2^(maxFeatureValue - 1) - 1 combinations</span><br><span class=\"line\">       //特征的取值数量</span><br><span class=\"line\">       val featureArity = metadata.featureArity(i)</span><br><span class=\"line\">       Array.tabulate[Split](metadata.numSplits(i)) &#123; splitIndex =&gt;</span><br><span class=\"line\">         val categories = extractMultiClassCategories(splitIndex + 1, featureArity)</span><br><span class=\"line\">         new CategoricalSplit(i, categories.toArray, featureArity)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     //对于有序离散特征，暂时不求解split, 在训练阶段求解</span><br><span class=\"line\">     case i if metadata.isCategorical(i) =&gt;</span><br><span class=\"line\">       // Ordered features</span><br><span class=\"line\">       //   Splits are constructed as needed during training.</span><br><span class=\"line\">       Array.empty[Split]</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   splits</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//将input这个数对应的二进制位置为1的位置加入到当前划分</span><br><span class=\"line\">private[tree] def extractMultiClassCategories(</span><br><span class=\"line\">      input: Int,</span><br><span class=\"line\">      maxFeatureValue: Int): List[Double] = &#123;</span><br><span class=\"line\">    var categories = List[Double]()</span><br><span class=\"line\">    var j = 0</span><br><span class=\"line\">    var bitShiftedInput = input</span><br><span class=\"line\">    while (j &lt; maxFeatureValue) &#123;</span><br><span class=\"line\">      if (bitShiftedInput % 2 != 0) &#123;</span><br><span class=\"line\">        // updating the list of categories.</span><br><span class=\"line\">        categories = j.toDouble :: categories</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      // Right shift by one</span><br><span class=\"line\">      bitShiftedInput = bitShiftedInput &gt;&gt; 1</span><br><span class=\"line\">      j += 1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    categories</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//对于连续特征，找到其对应的splits分割点</span><br><span class=\"line\">private[tree] def findSplitsForContinuousFeature(</span><br><span class=\"line\">      featureSamples: Iterable[Double], </span><br><span class=\"line\">      metadata: DecisionTreeMetadata, </span><br><span class=\"line\">      featureIndex: Int): Array[Double] = &#123;</span><br><span class=\"line\">    //确保有连续特征</span><br><span class=\"line\">    require(metadata.isContinuous(featureIndex),</span><br><span class=\"line\">      &quot;findSplitsForContinuousFeature can only be used to find splits for a continuous feature.&quot;)</span><br><span class=\"line\">    //寻找splits分割点</span><br><span class=\"line\">    val splits = if (featureSamples.isEmpty) &#123;</span><br><span class=\"line\">      Array.empty[Double]  //如果样本数为0， 返回空数组</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //得到metadata里的split数量</span><br><span class=\"line\">      val numSplits = metadata.numSplits(featureIndex) </span><br><span class=\"line\"></span><br><span class=\"line\">      //在采样得到的样本中，计算每个特征取值的计数、以及总样本数量</span><br><span class=\"line\">      val (valueCountMap, numSamples) = featureSamples.foldLeft((Map.empty[Double, Int], 0)) &#123;</span><br><span class=\"line\">        case ((m, cnt), x) =&gt;</span><br><span class=\"line\">          (m + ((x, m.getOrElse(x, 0) + 1)), cnt + 1)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      // 对于每个特征取值进行排序</span><br><span class=\"line\">      val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray</span><br><span class=\"line\">      //如果得到的possible splits数量小于metadata中该特征的的split数量，则直接以当前每个特征取值作为分割的阈值</span><br><span class=\"line\">      val possibleSplits = valueCounts.length - 1</span><br><span class=\"line\">      if (possibleSplits &lt;= numSplits) &#123; </span><br><span class=\"line\">        valueCounts.map(_._1).init</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        //否则，根据总样本数量，计算平均每个区间对应的特征取值数量，假设为n。然后，对于n, 2*n, 3*n ...的位置分别设置标记。设置2个游标分别指向valueCounts内部连续的两个特征取值，从前向后遍历，当后面游标到标记的距离大于前面的游标时，将前面游标的位置对应的特征取值设置为一个split点。</span><br><span class=\"line\">        //计算平均每个区间对应的特征取值数量</span><br><span class=\"line\">        val stride: Double = numSamples.toDouble / (numSplits + 1)</span><br><span class=\"line\">        logDebug(&quot;stride = &quot; + stride)</span><br><span class=\"line\">        //splitsBuilder用于存储每个分割阈值</span><br><span class=\"line\">        val splitsBuilder = mutable.ArrayBuilder.make[Double]</span><br><span class=\"line\">        //特征取值从小到大的位置索引</span><br><span class=\"line\">        var index = 1</span><br><span class=\"line\">        //当前访问的所有特征取值数量之和</span><br><span class=\"line\">        var currentCount = valueCounts(0)._2</span><br><span class=\"line\">        //下一次的标记位置      </span><br><span class=\"line\">        var targetCount = stride</span><br><span class=\"line\">        while (index &lt; valueCounts.length) &#123;</span><br><span class=\"line\">          val previousCount = currentCount</span><br><span class=\"line\">          currentCount += valueCounts(index)._2</span><br><span class=\"line\">          val previousGap = math.abs(previousCount - targetCount)</span><br><span class=\"line\">          val currentGap = math.abs(currentCount - targetCount)</span><br><span class=\"line\">          //使前面游标和后面游标的距离更小，且较小游标距离标记位置的距离最近</span><br><span class=\"line\">          if (previousGap &lt; currentGap) &#123;</span><br><span class=\"line\">            splitsBuilder += valueCounts(index - 1)._1</span><br><span class=\"line\">            targetCount += stride</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          index += 1</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        splitsBuilder.result()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    splits</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"TreePoint-convertToTreeRDD\"><a href=\"#TreePoint-convertToTreeRDD\" class=\"headerlink\" title=\"TreePoint.convertToTreeRDD\"></a>TreePoint.convertToTreeRDD</h4><p>调用TreePoint类的convertToTreeRDD方法，RDD[LabeledPoint]转化为RDD[TreePoint]。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def convertToTreeRDD(</span><br><span class=\"line\">     input: RDD[LabeledPoint],</span><br><span class=\"line\">     splits: Array[Array[Split]],</span><br><span class=\"line\">     metadata: DecisionTreeMetadata): RDD[TreePoint] = &#123;</span><br><span class=\"line\">   // 构建数组featureArity，存储每个特征对应的离散值个数，连续值对应的value为0</span><br><span class=\"line\">   val featureArity: Array[Int] = new Array[Int](metadata.numFeatures)</span><br><span class=\"line\">   var featureIndex = 0</span><br><span class=\"line\">   while (featureIndex &lt; metadata.numFeatures) &#123;</span><br><span class=\"line\">     featureArity(featureIndex) = metadata.featureArity.getOrElse(featureIndex, 0)</span><br><span class=\"line\">     featureIndex += 1</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   //获得所有连续特征的分裂阈值，如果是离散特征，则数组对应空</span><br><span class=\"line\">   val thresholds: Array[Array[Double]] = featureArity.zipWithIndex.map &#123; case (arity, idx) =&gt;</span><br><span class=\"line\">     if (arity == 0) &#123;</span><br><span class=\"line\">       splits(idx).map(_.asInstanceOf[ContinuousSplit].threshold)</span><br><span class=\"line\">     &#125; else &#123;</span><br><span class=\"line\">       Array.empty[Double]</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   //将样本的每个原始特征，转化为对应的bin特征值，用于训练</span><br><span class=\"line\">   input.map &#123; x =&gt;</span><br><span class=\"line\">     TreePoint.labeledPointToTreePoint(x, thresholds, featureArity)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//将单个样本的原始特征，转化为对应的bin特征值，用于训练</span><br><span class=\"line\">private def labeledPointToTreePoint(</span><br><span class=\"line\">    labeledPoint: LabeledPoint,</span><br><span class=\"line\">    thresholds: Array[Array[Double]],</span><br><span class=\"line\">    featureArity: Array[Int]): TreePoint = &#123;</span><br><span class=\"line\">  //特征数量</span><br><span class=\"line\">  val numFeatures = labeledPoint.features.size</span><br><span class=\"line\">  //为每个特征找到对应的bin特征值，存储在arr数组</span><br><span class=\"line\">  val arr = new Array[Int](numFeatures)</span><br><span class=\"line\">  var featureIndex = 0</span><br><span class=\"line\">  while (featureIndex &lt; numFeatures) &#123;</span><br><span class=\"line\">    //寻找数据点labeledPoint、当前特征featureIndex对应的bin特征值</span><br><span class=\"line\">    arr(featureIndex) =</span><br><span class=\"line\">      findBin(featureIndex, labeledPoint, featureArity(featureIndex), thresholds(featureIndex))</span><br><span class=\"line\">    featureIndex += 1</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  new TreePoint(labeledPoint.label, arr)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def findBin(</span><br><span class=\"line\">      featureIndex: Int,</span><br><span class=\"line\">      labeledPoint: LabeledPoint,</span><br><span class=\"line\">      featureArity: Int,</span><br><span class=\"line\">      thresholds: Array[Double]): Int = &#123;</span><br><span class=\"line\">    //获取当前labeledPoint的第featureIndex个原始特征值</span><br><span class=\"line\">    val featureValue = labeledPoint.features(featureIndex)</span><br><span class=\"line\">    </span><br><span class=\"line\">    if (featureArity == 0) &#123; </span><br><span class=\"line\">      //如果是连续特征，利用二分法得到当前特征值对应的离散区间下标</span><br><span class=\"line\">      val idx = java.util.Arrays.binarySearch(thresholds, featureValue)</span><br><span class=\"line\">      if (idx &gt;= 0) &#123;</span><br><span class=\"line\">        idx</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        -idx - 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //如果是离散值，则直接返回当前的特征值</span><br><span class=\"line\">      if (featureValue &lt; 0 || featureValue &gt;= featureArity) &#123;</span><br><span class=\"line\">        throw new IllegalArgumentException(</span><br><span class=\"line\">          s&quot;DecisionTree given invalid data:&quot; +</span><br><span class=\"line\">            s&quot; Feature $featureIndex is categorical with values in &#123;0,...,$&#123;featureArity - 1&#125;,&quot; +</span><br><span class=\"line\">            s&quot; but a data point gives it value $featureValue.\\n&quot; +</span><br><span class=\"line\">            &quot;  Bad data point: &quot; + labeledPoint.toString)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      featureValue.toInt</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//LabeledPoint类</span><br><span class=\"line\">case class LabeledPoint(@Since(&quot;2.0.0&quot;) label: Double, @Since(&quot;2.0.0&quot;) features: Vector) &#123;</span><br><span class=\"line\">  override def toString: String = &#123;</span><br><span class=\"line\">    s&quot;($label,$features)&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//TreePoint类</span><br><span class=\"line\">private[spark] class TreePoint(val label: Double, val binnedFeatures: Array[Int])</span><br><span class=\"line\">  extends Serializable &#123;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"BaggedPoint-convertToBaggedRDD\"><a href=\"#BaggedPoint-convertToBaggedRDD\" class=\"headerlink\" title=\"BaggedPoint.convertToBaggedRDD\"></a>BaggedPoint.convertToBaggedRDD</h4><p>RDD[Datum]数据集转换成RDD[BaggedPoint[Datum]的表示类型，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def convertToBaggedRDD[Datum] (</span><br><span class=\"line\">    input: RDD[Datum], //输入数据集</span><br><span class=\"line\">    subsamplingRate: Double, //采样率</span><br><span class=\"line\">    numSubsamples: Int, //采样次数</span><br><span class=\"line\">    withReplacement: Boolean, //是否有放回</span><br><span class=\"line\">    //随机数种子</span><br><span class=\"line\">    seed: Long = Utils.random.nextLong()): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class=\"line\">  if (withReplacement) &#123;//有放回采样，生成BaggedPoint结构表示</span><br><span class=\"line\">    convertToBaggedRDDSamplingWithReplacement(input, subsamplingRate, numSubsamples, seed)</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    //当采样比为1，并且采样次数为1时，不采样，只生成BaggedPoint结构表示</span><br><span class=\"line\">    if (numSubsamples == 1 &amp;&amp; subsamplingRate == 1.0) &#123;</span><br><span class=\"line\">      convertToBaggedRDDWithoutSampling(input)</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //无放回采样，生成BaggedPoint结构表示</span><br><span class=\"line\">      convertToBaggedRDDSamplingWithoutReplacement(input, subsamplingRate, numSubsamples, seed)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//有放回采样，数据转换为RDD[BaggedPoint[Datum]]</span><br><span class=\"line\">private def convertToBaggedRDDSamplingWithReplacement[Datum] (</span><br><span class=\"line\">    input: RDD[Datum],//输入数据集</span><br><span class=\"line\">    subsample: Double,//采样率</span><br><span class=\"line\">    numSubsamples: Int,//采样次数</span><br><span class=\"line\">    //随机数种子</span><br><span class=\"line\">    seed: Long): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class=\"line\">  input.mapPartitionsWithIndex &#123; (partitionIndex, instances) =&gt;</span><br><span class=\"line\">    //每个分区生成一个泊松采样器，通过采样率、随机种子、分区索引等初始化</span><br><span class=\"line\">    val poisson = new PoissonDistribution(subsample)</span><br><span class=\"line\">    poisson.reseedRandomGenerator(seed + partitionIndex + 1)</span><br><span class=\"line\">    //将每个实例变换成BaggedPoint结构表示</span><br><span class=\"line\">    instances.map &#123; instance =&gt;</span><br><span class=\"line\">      val subsampleWeights = new Array[Double](numSubsamples)</span><br><span class=\"line\">      var subsampleIndex = 0</span><br><span class=\"line\">      //依次对每次采样，生成权重（即该实例在每次无放回采样出现的次数）</span><br><span class=\"line\">      while (subsampleIndex &lt; numSubsamples) &#123;</span><br><span class=\"line\">        subsampleWeights(subsampleIndex) = poisson.sample()</span><br><span class=\"line\">        subsampleIndex += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      //生成BaggedPoint结构表示</span><br><span class=\"line\">      new BaggedPoint(instance, subsampleWeights) </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//BaggedPoint类，datum表示数据实例，subsampleWeights表示当前实例在每个采样中的权重。</span><br><span class=\"line\">如(datum, [1, 0, 4])表示有3次采样，数据实例在3次采样中出现的次数分别为1，0，4</span><br><span class=\"line\">private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])</span><br><span class=\"line\">  extends Serializable</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//原始数据（不采样）直接转换为BaggedPoint结构表示</span><br><span class=\"line\">private def convertToBaggedRDDWithoutSampling[Datum] (</span><br><span class=\"line\">    input: RDD[Datum]): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class=\"line\">  input.map(datum =&gt; new BaggedPoint(datum, Array(1.0)))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//无放回采样，数据转换为RDD[BaggedPoint[Datum]]</span><br><span class=\"line\">private def convertToBaggedRDDSamplingWithoutReplacement[Datum] (</span><br><span class=\"line\">    input: RDD[Datum],</span><br><span class=\"line\">    subsamplingRate: Double,</span><br><span class=\"line\">    numSubsamples: Int,</span><br><span class=\"line\">    seed: Long): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class=\"line\">  input.mapPartitionsWithIndex &#123; (partitionIndex, instances) =&gt;</span><br><span class=\"line\">    //使用随机数种子，分区索引，构建随机数生成器</span><br><span class=\"line\">    val rng = new XORShiftRandom</span><br><span class=\"line\">    rng.setSeed(seed + partitionIndex + 1)</span><br><span class=\"line\">    //将每个实例变换成BaggedPoint结构表示</span><br><span class=\"line\">    instances.map &#123; instance =&gt;</span><br><span class=\"line\">      val subsampleWeights = new Array[Double](numSubsamples)</span><br><span class=\"line\">      var subsampleIndex = 0</span><br><span class=\"line\">      //对于每次采样，生成0-1之间的随机数，如果小于采样比，则对应权重为1，否则为0</span><br><span class=\"line\">      while (subsampleIndex &lt; numSubsamples) &#123;</span><br><span class=\"line\">        val x = rng.nextDouble()</span><br><span class=\"line\">        subsampleWeights(subsampleIndex) = &#123;</span><br><span class=\"line\">          if (x &lt; subsamplingRate) 1.0 else 0.0</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        subsampleIndex += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      //转换为BaggedPoint结构数据</span><br><span class=\"line\">      new BaggedPoint(instance, subsampleWeights)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"RandomForest-selectNodesToSplit\"><a href=\"#RandomForest-selectNodesToSplit\" class=\"headerlink\" title=\"RandomForest.selectNodesToSplit\"></a>RandomForest.selectNodesToSplit</h4><p>选择当前迭代待分裂的节点，以及确定每个节点使用的特征。每次选择都根据内存限制、每个节点占用的内存（如果每个节点使用的是采样后的特征），自适应地确定节点个数。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private[tree] def selectNodesToSplit(</span><br><span class=\"line\">      nodeStack: mutable.Stack[(Int, LearningNode)], //存储节点的栈结构</span><br><span class=\"line\">      maxMemoryUsage: Long, //最大占用内存限制</span><br><span class=\"line\">      metadata: DecisionTreeMetadata, //元数据</span><br><span class=\"line\">      //随机数</span><br><span class=\"line\">      rng: Random): </span><br><span class=\"line\">      //返回值包括：（1）每个树对应的待分裂节点数组， </span><br><span class=\"line\">      //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）</span><br><span class=\"line\">      (Map[Int, Array[LearningNode]], Map[Int, Map[Int, NodeIndexInfo]]) = &#123;</span><br><span class=\"line\">      //nodesForGroup(treeIndex) 存储第treeIndex个树对应的待分裂节点数组</span><br><span class=\"line\">      val mutableNodesForGroup = new mutable.HashMap[Int, mutable.ArrayBuffer[LearningNode]]()</span><br><span class=\"line\">      //每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）</span><br><span class=\"line\">      val mutableTreeToNodeToIndexInfo =</span><br><span class=\"line\">      new mutable.HashMap[Int, mutable.HashMap[Int, NodeIndexInfo]]()</span><br><span class=\"line\">      var memUsage: Long = 0L  //当前使用内存</span><br><span class=\"line\">      var numNodesInGroup = 0  //当前分组的节点数量</span><br><span class=\"line\">      // If maxMemoryInMB is set very small, we want to still try to split 1 node,</span><br><span class=\"line\">      // so we allow one iteration if memUsage == 0.</span><br><span class=\"line\">      //如果栈不空，并且（1）如果内存上限设置非常小，我们要去报至少能有1个节点用于分裂</span><br><span class=\"line\">      //（2）当前使用内存小于内存上限值，则进一步选择节点用于分裂</span><br><span class=\"line\">      while (nodeStack.nonEmpty &amp;&amp; (memUsage &lt; maxMemoryUsage || memUsage == 0)) &#123;</span><br><span class=\"line\">      val (treeIndex, node) = nodeStack.top //选择栈顶节点</span><br><span class=\"line\">      // Choose subset of features for node (if subsampling).</span><br><span class=\"line\">     </span><br><span class=\"line\">      val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) &#123;       //如果特征需要采样，则对所有特征进行无放回采样</span><br><span class=\"line\">        Some(SamplingUtils.reservoirSampleAndCount(Range(0,</span><br><span class=\"line\">          metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong())._1)</span><br><span class=\"line\">      &#125; else &#123;//如果特征不需要采样，则返回None</span><br><span class=\"line\">        None</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      //通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的内存</span><br><span class=\"line\">      val nodeMemUsage = RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L</span><br><span class=\"line\">      ////检查增加当前节点后，内存容量是是否超过限制</span><br><span class=\"line\">      if (memUsage + nodeMemUsage &lt;= maxMemoryUsage || memUsage == 0) &#123;</span><br><span class=\"line\">        //如果加入该节点后内存没有超过限制</span><br><span class=\"line\">        nodeStack.pop() //当前节点出栈</span><br><span class=\"line\">        //更新mutableNodesForGroup，将当前节点加入对应treeIndex的节点数组</span><br><span class=\"line\">        mutableNodesForGroup.getOrElseUpdate(treeIndex, new mutable.ArrayBuffer[LearningNode]()) +=</span><br><span class=\"line\">          node</span><br><span class=\"line\">        //更新mutableTreeToNodeToIndexInfo，将当前节点的具体信息，加入对应treeindex的节点map</span><br><span class=\"line\">        mutableTreeToNodeToIndexInfo</span><br><span class=\"line\">          .getOrElseUpdate(treeIndex, new mutable.HashMap[Int, NodeIndexInfo]())(node.id)</span><br><span class=\"line\">          = new NodeIndexInfo(numNodesInGroup, featureSubset)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      numNodesInGroup += 1 //当前分组的节点数量加一</span><br><span class=\"line\">      memUsage += nodeMemUsage //当前使用内存数量加一</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (memUsage &gt; maxMemoryUsage) &#123;</span><br><span class=\"line\">      // If maxMemoryUsage is 0, we should still allow splitting 1 node.</span><br><span class=\"line\">      logWarning(s&quot;Tree learning is using approximately $memUsage bytes per iteration, which&quot; +</span><br><span class=\"line\">        s&quot; exceeds requested limit maxMemoryUsage=$maxMemoryUsage. This allows splitting&quot; +</span><br><span class=\"line\">        s&quot; $numNodesInGroup nodes in this iteration.&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //转换可变map为不可变map类型</span><br><span class=\"line\">    val nodesForGroup: Map[Int, Array[LearningNode]] =</span><br><span class=\"line\">      mutableNodesForGroup.mapValues(_.toArray).toMap</span><br><span class=\"line\">    val treeToNodeToIndexInfo = mutableTreeToNodeToIndexInfo.mapValues(_.toMap).toMap</span><br><span class=\"line\">    //返回（1）每个树对应的待分裂节点数组， </span><br><span class=\"line\">    //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）</span><br><span class=\"line\">    (nodesForGroup, treeToNodeToIndexInfo)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//无放回采样</span><br><span class=\"line\">def reservoirSampleAndCount[T: ClassTag](</span><br><span class=\"line\">      input: Iterator[T], //input输入的迭代器</span><br><span class=\"line\">      k: Int, //采样的样本数</span><br><span class=\"line\">      seed: Long = Random.nextLong()) //随机数种子</span><br><span class=\"line\">    : (Array[T], Long) = &#123;</span><br><span class=\"line\">    val reservoir = new Array[T](k) //存储采样结果的数组</span><br><span class=\"line\">    // 放置迭代器的前k个元素到结果数组</span><br><span class=\"line\">    var i = 0</span><br><span class=\"line\">    while (i &lt; k &amp;&amp; input.hasNext) &#123;</span><br><span class=\"line\">      val item = input.next()</span><br><span class=\"line\">      reservoir(i) = item</span><br><span class=\"line\">      i += 1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    //如果输入元素个数小于k, 则这k个特征作为返回的结果</span><br><span class=\"line\">    if (i &lt; k) &#123;</span><br><span class=\"line\">      // If input size &lt; k, trim the array to return only an array of input size.</span><br><span class=\"line\">      val trimReservoir = new Array[T](i)</span><br><span class=\"line\">      System.arraycopy(reservoir, 0, trimReservoir, 0, i)</span><br><span class=\"line\">      (trimReservoir, i) //返回结果数组，以及原始数组的元素个数</span><br><span class=\"line\">    &#125; else &#123; </span><br><span class=\"line\">      //如果输入元素个数大于k, 继续采样过程，将后面元素以一定概率随机替换前面的某个元素</span><br><span class=\"line\">      var l = i.toLong</span><br><span class=\"line\">      val rand = new XORShiftRandom(seed)</span><br><span class=\"line\">      while (input.hasNext) &#123;</span><br><span class=\"line\">        val item = input.next()</span><br><span class=\"line\">        l += 1</span><br><span class=\"line\">        //当前结果数组有k个元素，l为当前元素的序号。k/l为当前元素替换结果数组中某个元素的概率。</span><br><span class=\"line\">        //在进行替换时，对结果数组的每个元素以相等概率发生替换</span><br><span class=\"line\">        //具体方式是产生一个0到l-1之间的随机整数replacementIndex，</span><br><span class=\"line\">        //如果小于k则对第replacementIndex这个元素进行替换</span><br><span class=\"line\">        val replacementIndex = (rand.nextDouble() * l).toLong</span><br><span class=\"line\">        if (replacementIndex &lt; k) &#123;</span><br><span class=\"line\">          reservoir(replacementIndex.toInt) = item</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      (reservoir, l) //返回结果数组，以及原始数组的元素个数</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的字节数</span><br><span class=\"line\">private def aggregateSizeForNode(</span><br><span class=\"line\">    metadata: DecisionTreeMetadata,</span><br><span class=\"line\">    featureSubset: Option[Array[Int]]): Long = &#123;</span><br><span class=\"line\">  //得到所有使用的特征的bin的数量之后</span><br><span class=\"line\">  val totalBins = if (featureSubset.nonEmpty) &#123;</span><br><span class=\"line\">    //如果使用采样特征，得到采样后的所有特征bin数量之和</span><br><span class=\"line\">    featureSubset.get.map(featureIndex =&gt; metadata.numBins(featureIndex).toLong).sum</span><br><span class=\"line\">  &#125; else &#123;//否则使用所有的特征的bin数量之和</span><br><span class=\"line\">    metadata.numBins.map(_.toLong).sum</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  if (metadata.isClassification) &#123;</span><br><span class=\"line\">    //如果是分类问题，则返回bin数量之和*类别个数</span><br><span class=\"line\">    metadata.numClasses * totalBins </span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    //否则返回bin数量之和*3</span><br><span class=\"line\">    3 * totalBins</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"RandomForest-findBestSplits\"><a href=\"#RandomForest-findBestSplits\" class=\"headerlink\" title=\"RandomForest.findBestSplits\"></a>RandomForest.findBestSplits</h4><p>给定selectNodesToSplit方法选择的一组节点，找到每个节点对应的最佳分类特征的分裂位置。<strong>求解的主要思想如下：</strong></p>\n<p><strong>基于节点的分组进行并行训练：</strong>对一组的节点同时进行每个bin的统计和计算，减少不必要的数据传输成本。这样每次迭代需要更多的计算和存储成本，但是可以大大减少迭代的次数</p>\n<p><strong>基于bin的最佳分割点计算：</strong>基于bin的计算来寻找最佳分割点，计算的思想不是依次对每个样本计算其对每个孩子节点的增益贡献，而是先将所有样本的每个特征映射到对应的bin，通过聚合每个bin的数据，进一步计算对应每个特征每个分割的增益。</p>\n<p><strong>对每个partition进行聚合：</strong>由于提取知道了每个特征对应的split个数，因此可以用一个数组存储所有的bin的聚合信息，通过使用RDD的聚合方法，大大减少通讯开销。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private[tree] def findBestSplits(</span><br><span class=\"line\">     input: RDD[BaggedPoint[TreePoint]], //训练数据</span><br><span class=\"line\">     metadata: DecisionTreeMetadata, //随机森林元数据信息</span><br><span class=\"line\">     topNodesForGroup: Map[Int, LearningNode], //存储当前节点分组对应的每个树的根节点</span><br><span class=\"line\">     nodesForGroup: Map[Int, Array[LearningNode]],//存储当前节点分组对应的每个树的节点数组</span><br><span class=\"line\">     treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]],//存储当前节点分组对应的每个树索引、节点索引、及详细信息</span><br><span class=\"line\">     splits: Array[Array[Split]], //存储每个特征的所有split信息</span><br><span class=\"line\">     //存储节点的栈结构，初始化时为各个树的根节点</span><br><span class=\"line\">     nodeStack: mutable.Stack[(Int, LearningNode)],</span><br><span class=\"line\">     timer: TimeTracker = new TimeTracker,       </span><br><span class=\"line\">     nodeIdCache: Option[NodeIdCache] = None): Unit = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   //存储当前分组的节点数量</span><br><span class=\"line\">   val numNodes = nodesForGroup.values.map(_.length).sum</span><br><span class=\"line\">   logDebug(&quot;numNodes = &quot; + numNodes)</span><br><span class=\"line\">   logDebug(&quot;numFeatures = &quot; + metadata.numFeatures)</span><br><span class=\"line\">   logDebug(&quot;numClasses = &quot; + metadata.numClasses)</span><br><span class=\"line\">   logDebug(&quot;isMulticlass = &quot; + metadata.isMulticlass)</span><br><span class=\"line\">   logDebug(&quot;isMulticlassWithCategoricalFeatures = &quot; +</span><br><span class=\"line\">     metadata.isMulticlassWithCategoricalFeatures)</span><br><span class=\"line\">   logDebug(&quot;using nodeIdCache = &quot; + nodeIdCache.nonEmpty.toString)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\">   //对于一个特定的树的特定节点，通过baggedPoint数据点，更新DTStatsAggregator聚合信息（更新相关的特征及bin的聚合类信息）</span><br><span class=\"line\">   def nodeBinSeqOp(</span><br><span class=\"line\">       treeIndex: Int, //树的索引</span><br><span class=\"line\">       nodeInfo: NodeIndexInfo, //节点信息</span><br><span class=\"line\">       agg: Array[DTStatsAggregator], //聚合信息，(node, feature, bin)</span><br><span class=\"line\">       baggedPoint: BaggedPoint[TreePoint]): Unit = &#123;//数据点</span><br><span class=\"line\">     if (nodeInfo != null) &#123;//如果节点信息不为空，表示该节点在当前计算的节点集合中</span><br><span class=\"line\">       val aggNodeIndex = nodeInfo.nodeIndexInGroup //该节点在当前分组的编号</span><br><span class=\"line\">       val featuresForNode = nodeInfo.featureSubset //该节点对应的特征集合</span><br><span class=\"line\">       //该样本在该树上的采样次数，如果为n表示5个同样的数据点同时用于更新对应的聚合信息</span><br><span class=\"line\">       val instanceWeight = baggedPoint.subsampleWeights(treeIndex) </span><br><span class=\"line\">       if (metadata.unorderedFeatures.isEmpty) &#123;</span><br><span class=\"line\">         //如果不存在无序特征，根据有序特征进行更新</span><br><span class=\"line\">         orderedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, instanceWeight, featuresForNode)</span><br><span class=\"line\">       &#125; else &#123; //都是有序特征</span><br><span class=\"line\">         mixedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, splits,</span><br><span class=\"line\">           metadata.unorderedFeatures, instanceWeight, featuresForNode)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       agg(aggNodeIndex).updateParent(baggedPoint.datum.label, instanceWeight)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   //计算当前数据被划分到的树的节点，并更新在对应节点的聚合信息。对于每个特征的相关bin,更新其聚合信息。</span><br><span class=\"line\">   def binSeqOp(</span><br><span class=\"line\">       agg: Array[DTStatsAggregator],//agg数组存储聚合信息，数据结构为（node, feature, bin）</span><br><span class=\"line\">       baggedPoint: BaggedPoint[TreePoint]): Array[DTStatsAggregator] = &#123;</span><br><span class=\"line\">     treeToNodeToIndexInfo.foreach &#123; case (treeIndex, nodeIndexToInfo) =&gt;</span><br><span class=\"line\">       //得到要更新的节点编号</span><br><span class=\"line\">       val nodeIndex = </span><br><span class=\"line\">         topNodesForGroup(treeIndex).predictImpl(baggedPoint.datum.binnedFeatures, splits)</span><br><span class=\"line\">       //对上步得到的节点，根据样本点更新其对应的bin的聚合信息</span><br><span class=\"line\">       nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     agg</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   /**</span><br><span class=\"line\">    * Do the same thing as binSeqOp, but with nodeIdCache.</span><br><span class=\"line\">    */</span><br><span class=\"line\">   def binSeqOpWithNodeIdCache(</span><br><span class=\"line\">       agg: Array[DTStatsAggregator],</span><br><span class=\"line\">       dataPoint: (BaggedPoint[TreePoint], Array[Int])): Array[DTStatsAggregator] = &#123;</span><br><span class=\"line\">     treeToNodeToIndexInfo.foreach &#123; case (treeIndex, nodeIndexToInfo) =&gt;</span><br><span class=\"line\">       val baggedPoint = dataPoint._1</span><br><span class=\"line\">       val nodeIdCache = dataPoint._2</span><br><span class=\"line\">       val nodeIndex = nodeIdCache(treeIndex)</span><br><span class=\"line\">       nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">     agg</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   </span><br><span class=\"line\">   //从treeToNodeToIndexInfo中获取每个节点对应的特征集合。key为节点在本组节点的编号，value为对应特征集合</span><br><span class=\"line\">   def getNodeToFeatures(</span><br><span class=\"line\">       treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]]): Option[Map[Int, Array[Int]]] = &#123;</span><br><span class=\"line\">     if (!metadata.subsamplingFeatures) &#123; //如果定义为不进行特征采样</span><br><span class=\"line\">       None</span><br><span class=\"line\">     &#125; else &#123;</span><br><span class=\"line\">       //定义为特征采样，从treeToNodeToIndexInfo中获取对应的节点编号和特征集合。</span><br><span class=\"line\">       val mutableNodeToFeatures = new mutable.HashMap[Int, Array[Int]]()</span><br><span class=\"line\">       treeToNodeToIndexInfo.values.foreach &#123; nodeIdToNodeInfo =&gt;</span><br><span class=\"line\">         nodeIdToNodeInfo.values.foreach &#123; nodeIndexInfo =&gt;</span><br><span class=\"line\">           assert(nodeIndexInfo.featureSubset.isDefined)</span><br><span class=\"line\">           mutableNodeToFeatures(nodeIndexInfo.nodeIndexInGroup) = nodeIndexInfo.featureSubset.get</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       Some(mutableNodeToFeatures.toMap)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   </span><br><span class=\"line\">   //用于训练的节点数组</span><br><span class=\"line\">   val nodes = new Array[LearningNode](numNodes)</span><br><span class=\"line\">   //根据nodesForGroup，在nodes中存储本轮迭代的节点，存储到nodes中</span><br><span class=\"line\">   nodesForGroup.foreach &#123; case (treeIndex, nodesForTree) =&gt;</span><br><span class=\"line\">     nodesForTree.foreach &#123; node =&gt;</span><br><span class=\"line\">       nodes(treeToNodeToIndexInfo(treeIndex)(node.id).nodeIndexInGroup) = node</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   //对于所有的节点，计算最佳特征及分割点</span><br><span class=\"line\">   timer.start(&quot;chooseSplits&quot;)</span><br><span class=\"line\">   //对于每个分区，迭代所有的样本，计算每个节点的聚合信息，</span><br><span class=\"line\">   //产出(nodeIndex, nodeAggregateStats)数据结构，</span><br><span class=\"line\">   //通过reduceByKey操作，一个节点的所有信息会被shuffle到同一个分区，通过合并信息，</span><br><span class=\"line\">   //计算每个节点的最佳分割，最后只有最佳的分割用于进一步构建决策树。</span><br><span class=\"line\">   val nodeToFeatures = getNodeToFeatures(treeToNodeToIndexInfo)//</span><br><span class=\"line\">   val nodeToFeaturesBc = input.sparkContext.broadcast(nodeToFeatures)</span><br><span class=\"line\"></span><br><span class=\"line\">   val partitionAggregates: RDD[(Int, DTStatsAggregator)] = if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">     input.zip(nodeIdCache.get.nodeIdsForInstances).mapPartitions &#123; points =&gt;</span><br><span class=\"line\">       // Construct a nodeStatsAggregators array to hold node aggregate stats,</span><br><span class=\"line\">       // each node will have a nodeStatsAggregator</span><br><span class=\"line\">       val nodeStatsAggregators = Array.tabulate(numNodes) &#123; nodeIndex =&gt;</span><br><span class=\"line\">         val featuresForNode = nodeToFeaturesBc.value.map &#123; nodeToFeatures =&gt;</span><br><span class=\"line\">           nodeToFeatures(nodeIndex)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         new DTStatsAggregator(metadata, featuresForNode)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       // iterator all instances in current partition and update aggregate stats</span><br><span class=\"line\">       points.foreach(binSeqOpWithNodeIdCache(nodeStatsAggregators, _))</span><br><span class=\"line\">       // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,</span><br><span class=\"line\">       // which can be combined with other partition using `reduceByKey`</span><br><span class=\"line\">       nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125; else &#123;</span><br><span class=\"line\">     input.mapPartitions &#123; points =&gt;</span><br><span class=\"line\">       // 在每个分区内，构建一个nodeStatsAggregators数组，其中每个元素对应一个node的DTStatsAggregator，该DTStatsAggregator包括了决策树元数据信息、以及该node对应的特征集合</span><br><span class=\"line\">       val nodeStatsAggregators = Array.tabulate(numNodes) &#123; nodeIndex =&gt;</span><br><span class=\"line\">         val featuresForNode = nodeToFeaturesBc.value.flatMap &#123; nodeToFeatures =&gt;</span><br><span class=\"line\">           Some(nodeToFeatures(nodeIndex))</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         new DTStatsAggregator(metadata, featuresForNode)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       //对当前分区，迭代所有样本，更新nodeStatsAggregators，即每个node对应的DTStatsAggregator</span><br><span class=\"line\">       points.foreach(binSeqOp(nodeStatsAggregators, _))</span><br><span class=\"line\">       //转化成(nodeIndex, nodeAggregateStats)格式，用于后续通过reduceByKey对多个分区的结果进行聚合。</span><br><span class=\"line\">       nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   //reduceByKey聚合多个partition的统计特征</span><br><span class=\"line\">   val nodeToBestSplits = partitionAggregates.reduceByKey((a, b) =&gt; a.merge(b)).map &#123;</span><br><span class=\"line\">     case (nodeIndex, aggStats) =&gt;</span><br><span class=\"line\">       //得到节点对应的特征集合</span><br><span class=\"line\">       val featuresForNode = nodeToFeaturesBc.value.flatMap &#123; nodeToFeatures =&gt;</span><br><span class=\"line\">         Some(nodeToFeatures(nodeIndex))</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">       // 找到最佳分裂特征和分裂位置，并返回度量的统计特征</span><br><span class=\"line\">       val (split: Split, stats: ImpurityStats) =</span><br><span class=\"line\">         binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))</span><br><span class=\"line\">       (nodeIndex, (split, stats))</span><br><span class=\"line\">   &#125;.collectAsMap()</span><br><span class=\"line\"></span><br><span class=\"line\">   timer.stop(&quot;chooseSplits&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   val nodeIdUpdaters = if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">     Array.fill[mutable.Map[Int, NodeIndexUpdater]](</span><br><span class=\"line\">       metadata.numTrees)(mutable.Map[Int, NodeIndexUpdater]())</span><br><span class=\"line\">   &#125; else &#123;</span><br><span class=\"line\">     null</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   // Iterate over all nodes in this group.</span><br><span class=\"line\">   //对于本组所有节点，更新节点本身信息，如果孩子节点是课分裂的叶子节点，则将其加入栈中</span><br><span class=\"line\">   nodesForGroup.foreach &#123; case (treeIndex, nodesForTree) =&gt;</span><br><span class=\"line\">     nodesForTree.foreach &#123; node =&gt;</span><br><span class=\"line\">       val nodeIndex = node.id //节点id</span><br><span class=\"line\">       val nodeInfo = treeToNodeToIndexInfo(treeIndex)(nodeIndex) //节点信息，包括节点在当前分组编号，节点特征等</span><br><span class=\"line\">       val aggNodeIndex = nodeInfo.nodeIndexInGroup //节点在当前分组编号</span><br><span class=\"line\">       //节点对应的最佳分裂，及最佳分裂对应的不纯度度量相关统计信息</span><br><span class=\"line\">       val (split: Split, stats: ImpurityStats) =</span><br><span class=\"line\">         nodeToBestSplits(aggNodeIndex) </span><br><span class=\"line\">       logDebug(&quot;best split = &quot; + split)</span><br><span class=\"line\"></span><br><span class=\"line\">       //如果信息增益小于0，或者层次达到上限，则将当前节点设置为叶子节点</span><br><span class=\"line\">       val isLeaf =</span><br><span class=\"line\">         (stats.gain &lt;= 0) || (LearningNode.indexToLevel(nodeIndex) == metadata.maxDepth)</span><br><span class=\"line\">       node.isLeaf = isLeaf</span><br><span class=\"line\">       node.stats = stats</span><br><span class=\"line\">       logDebug(&quot;Node = &quot; + node)</span><br><span class=\"line\">       </span><br><span class=\"line\">       //当前节点非叶子节点，创建子节点</span><br><span class=\"line\">       if (!isLeaf) &#123;</span><br><span class=\"line\">         node.split = Some(split) //设置节点split参数</span><br><span class=\"line\">         //子节点层数是否达到最大值</span><br><span class=\"line\">         val childIsLeaf = (LearningNode.indexToLevel(nodeIndex) + 1) == metadata.maxDepth</span><br><span class=\"line\">         //左孩子节点层数达到最大值，或者不纯度度量等于0，则左孩子节点为叶子节点</span><br><span class=\"line\">         val leftChildIsLeaf = childIsLeaf || (stats.leftImpurity == 0.0)</span><br><span class=\"line\">         //右孩子节点层数达到最大值，或者不纯度度量等于0，则右孩子节点为叶子节点          </span><br><span class=\"line\">         val rightChildIsLeaf = childIsLeaf || (stats.rightImpurity == 0.0)</span><br><span class=\"line\">         //创建左孩子节点，getEmptyImpurityStats(stats.leftImpurityCalculator)为左孩子的不纯度度量，只有impurity、impurityCalculator两个属性</span><br><span class=\"line\">         node.leftChild = Some(LearningNode(LearningNode.leftChildIndex(nodeIndex),</span><br><span class=\"line\">           leftChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.leftImpurityCalculator)))</span><br><span class=\"line\">         //创建右孩子节点</span><br><span class=\"line\">         node.rightChild = Some(LearningNode(LearningNode.rightChildIndex(nodeIndex),</span><br><span class=\"line\">           rightChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.rightImpurityCalculator)))</span><br><span class=\"line\"></span><br><span class=\"line\">         if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">           val nodeIndexUpdater = NodeIndexUpdater(</span><br><span class=\"line\">             split = split,</span><br><span class=\"line\">             nodeIndex = nodeIndex)</span><br><span class=\"line\">           nodeIdUpdaters(treeIndex).put(nodeIndex, nodeIndexUpdater)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         // enqueue left child and right child if they are not leaves</span><br><span class=\"line\">         //如果左孩子节点不是叶子节点，则将左孩子节点入栈</span><br><span class=\"line\">         if (!leftChildIsLeaf) &#123;</span><br><span class=\"line\">           nodeStack.push((treeIndex, node.leftChild.get))</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         if (!rightChildIsLeaf) &#123;</span><br><span class=\"line\">           //如果右孩子节点不是叶子节点，则将右孩子节点入栈</span><br><span class=\"line\">           nodeStack.push((treeIndex, node.rightChild.get))</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         logDebug(&quot;leftChildIndex = &quot; + node.leftChild.get.id +</span><br><span class=\"line\">           &quot;, impurity = &quot; + stats.leftImpurity)</span><br><span class=\"line\">         logDebug(&quot;rightChildIndex = &quot; + node.rightChild.get.id +</span><br><span class=\"line\">           &quot;, impurity = &quot; + stats.rightImpurity)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">     // Update the cache if needed.</span><br><span class=\"line\">     nodeIdCache.get.updateNodeIndices(input, nodeIdUpdaters, splits)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//得到当前数据点对应的node index输出,模仿对数据的预测过程，从根节点开始向下传播，</span><br><span class=\"line\">//直到一个叶子节点或者未进行分裂的节点终止，返回终止节点对应的索引。</span><br><span class=\"line\">def predictImpl(binnedFeatures: Array[Int], splits: Array[Array[Split]]): Int = &#123;</span><br><span class=\"line\">  if (this.isLeaf || this.split.isEmpty) &#123;</span><br><span class=\"line\">    this.id //如果当前节点是叶子节点或者未分裂的节点，返回当前节点索引</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    val split = this.split.get //当前节点的split</span><br><span class=\"line\">    val featureIndex = split.featureIndex //当前节点split对应的特征索引</span><br><span class=\"line\">    //根据数据点在featureIndex特征上的取值，以及featureIndex特征对应的分裂，判断当前数据点是否应该向左传递。</span><br><span class=\"line\">    val splitLeft = split.shouldGoLeft(binnedFeatures(featureIndex), splits(featureIndex)) </span><br><span class=\"line\">    if (this.leftChild.isEmpty) &#123; //如果左孩子为空</span><br><span class=\"line\">      // Not yet split. Return next layer of nodes to train</span><br><span class=\"line\">      if (splitLeft) &#123; //当前节点应该向左传递，得到左孩子节点索引值</span><br><span class=\"line\">        LearningNode.leftChildIndex(this.id)</span><br><span class=\"line\">      &#125; else &#123; //当前节点应该向右传递，得到右孩子节点索引值</span><br><span class=\"line\">        LearningNode.rightChildIndex(this.id)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123; //如果左孩子不为空，</span><br><span class=\"line\">      if (splitLeft) &#123; //当前节点应该向左传递，从左节点开始，递归计算最终节点的索引</span><br><span class=\"line\">        this.leftChild.get.predictImpl(binnedFeatures, splits)</span><br><span class=\"line\">      &#125; else &#123; //当前节点应该向右传递，从右节点开始，递归计算最终节点的索引</span><br><span class=\"line\">        this.rightChild.get.predictImpl(binnedFeatures, splits)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//对于排序类特征，根据数据点、权重，更新每个特征的每个bin信息        </span><br><span class=\"line\">private def orderedBinSeqOp(</span><br><span class=\"line\">      agg: DTStatsAggregator, //聚合信息，(feature, bin)</span><br><span class=\"line\">      treePoint: TreePoint,</span><br><span class=\"line\">      instanceWeight: Double,</span><br><span class=\"line\">      featuresForNode: Option[Array[Int]]): Unit = &#123;</span><br><span class=\"line\">    val label = treePoint.label</span><br><span class=\"line\"></span><br><span class=\"line\">    // 如果是采样特征</span><br><span class=\"line\">    if (featuresForNode.nonEmpty) &#123;</span><br><span class=\"line\">      // 使用采样的特征，对于每个特征的每个bin，进行更新</span><br><span class=\"line\">      var featureIndexIdx = 0</span><br><span class=\"line\">      while (featureIndexIdx &lt; featuresForNode.get.length) &#123;</span><br><span class=\"line\">        val binIndex = treePoint.binnedFeatures(featuresForNode.get.apply(featureIndexIdx))</span><br><span class=\"line\">        agg.update(featureIndexIdx, binIndex, label, instanceWeight)</span><br><span class=\"line\">        featureIndexIdx += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      // 如果是非采样特征，使用所有特征，对每个特征的每个bin，进行更新</span><br><span class=\"line\">      val numFeatures = agg.metadata.numFeatures</span><br><span class=\"line\">      var featureIndex = 0</span><br><span class=\"line\">      while (featureIndex &lt; numFeatures) &#123;</span><br><span class=\"line\">        val binIndex = treePoint.binnedFeatures(featureIndex)</span><br><span class=\"line\">        agg.update(featureIndex, binIndex, label, instanceWeight)</span><br><span class=\"line\">        featureIndex += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//相对于orderedBinSeqOp函数，mixedBinSeqOp函数在同时包括排序和非排序特征情况下，更新聚合信息.</span><br><span class=\"line\">//对于有序特征，对每个特征更新一个bin</span><br><span class=\"line\">//对于无序特征，类别的子集对应的bin需要消息，每个子集的靠左bin或者靠右bin需要更新</span><br><span class=\"line\">private def mixedBinSeqOp(</span><br><span class=\"line\">      agg: DTStatsAggregator, //聚合信息，(feature, bin)</span><br><span class=\"line\">      treePoint: TreePoint,</span><br><span class=\"line\">      splits: Array[Array[Split]],</span><br><span class=\"line\">      unorderedFeatures: Set[Int],</span><br><span class=\"line\">      instanceWeight: Double,</span><br><span class=\"line\">      featuresForNode: Option[Array[Int]]): Unit = &#123;</span><br><span class=\"line\">    val numFeaturesPerNode = if (featuresForNode.nonEmpty) &#123;</span><br><span class=\"line\">      // 如果特征需要采样，使用采样特征</span><br><span class=\"line\">      featuresForNode.get.length</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      // 否则使用所有特征</span><br><span class=\"line\">      agg.metadata.numFeatures</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 迭代每个特征，更新该节点对应的bin聚合信息.</span><br><span class=\"line\">    var featureIndexIdx = 0</span><br><span class=\"line\">    while (featureIndexIdx &lt; numFeaturesPerNode) &#123;</span><br><span class=\"line\">      //得到特征对应的原始索引值</span><br><span class=\"line\">      val featureIndex = if (featuresForNode.nonEmpty) &#123;</span><br><span class=\"line\">        featuresForNode.get.apply(featureIndexIdx)</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        featureIndexIdx</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (unorderedFeatures.contains(featureIndex)) &#123;</span><br><span class=\"line\">        //如果当前特征是无序特征</span><br><span class=\"line\">        val featureValue = treePoint.binnedFeatures(featureIndex) //得到bin features</span><br><span class=\"line\">        //得到当前特征偏移量</span><br><span class=\"line\">        val leftNodeFeatureOffset = agg.getFeatureOffset(featureIndexIdx)</span><br><span class=\"line\">        // Update the left or right bin for each split.</span><br><span class=\"line\">        //得到当前特征的split数量</span><br><span class=\"line\">        val numSplits = agg.metadata.numSplits(featureIndex)</span><br><span class=\"line\">        //得到当前特征分裂信息</span><br><span class=\"line\">        val featureSplits = splits(featureIndex)</span><br><span class=\"line\">        var splitIndex = 0</span><br><span class=\"line\">        while (splitIndex &lt; numSplits) &#123;</span><br><span class=\"line\">          //根据当前特征值，判断是否应该向左传递，如果向左传递，则将节点对当前特征的当前区间聚合信息进行更新</span><br><span class=\"line\">          if (featureSplits(splitIndex).shouldGoLeft(featureValue, featureSplits)) &#123;</span><br><span class=\"line\">            agg.featureUpdate(leftNodeFeatureOffset, splitIndex, treePoint.label, instanceWeight)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          splitIndex += 1</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        // 如果是有序特征，则直接更新对应特征的对应bin信息</span><br><span class=\"line\">        val binIndex = treePoint.binnedFeatures(featureIndex)</span><br><span class=\"line\">        agg.update(featureIndexIdx, binIndex, treePoint.label, instanceWeight)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      featureIndexIdx += 1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//寻找最佳分裂特征和分裂位置</span><br><span class=\"line\">private[tree] def binsToBestSplit(</span><br><span class=\"line\">      binAggregates: DTStatsAggregator, //所有feature的bin的统计信息</span><br><span class=\"line\">      splits: Array[Array[Split]],//所有feature的所有split</span><br><span class=\"line\">      featuresForNode: Option[Array[Int]],//node对应的feature子集</span><br><span class=\"line\">      //当前node</span><br><span class=\"line\">      node: LearningNode): (Split, ImpurityStats) = &#123; //返回值为最佳分裂，及对应的不纯度相关度量</span><br><span class=\"line\"></span><br><span class=\"line\">    // Calculate InformationGain and ImpurityStats if current node is top node</span><br><span class=\"line\">    // 当前节点对应的树的层次</span><br><span class=\"line\">    val level = LearningNode.indexToLevel(node.id)</span><br><span class=\"line\">    // 如果是根节点，不纯度度量为0</span><br><span class=\"line\">    var gainAndImpurityStats: ImpurityStats = if (level == 0) &#123;</span><br><span class=\"line\">      null</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //否则为当前节点对应的相关度量stats</span><br><span class=\"line\">      node.stats</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //获得合法的特征分裂</span><br><span class=\"line\">    val validFeatureSplits =</span><br><span class=\"line\">      Range(0, binAggregates.metadata.numFeaturesPerNode).view.map &#123; </span><br><span class=\"line\">      //得到原始特征对应的feature index</span><br><span class=\"line\">      featureIndexIdx =&gt;</span><br><span class=\"line\">        featuresForNode.map(features =&gt; (featureIndexIdx, features(featureIndexIdx)))</span><br><span class=\"line\">          .getOrElse((featureIndexIdx, featureIndexIdx))</span><br><span class=\"line\">      &#125;.withFilter &#123; case (_, featureIndex) =&gt; //过滤对应split数量为0的特征</span><br><span class=\"line\">        binAggregates.metadata.numSplits(featureIndex) != 0</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //对每个(feature,split), 计算增益，并选择增益最大的(feature,split)</span><br><span class=\"line\">    val (bestSplit, bestSplitStats) =</span><br><span class=\"line\">      validFeatureSplits.map &#123; case (featureIndexIdx, featureIndex) =&gt;</span><br><span class=\"line\">        //得到索引为featureIndex的特征对应的split数量</span><br><span class=\"line\">        val numSplits = binAggregates.metadata.numSplits(featureIndex)</span><br><span class=\"line\">        if (binAggregates.metadata.isContinuous(featureIndex)) &#123;</span><br><span class=\"line\">          //如果是连续特征</span><br><span class=\"line\">          //计算每个bin的累积统计信息（包括第一个bin到当前bin之间的所有bin对应的统计信息）</span><br><span class=\"line\">          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class=\"line\">          var splitIndex = 0</span><br><span class=\"line\">          while (splitIndex &lt; numSplits) &#123;</span><br><span class=\"line\">            binAggregates.mergeForFeature(nodeFeatureOffset, splitIndex + 1, splitIndex)</span><br><span class=\"line\">            splitIndex += 1</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          //找到最好的split</span><br><span class=\"line\">          val (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class=\"line\">            Range(0, numSplits).map &#123; case splitIdx =&gt;</span><br><span class=\"line\">              //得到当前split左孩子对应的统计信息</span><br><span class=\"line\">              val leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, splitIdx)</span><br><span class=\"line\">              //得到当前split右孩子对应的统计信息， 为得到右孩子对应的统计信息，需要所有的统计信息减去左孩子的统计信息</span><br><span class=\"line\">              val rightChildStats =</span><br><span class=\"line\">                binAggregates.getImpurityCalculator(nodeFeatureOffset, numSplits)</span><br><span class=\"line\">              //所有的统计信息减去左孩子的统计信息</span><br><span class=\"line\">              rightChildStats.subtract(leftChildStats)</span><br><span class=\"line\">              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,</span><br><span class=\"line\">                leftChildStats, rightChildStats, binAggregates.metadata)</span><br><span class=\"line\">              (splitIdx, gainAndImpurityStats)//分裂索引，不纯度度量信息</span><br><span class=\"line\">            &#125;.maxBy(_._2.gain)//取信息增益最大的分裂</span><br><span class=\"line\">          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)</span><br><span class=\"line\">        &#125; else if (binAggregates.metadata.isUnordered(featureIndex)) &#123;</span><br><span class=\"line\">          //无序离散特征</span><br><span class=\"line\">          val leftChildOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class=\"line\">          val (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class=\"line\">            Range(0, numSplits).map &#123; splitIndex =&gt;</span><br><span class=\"line\">              //得到左孩子聚合信息</span><br><span class=\"line\">              val leftChildStats = binAggregates.getImpurityCalculator(leftChildOffset, splitIndex)</span><br><span class=\"line\">              //得到右孩子聚合信息</span><br><span class=\"line\">              val rightChildStats = binAggregates.getParentImpurityCalculator()</span><br><span class=\"line\">                .subtract(leftChildStats)</span><br><span class=\"line\">              //计算不纯度度量相关统计信息</span><br><span class=\"line\">              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,</span><br><span class=\"line\">                leftChildStats, rightChildStats, binAggregates.metadata)</span><br><span class=\"line\">              (splitIndex, gainAndImpurityStats) //分裂索引，不纯度度量信息</span><br><span class=\"line\">            &#125;.maxBy(_._2.gain)//取信息增益最大的分裂</span><br><span class=\"line\">          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          // 对于排序离散特征</span><br><span class=\"line\">          //得到聚合信息的其实地址</span><br><span class=\"line\">          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class=\"line\">          //得到类别数量</span><br><span class=\"line\">          val numCategories = binAggregates.metadata.numBins(featureIndex)</span><br><span class=\"line\"></span><br><span class=\"line\">          //每个bin是一个特征值，根据质心对这些特征值排序，共K个特征值，对应生成K-1个划分</span><br><span class=\"line\">          val centroidForCategories = Range(0, numCategories).map &#123; case featureValue =&gt;</span><br><span class=\"line\">            //得到不纯度度量的统计信息</span><br><span class=\"line\">            val categoryStats =</span><br><span class=\"line\">              binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)</span><br><span class=\"line\">            val centroid = if (categoryStats.count != 0) &#123;//如果对应样本数量不为0，</span><br><span class=\"line\">              if (binAggregates.metadata.isMulticlass) &#123;</span><br><span class=\"line\">                //如果是多分类决策树，则将对应多标签的不纯度度量作为质心</span><br><span class=\"line\">                categoryStats.calculate()</span><br><span class=\"line\">              &#125; else if (binAggregates.metadata.isClassification) &#123;</span><br><span class=\"line\">                //如果是二分类问题，则将对应的正样本数量作为质心</span><br><span class=\"line\">                categoryStats.stats(1)</span><br><span class=\"line\">              &#125; else &#123;</span><br><span class=\"line\">                //如果是回归问题，则将对应的预测值作为质心</span><br><span class=\"line\">                categoryStats.predict</span><br><span class=\"line\">              &#125;</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">              Double.MaxValue //如果对应样本数量为0，则质心为Double.MaxValue</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            (featureValue, centroid) //返回每个特征值对应的样本质心</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">          logDebug(&quot;Centroids for categorical variable: &quot; + centroidForCategories.mkString(&quot;,&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">          // 根据质心，将特征对应的bin排序（即对应的离散特征值排序）</span><br><span class=\"line\">          val categoriesSortedByCentroid = centroidForCategories.toList.sortBy(_._2)</span><br><span class=\"line\"></span><br><span class=\"line\">          logDebug(&quot;Sorted centroids for categorical variable = &quot; +</span><br><span class=\"line\">            categoriesSortedByCentroid.mkString(&quot;,&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">          // 从左到右，依次计算每个category对应的从第一个category到当前categofy的统计信息聚合结果</span><br><span class=\"line\">          var splitIndex = 0</span><br><span class=\"line\">          while (splitIndex &lt; numSplits) &#123;</span><br><span class=\"line\">            val currentCategory = categoriesSortedByCentroid(splitIndex)._1</span><br><span class=\"line\">            val nextCategory = categoriesSortedByCentroid(splitIndex + 1)._1</span><br><span class=\"line\">            binAggregates.mergeForFeature(nodeFeatureOffset, nextCategory, currentCategory)</span><br><span class=\"line\">            splitIndex += 1</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          </span><br><span class=\"line\">          //所有特征值的聚合结果对应的category索引</span><br><span class=\"line\">          val lastCategory = categoriesSortedByCentroid.last._1</span><br><span class=\"line\">          //找到最佳的分裂</span><br><span class=\"line\">          val (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class=\"line\">            Range(0, numSplits).map &#123; splitIndex =&gt;</span><br><span class=\"line\">              //得到当前索引的特征值</span><br><span class=\"line\">              val featureValue = categoriesSortedByCentroid(splitIndex)._1</span><br><span class=\"line\">              //得到左孩子对应的聚合信息</span><br><span class=\"line\">              val leftChildStats =</span><br><span class=\"line\">                binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)</span><br><span class=\"line\">              //得到右孩子对应的聚合信息</span><br><span class=\"line\">              val rightChildStats =</span><br><span class=\"line\">                binAggregates.getImpurityCalculator(nodeFeatureOffset, lastCategory)</span><br><span class=\"line\">              rightChildStats.subtract(leftChildStats)</span><br><span class=\"line\">              //得到不纯度度量的相关统计信息</span><br><span class=\"line\">              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,</span><br><span class=\"line\">                leftChildStats, rightChildStats, binAggregates.metadata)</span><br><span class=\"line\">              (splitIndex, gainAndImpurityStats)</span><br><span class=\"line\">            &#125;.maxBy(_._2.gain)//根据信息增益进行排序，得到信息增益最大的split索引及增益</span><br><span class=\"line\">          </span><br><span class=\"line\">          //得到最佳分裂边界</span><br><span class=\"line\">          val categoriesForSplit =</span><br><span class=\"line\">            categoriesSortedByCentroid.map(_._1.toDouble).slice(0, bestFeatureSplitIndex + 1)</span><br><span class=\"line\">          //得到最佳分裂，包括特征索引、划分边界、类别数量等</span><br><span class=\"line\">          val bestFeatureSplit =</span><br><span class=\"line\">            new CategoricalSplit(featureIndex, categoriesForSplit.toArray, numCategories)</span><br><span class=\"line\">           //返回最佳分裂，及对应的增益统计信息</span><br><span class=\"line\">          (bestFeatureSplit, bestFeatureGainStats)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;.maxBy(_._2.gain)//针对所有特征，按照信息增益进行排序，取增益最大的特征</span><br><span class=\"line\"></span><br><span class=\"line\">    (bestSplit, bestSplitStats)//返回最佳分裂，及对应的增益统计信息</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">根据分裂对应的左孩子聚合信息，右孩子聚合信息，计算当前节点不纯度度量的相关统计信息</span><br><span class=\"line\">private def calculateImpurityStats(</span><br><span class=\"line\">      stats: ImpurityStats,</span><br><span class=\"line\">      leftImpurityCalculator: ImpurityCalculator,</span><br><span class=\"line\">      rightImpurityCalculator: ImpurityCalculator,</span><br><span class=\"line\">      metadata: DecisionTreeMetadata): ImpurityStats = &#123;</span><br><span class=\"line\">    //得到父节点的聚合信息</span><br><span class=\"line\">    val parentImpurityCalculator: ImpurityCalculator = if (stats == null) &#123;</span><br><span class=\"line\">      leftImpurityCalculator.copy.add(rightImpurityCalculator)</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      stats.impurityCalculator</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //得到父节点不纯度度量</span><br><span class=\"line\">    val impurity: Double = if (stats == null) &#123;</span><br><span class=\"line\">      parentImpurityCalculator.calculate()</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      stats.impurity</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">   </span><br><span class=\"line\">    val leftCount = leftImpurityCalculator.count //根据当前分裂得到的左孩子对应样本数量</span><br><span class=\"line\">    val rightCount = rightImpurityCalculator.count //根据当前分裂得到的右孩子对应样本数量</span><br><span class=\"line\"></span><br><span class=\"line\">    val totalCount = leftCount + rightCount  //当前分裂对应的总样本数量</span><br><span class=\"line\"></span><br><span class=\"line\">    // If left child or right child doesn&apos;t satisfy minimum instances per node,</span><br><span class=\"line\">    // then this split is invalid, return invalid information gain stats.</span><br><span class=\"line\">    //如果左孩子或者右孩子样本数量小于下限值，返回不合法的不纯度度量信息</span><br><span class=\"line\">    if ((leftCount &lt; metadata.minInstancesPerNode) ||</span><br><span class=\"line\">      (rightCount &lt; metadata.minInstancesPerNode)) &#123;</span><br><span class=\"line\">      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //左孩子对应的不纯度度量</span><br><span class=\"line\">    val leftImpurity = leftImpurityCalculator.calculate() // Note: This equals 0 if count = 0</span><br><span class=\"line\">    //右孩子对应的不纯度度量</span><br><span class=\"line\">    val rightImpurity = rightImpurityCalculator.calculate()</span><br><span class=\"line\">    //左孩子权重</span><br><span class=\"line\">    val leftWeight = leftCount / totalCount.toDouble</span><br><span class=\"line\">    //右孩子权重</span><br><span class=\"line\">    val rightWeight = rightCount / totalCount.toDouble</span><br><span class=\"line\">    //信息增益</span><br><span class=\"line\">    val gain = impurity - leftWeight * leftImpurity - rightWeight * rightImpurity</span><br><span class=\"line\">    //信息增益小于下限值，则返回不合法的不纯度度量信息</span><br><span class=\"line\">      if (gain &lt; metadata.minInfoGain) &#123;</span><br><span class=\"line\">      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //返回不纯度度量信息</span><br><span class=\"line\">    new ImpurityStats(gain, impurity, parentImpurityCalculator,</span><br><span class=\"line\">      leftImpurityCalculator, rightImpurityCalculator)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"模型预测\"><a href=\"#模型预测\" class=\"headerlink\" title=\"模型预测\"></a>模型预测</h2><p>通过模型训练生成决策树（随机森林）模型RandomForestModel，随机森林模型继承了树的组合模型TreeEnsembleModel，进一步通过predictBySumming函数，对传进的样本点进行预测。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//对样本点features进行预测</span><br><span class=\"line\">private def predictBySumming(features: Vector): Double = &#123;</span><br><span class=\"line\">  //对每棵决策树进行预测，然后自后结果为每个决策树结果的加权求和</span><br><span class=\"line\">  val treePredictions = trees.map(_.predict(features))</span><br><span class=\"line\">  blas.ddot(numTrees, treePredictions, 1, treeWeights, 1)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//DecisionTreeModel.predict方法</span><br><span class=\"line\">def predict(features: Vector): Double = &#123;</span><br><span class=\"line\">  //根据头部节点预测lable</span><br><span class=\"line\">  topNode.predict(features)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//Node. predict方法</span><br><span class=\"line\">def predict(features: Vector): Double = &#123;</span><br><span class=\"line\">  if (isLeaf) &#123;</span><br><span class=\"line\">    predict.predict //如果是叶子节点，直接输出</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    if (split.get.featureType == Continuous) &#123; </span><br><span class=\"line\">      //如果是连续特征，根据分裂阈值，决定走左孩子节点还是右孩子节点</span><br><span class=\"line\">      if (features(split.get.feature) &lt;= split.get.threshold) &#123;</span><br><span class=\"line\">        leftNode.get.predict(features)</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        rightNode.get.predict(features)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //如果是离散特征，根据特征是否被当前节点对应的特征集合包含，决定走左孩子节点还是右孩子节点</span><br><span class=\"line\">      if (split.get.categories.contains(features(split.get.feature))) &#123;</span><br><span class=\"line\">        leftNode.get.predict(features)</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        rightNode.get.predict(features)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>【1】<a href=\"http://spark.apache.org/mllib/\" target=\"_blank\" rel=\"noopener\">http://spark.apache.org/mllib/</a><br>【2】<a href=\"http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      displayMath: [ ['$$','$$']],\n      processEscapes: true\n    }\n  });\n</script>\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js\">\n</script>\n\n<p>该文章来自于2016年后半年整理的算法源码笔记，由于之前没有写博客的习惯，都直接以笔记的形式存在电脑上，分享起来非常不便，因此抽出时间，将其整理成博客的形式，和大家一起学习交流。</p>\n<h1 id=\"决策树算法简要介绍\"><a href=\"#决策树算法简要介绍\" class=\"headerlink\" title=\"决策树算法简要介绍\"></a>决策树算法简要介绍</h1><p>决策树算法是一种常见的分类算法，也可以用于回归问题。相对于其他分类算法，决策树的优点在于简单,可解释性强；对特征尺度不敏感，不需要做太多的特征预处理工作;能够自动挖掘特征之间的关联关系。缺点是比较容易过拟合（通过随机森林可以避免过拟合）</p>\n<p>决策树是一个树形结构，其中叶子节点表示分类（或回归）结果，非叶子节点是属性判断判断节点，每个属性判断节点都选择样本的一个特征，并根据该特征的取值决定选择哪一个分支路径。在对样本进行预测时，从根节点开始直到叶子节点，对于路径上的每个分支节点，都根据其对应的属性取值选择下一个分支节点，直到叶子节点。整个完整的路径，表示对样本的预测过程。如图1所示，表示一个女孩在决定是否决定去相亲的一个过程，最终选择去或者不去，对应分类的结果，中间的各种条件对应相关的属性。</p>\n<center><br><img src=\"/decision_tree/decision_tree_example.png\" alt=\"“决策树样例”\"><br></center><br><center>图1：决策树样例：对女孩决定是否参加相亲的问题进行决策树建模</center>\n\n\n<h2 id=\"决策树的训练\"><a href=\"#决策树的训练\" class=\"headerlink\" title=\"决策树的训练\"></a>决策树的训练</h2><p>从根节点开始，根据信息增益或其他条件，不断选择分裂的属性，直到生成叶子节点的过程。具体过程如下所示：</p>\n<ul>\n<li>对不同的属性，计算其信息增益，选择增益最大的特征对应根节点的最佳分裂。</li>\n<li>从根节点开始，对于不同的分支节点，分别选择信息增益最大的特征作为分支节点的最佳分裂。</li>\n<li>如果达到停止分裂的条件，则将该节点作为叶子节点：当前节点对应的样本都是一类样本，分类结果为对应的样本的类别；总样本数量小于一定值，或者树的高度达到最大值，或者信息增益小于一定值，或者已经用完所有的属性，选择占比最大的样本分类作为节点对应的分类结果。否则，根据步骤2进一步构造分裂节点。</li>\n</ul>\n<h2 id=\"属性度量\"><a href=\"#属性度量\" class=\"headerlink\" title=\"属性度量\"></a>属性度量</h2><p>决策树构建的关键，在于不断地选择最佳分裂属性。属性的收益度量方法，常见的有信息增益（ID3算法）、信息增益率（C4.5算法），基尼系数(CART算法)等。</p>\n<p><strong>ID3算法:</strong></p>\n<p>熵：信息论中，用于描述信息的不确定性，定义如式1，其中$D$表示对样本的一个划分，$m$表示划分的类别数量，$p_i$表示第i个类别的样本数量比例。</p>\n<p>$info(D)=-\\sum_{i=1}^m p_ilog_2(p_i)\\;\\;\\;（式1）$</p>\n<p>假设按照属性A对样本D进行划分，$v$为属性$A$的划分数量。则$A$对$D$划分的期望熵如式2：</p>\n<p>$info_A(D)=\\sum_{j=1}^v\\frac{|D_j|}{|D|}info(D_j)\\;\\;\\;（式2）$</p>\n<p>信心增益为上述原始熵和属性A对D划分后期望熵的差值，可以看做是加入信息A后，不确定性的减少程度。信息增益的定义如式3所示：</p>\n<p>$gain(A)=info(D)-info_A(D)\\;\\;\\;（式3）$</p>\n<p>ID3算法即在每次选择最佳分裂的属性时，根据信息增益进行选择。</p>\n<p><strong>C4.5算法:</strong><br>ID3算法容易使得选取值较多的属性。一种极端的情况是，对于ID类特征有很多的无意义的值的划分，ID3会选择该属性其作为最佳划分。C4.5算法通过采用信息增益率作为衡量特征有效性的指标，可以克服这个问题。</p>\n<p>首先定义分裂信息：<br>$splitInfo_A(D)=-\\sum_{j=1}^v\\frac{|D_j|}{|D|}log_2(\\frac{|D_j|}{|D|})\\;\\;\\;（式4）$</p>\n<p>信息增益率：<br>$gainRatio(A)=\\frac{gain(A)}{splitInfo_A(D)}\\;\\;\\;（式5）$</p>\n<p><strong>CART算法:</strong></p>\n<p>使用基尼系数作为不纯度的度量。<br>基尼系数:表示在样本集合中一个随机选中的样本被分错的概率，Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。当所有样本属于一个类别时，基尼系数最小为0。所有类别以等概率出现时，基尼系数最大。<br>$GINI(P)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^K p_k^2\\;\\;\\;（式6）$</p>\n<p>由于cart建立的树是个二叉树，所以K的取值为2。对于特征取值超过2的情况，以每个取值作为划分点，计算该划分下对应的基尼系数的期望。期望值最小的划分点，作为最佳分裂使用的特征划分。</p>\n<h1 id=\"spark-决策树源码分析\"><a href=\"#spark-决策树源码分析\" class=\"headerlink\" title=\"spark 决策树源码分析\"></a>spark 决策树源码分析</h1><p>为加深对ALS算法的理解，该部分主要分析spark mllib中决策树源码的实现。主要包括模型训练、模型预测2个部分</p>\n<h2 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h2><h3 id=\"决策树伴生类\"><a href=\"#决策树伴生类\" class=\"headerlink\" title=\"决策树伴生类\"></a>决策树伴生类</h3><p>DecisionTree伴随类，外部调用决策树模型进行训练的入口。通过外部传入数据和配置参数，调用DecisionTree中的run方法进行模型训练， 最终返回DecisionTreeModel类型对象。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">DecisionTree</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> <span class=\"keyword\">with</span> <span class=\"title\">Logging</span> </span>&#123;</span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span></span>(</span><br><span class=\"line\">      input: <span class=\"type\">RDD</span>[<span class=\"type\">LabeledPoint</span>], <span class=\"comment\">//训练数据，包括label和特征向量</span></span><br><span class=\"line\">      algo: <span class=\"type\">Algo</span>,<span class=\"comment\">//决策树类型，分类树or回归树</span></span><br><span class=\"line\">      impurity: <span class=\"type\">Impurity</span>,<span class=\"comment\">//衡量特征信息增益的标准，如信息增益、基尼、方差</span></span><br><span class=\"line\">      maxDepth: <span class=\"type\">Int</span>,<span class=\"comment\">//树的深度</span></span><br><span class=\"line\">      numClasses: <span class=\"type\">Int</span>,<span class=\"comment\">//待分类类别的数量</span></span><br><span class=\"line\">      maxBins: <span class=\"type\">Int</span>,<span class=\"comment\">//用于特征分裂的bin的最大数量</span></span><br><span class=\"line\">      quantileCalculationStrategy: <span class=\"type\">QuantileStrategy</span>,<span class=\"comment\">//计算分位数的算法</span></span><br><span class=\"line\">      <span class=\"comment\">//离散特征存储，如n-&gt;k表示第n个特征有k个取值（0，1，..., k-1）</span></span><br><span class=\"line\">      categoricalFeaturesInfo: <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Int</span>]): <span class=\"type\">DecisionTreeModel</span> = &#123; </span><br><span class=\"line\">    <span class=\"comment\">//根据参数信息，生成决策树配置</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> strategy = <span class=\"keyword\">new</span> <span class=\"type\">Strategy</span>(algo, impurity, maxDepth, numClasses, maxBins,</span><br><span class=\"line\">      quantileCalculationStrategy, categoricalFeaturesInfo)</span><br><span class=\"line\">    <span class=\"comment\">//调用DecisionTree对象的run方法，训练决策树模型</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">DecisionTree</span>(strategy).run(input)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   <span class=\"comment\">//训练分类决策树</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trainClassifier</span></span>(</span><br><span class=\"line\">      input: <span class=\"type\">RDD</span>[<span class=\"type\">LabeledPoint</span>],</span><br><span class=\"line\">      numClasses: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      categoricalFeaturesInfo: <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Int</span>],</span><br><span class=\"line\">      impurity: <span class=\"type\">String</span>,</span><br><span class=\"line\">      maxDepth: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      maxBins: <span class=\"type\">Int</span>): <span class=\"type\">DecisionTreeModel</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> impurityType = <span class=\"type\">Impurities</span>.fromString(impurity)</span><br><span class=\"line\">    train(input, <span class=\"type\">Classification</span>, impurityType, maxDepth, numClasses, maxBins, <span class=\"type\">Sort</span>,categoricalFeaturesInfo)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">    <span class=\"comment\">//训练回归决策树</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trainRegressor</span></span>(</span><br><span class=\"line\">      input: <span class=\"type\">RDD</span>[<span class=\"type\">LabeledPoint</span>],</span><br><span class=\"line\">      categoricalFeaturesInfo: <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Int</span>],</span><br><span class=\"line\">      impurity: <span class=\"type\">String</span>,</span><br><span class=\"line\">      maxDepth: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      maxBins: <span class=\"type\">Int</span>): <span class=\"type\">DecisionTreeModel</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> impurityType = <span class=\"type\">Impurities</span>.fromString(impurity) <span class=\"comment\">//基尼、熵、方差三种衡量标准</span></span><br><span class=\"line\">    train(input, <span class=\"type\">Regression</span>, impurityType, maxDepth, <span class=\"number\">0</span>, maxBins, <span class=\"type\">Sort</span>, categoricalFeaturesInfo)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"决策树类\"><a href=\"#决策树类\" class=\"headerlink\" title=\"决策树类\"></a>决策树类</h3><p>接受strategy参数初始化，并通过对run方法调用随机森林的run方法，通过设置特征集合为全集、树的个数为1，将随机森林训练后结果集中的第一棵树作为结果返回。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class DecisionTree private[spark] (private val strategy: Strategy, private val seed: Int)</span><br><span class=\"line\">  extends Serializable with Logging &#123;</span><br><span class=\"line\">  def run(input: RDD[LabeledPoint]): DecisionTreeModel = &#123;</span><br><span class=\"line\">    val rf = new RandomForest(strategy, numTrees = 1, featureSubsetStrategy = &quot;all&quot;, seed = seed)</span><br><span class=\"line\">    val rfModel = rf.run(input)</span><br><span class=\"line\">    rfModel.trees(0)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"RandomForest私有类run方法-通过run方法完成模型的训练\"><a href=\"#RandomForest私有类run方法-通过run方法完成模型的训练\" class=\"headerlink\" title=\"RandomForest私有类run方法,通过run方法完成模型的训练\"></a>RandomForest私有类run方法,通过run方法完成模型的训练</h3><p><strong>分布式训练思想：</strong></p>\n<ul>\n<li>分布式存储样本</li>\n<li>对于每次迭代，算法都会对一个node集合进行分裂。对于每个node，相关worker计算的的所有相关统计特征全部传递到某个worker进行汇总，并选择最好的特征分裂</li>\n<li>findSplitsBins方法可用于将连续特征离散化，在初始化阶段完成</li>\n<li>迭代算法<br>每次都作用于树的边缘节点，如果是随机森林，则选择所有的树的边缘节点。具体迭代步骤如下：<ol>\n<li>Master 节点: 从node queue中选取节点，如果训练的是随机森林,且featureSubsetStrategy取值不是all，则对于每个节点选择随机特征子集。selectNodesToSplit用于选择待分裂的节点。</li>\n<li>Worer节点: findBestSplits函数，对每个(tree, node, feature, split)，遍历所有本地所有样本计算相关特征，计算结果通过reduceByKey传递给某个节点，由该节点汇总数据，得到(feature, split)或者判断是否停止分裂</li>\n<li>Master节点: 收集所有节点分裂信息，更新model, 并将新的model传递给各个worker节点 </li>\n</ol>\n</li>\n</ul>\n<p>####<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def run(</span><br><span class=\"line\">      input: RDD[LabeledPoint],</span><br><span class=\"line\">      strategy: OldStrategy,</span><br><span class=\"line\">      numTrees: Int,</span><br><span class=\"line\">      featureSubsetStrategy: String,</span><br><span class=\"line\">      seed: Long,</span><br><span class=\"line\">      instr: Option[Instrumentation[_]],</span><br><span class=\"line\">      parentUID: Option[String] = None): Array[DecisionTreeModel] = &#123;</span><br><span class=\"line\">    val timer = new TimeTracker()</span><br><span class=\"line\">    timer.start(&quot;total&quot;)</span><br><span class=\"line\">    timer.start(&quot;init&quot;)</span><br><span class=\"line\">    </span><br><span class=\"line\">    val retaggedInput = input.retag(classOf[LabeledPoint])</span><br><span class=\"line\">    //构建元数据</span><br><span class=\"line\">    val metadata =</span><br><span class=\"line\">      DecisionTreeMetadata.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)</span><br><span class=\"line\">    instr match &#123;</span><br><span class=\"line\">      case Some(instrumentation) =&gt;</span><br><span class=\"line\">        instrumentation.logNumFeatures(metadata.numFeatures)</span><br><span class=\"line\">        instrumentation.logNumClasses(metadata.numClasses)</span><br><span class=\"line\">      case None =&gt;</span><br><span class=\"line\">        logInfo(&quot;numFeatures: &quot; + metadata.numFeatures)</span><br><span class=\"line\">        logInfo(&quot;numClasses: &quot; + metadata.numClasses)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    //每个特征对应的splits和bins</span><br><span class=\"line\">    timer.start(&quot;findSplits&quot;)</span><br><span class=\"line\">    val splits = findSplits(retaggedInput, metadata, seed)</span><br><span class=\"line\">    timer.stop(&quot;findSplits&quot;)</span><br><span class=\"line\">    logDebug(&quot;numBins: feature: number of bins&quot;)</span><br><span class=\"line\">    logDebug(Range(0, metadata.numFeatures).map &#123; featureIndex =&gt;</span><br><span class=\"line\">      s&quot;\\t$featureIndex\\t$&#123;metadata.numBins(featureIndex)&#125;&quot;</span><br><span class=\"line\">    &#125;.mkString(&quot;\\n&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">    // Bin feature values (TreePoint representation).</span><br><span class=\"line\">    // Cache input RDD for speedup during multiple passes.</span><br><span class=\"line\">    //输入</span><br><span class=\"line\">    val treeInput = TreePoint.convertToTreeRDD(retaggedInput, splits, metadata)</span><br><span class=\"line\"></span><br><span class=\"line\">    val withReplacement = numTrees &gt; 1</span><br><span class=\"line\"></span><br><span class=\"line\">    val baggedInput = BaggedPoint</span><br><span class=\"line\">      .convertToBaggedRDD(treeInput, strategy.subsamplingRate, numTrees, withReplacement, seed)</span><br><span class=\"line\">      .persist(StorageLevel.MEMORY_AND_DISK)</span><br><span class=\"line\"></span><br><span class=\"line\">    // depth of the decision tree</span><br><span class=\"line\">    val maxDepth = strategy.maxDepth</span><br><span class=\"line\">    require(maxDepth &lt;= 30,</span><br><span class=\"line\">      s&quot;DecisionTree currently only supports maxDepth &lt;= 30, but was given maxDepth = $maxDepth.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // Max memory usage for aggregates</span><br><span class=\"line\">    // TODO: Calculate memory usage more precisely.</span><br><span class=\"line\">    val maxMemoryUsage: Long = strategy.maxMemoryInMB * 1024L * 1024L</span><br><span class=\"line\">    logDebug(&quot;max memory usage for aggregates = &quot; + maxMemoryUsage + &quot; bytes.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    /*</span><br><span class=\"line\">     * The main idea here is to perform group-wise training of the decision tree nodes thus</span><br><span class=\"line\">     * reducing the passes over the data from (# nodes) to (# nodes / maxNumberOfNodesPerGroup).</span><br><span class=\"line\">     * Each data sample is handled by a particular node (or it reaches a leaf and is not used</span><br><span class=\"line\">     * in lower levels).</span><br><span class=\"line\">     */</span><br><span class=\"line\"></span><br><span class=\"line\">    // Create an RDD of node Id cache.</span><br><span class=\"line\">    // At first, all the rows belong to the root nodes (node Id == 1).</span><br><span class=\"line\">    val nodeIdCache = if (strategy.useNodeIdCache) &#123;</span><br><span class=\"line\">      Some(NodeIdCache.init(</span><br><span class=\"line\">        data = baggedInput,</span><br><span class=\"line\">        numTrees = numTrees,</span><br><span class=\"line\">        checkpointInterval = strategy.checkpointInterval,</span><br><span class=\"line\">        initVal = 1))</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      None</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    /*</span><br><span class=\"line\">      Stack of nodes to train: (treeIndex, node)</span><br><span class=\"line\">      The reason this is a stack is that we train many trees at once, but we want to focus on</span><br><span class=\"line\">      completing trees, rather than training all simultaneously.  If we are splitting nodes from</span><br><span class=\"line\">      1 tree, then the new nodes to split will be put at the top of this stack, so we will continue</span><br><span class=\"line\">      training the same tree in the next iteration.  This focus allows us to send fewer trees to</span><br><span class=\"line\">      workers on each iteration; see topNodesForGroup below.</span><br><span class=\"line\">     */</span><br><span class=\"line\">    val nodeStack = new mutable.Stack[(Int, LearningNode)]</span><br><span class=\"line\"></span><br><span class=\"line\">    val rng = new Random()</span><br><span class=\"line\">    rng.setSeed(seed)</span><br><span class=\"line\"></span><br><span class=\"line\">    // Allocate and queue root nodes.</span><br><span class=\"line\">    val topNodes = Array.fill[LearningNode](numTrees)(LearningNode.emptyNode(nodeIndex = 1))</span><br><span class=\"line\">    Range(0, numTrees).foreach(treeIndex =&gt; nodeStack.push((treeIndex, topNodes(treeIndex))))</span><br><span class=\"line\"></span><br><span class=\"line\">    timer.stop(&quot;init&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    while (nodeStack.nonEmpty) &#123;</span><br><span class=\"line\">      // Collect some nodes to split, and choose features for each node (if subsampling).</span><br><span class=\"line\">      // Each group of nodes may come from one or multiple trees, and at multiple levels.</span><br><span class=\"line\">      val (nodesForGroup, treeToNodeToIndexInfo) =</span><br><span class=\"line\">        RandomForest.selectNodesToSplit(nodeStack, maxMemoryUsage, metadata, rng)</span><br><span class=\"line\">      // Sanity check (should never occur):</span><br><span class=\"line\">      assert(nodesForGroup.nonEmpty,</span><br><span class=\"line\">        s&quot;RandomForest selected empty nodesForGroup.  Error for unknown reason.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">      // Only send trees to worker if they contain nodes being split this iteration.</span><br><span class=\"line\">      val topNodesForGroup: Map[Int, LearningNode] =</span><br><span class=\"line\">        nodesForGroup.keys.map(treeIdx =&gt; treeIdx -&gt; topNodes(treeIdx)).toMap</span><br><span class=\"line\"></span><br><span class=\"line\">      // Choose node splits, and enqueue new nodes as needed.</span><br><span class=\"line\">      timer.start(&quot;findBestSplits&quot;)</span><br><span class=\"line\">      RandomForest.findBestSplits(baggedInput, metadata, topNodesForGroup, nodesForGroup,</span><br><span class=\"line\">        treeToNodeToIndexInfo, splits, nodeStack, timer, nodeIdCache)</span><br><span class=\"line\">      timer.stop(&quot;findBestSplits&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    baggedInput.unpersist()</span><br><span class=\"line\"></span><br><span class=\"line\">    timer.stop(&quot;total&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    logInfo(&quot;Internal timing for DecisionTree:&quot;)</span><br><span class=\"line\">    logInfo(s&quot;$timer&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    // Delete any remaining checkpoints used for node Id cache.</span><br><span class=\"line\">    if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">        nodeIdCache.get.deleteAllCheckpoints()</span><br><span class=\"line\">      &#125; catch &#123;</span><br><span class=\"line\">        case e: IOException =&gt;</span><br><span class=\"line\">          logWarning(s&quot;delete all checkpoints failed. Error reason: $&#123;e.getMessage&#125;&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    val numFeatures = metadata.numFeatures</span><br><span class=\"line\"></span><br><span class=\"line\">    parentUID match &#123;</span><br><span class=\"line\">      case Some(uid) =&gt;</span><br><span class=\"line\">        if (strategy.algo == OldAlgo.Classification) &#123;</span><br><span class=\"line\">          topNodes.map &#123; rootNode =&gt;</span><br><span class=\"line\">            new DecisionTreeClassificationModel(uid, rootNode.toNode, numFeatures,</span><br><span class=\"line\">              strategy.getNumClasses)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          topNodes.map &#123; rootNode =&gt;</span><br><span class=\"line\">            new DecisionTreeRegressionModel(uid, rootNode.toNode, numFeatures)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      case None =&gt;</span><br><span class=\"line\">        if (strategy.algo == OldAlgo.Classification) &#123;</span><br><span class=\"line\">          topNodes.map &#123; rootNode =&gt;</span><br><span class=\"line\">            new DecisionTreeClassificationModel(rootNode.toNode, numFeatures,</span><br><span class=\"line\">              strategy.getNumClasses)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          topNodes.map(rootNode =&gt; new DecisionTreeRegressionModel(rootNode.toNode, numFeatures))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"buildMetadata\"><a href=\"#buildMetadata\" class=\"headerlink\" title=\"buildMetadata\"></a>buildMetadata</h4><p>决策树训练的元数据构造。主要用于计算每个特征的bin数量，以及无序类特征集合, 每个节点使用的特征数量等。其中决策树一般使用所有特征、随机森林分类采用$sqrt(n)$个特征，随机森林回归采用$\\frac{n}{3}$个特征</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def buildMetadata(</span><br><span class=\"line\">      input: RDD[LabeledPoint],</span><br><span class=\"line\">      strategy: Strategy,</span><br><span class=\"line\">      numTrees: Int,</span><br><span class=\"line\">      featureSubsetStrategy: String): DecisionTreeMetadata = &#123;</span><br><span class=\"line\">    //特征数量</span><br><span class=\"line\">    val numFeatures = input.map(_.features.size).take(1).headOption.getOrElse &#123;</span><br><span class=\"line\">      throw new IllegalArgumentException(s&quot;DecisionTree requires size of input RDD &gt; 0, &quot; +</span><br><span class=\"line\">        s&quot;but was given by empty one.&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    val numExamples = input.count() //样本数量</span><br><span class=\"line\">    val numClasses = strategy.algo match &#123;</span><br><span class=\"line\">      case Classification =&gt; strategy.numClasses</span><br><span class=\"line\">      case Regression =&gt; 0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //最大划分数量 </span><br><span class=\"line\">    val maxPossibleBins = math.min(strategy.maxBins, numExamples).toInt</span><br><span class=\"line\">    if (maxPossibleBins &lt; strategy.maxBins) &#123;</span><br><span class=\"line\">      logWarning(s&quot;DecisionTree reducing maxBins from $&#123;strategy.maxBins&#125; to $maxPossibleBins&quot; +</span><br><span class=\"line\">        s&quot; (= number of training instances)&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //maxPossibleBins可能被numExamples修改过，导致小于刚开始设置的strategy.maxBins。</span><br><span class=\"line\">    //需要进一步确保离散值的特征取值数量小于maxPossibleBins，</span><br><span class=\"line\">    if (strategy.categoricalFeaturesInfo.nonEmpty) &#123;</span><br><span class=\"line\">      val maxCategoriesPerFeature = strategy.categoricalFeaturesInfo.values.max</span><br><span class=\"line\">      val maxCategory =</span><br><span class=\"line\">        strategy.categoricalFeaturesInfo.find(_._2 == maxCategoriesPerFeature).get._1</span><br><span class=\"line\">      require(maxCategoriesPerFeature &lt;= maxPossibleBins,</span><br><span class=\"line\">        s&quot;DecisionTree requires maxBins (= $maxPossibleBins) to be at least as large as the &quot; +</span><br><span class=\"line\">        s&quot;number of values in each categorical feature, but categorical feature $maxCategory &quot; +</span><br><span class=\"line\">        s&quot;has $maxCategoriesPerFeature values. Considering remove this and other categorical &quot; +</span><br><span class=\"line\">        &quot;features with a large number of values, or add more training examples.&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //存储每个无序特征的索引</span><br><span class=\"line\">    val unorderedFeatures = new mutable.HashSet[Int]()</span><br><span class=\"line\">    //存储每个无序特征的bin数量</span><br><span class=\"line\">    val numBins = Array.fill[Int](numFeatures)(maxPossibleBins)</span><br><span class=\"line\">    if (numClasses &gt; 2) &#123; //多分类问题</span><br><span class=\"line\">      //根据maxPossibleBins，计算每个无序特征对应的最大类别数量</span><br><span class=\"line\">      val maxCategoriesForUnorderedFeature =</span><br><span class=\"line\">        ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt</span><br><span class=\"line\">      strategy.categoricalFeaturesInfo.foreach &#123; case (featureIndex, numCategories) =&gt;</span><br><span class=\"line\">        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤</span><br><span class=\"line\">          if (numCategories &gt; 1) &#123;</span><br><span class=\"line\">          //判断离散特征是否可当做无序特征，需要保证</span><br><span class=\"line\">          //bins的数量需要小于2 * ((1 &lt;&lt; numCategories - 1) - 1)）</span><br><span class=\"line\">          if (numCategories &lt;= maxCategoriesForUnorderedFeature) &#123;</span><br><span class=\"line\">            unorderedFeatures.add(featureIndex)</span><br><span class=\"line\">            //有numCategories个取值的的特征，对应bins数量为(1 &lt;&lt; numCategories - 1) - 1</span><br><span class=\"line\">            //此处刚开始有点疑惑，感觉应该是2 *（(1 &lt;&lt; numCategories - 1) - 1）</span><br><span class=\"line\">            //通过DecisionTreeMetadata中numSplits函数发现，此处的bin数量和split数量有一定对应关系，(featureIndex)</span><br><span class=\"line\">           //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1</span><br><span class=\"line\">            numBins(featureIndex) = numUnorderedBins(numCategories)</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">            //对于其他离散特征，numBins数量为特征可能的取值数量</span><br><span class=\"line\">            numBins(featureIndex) = numCategories</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123; //对于二值分类或回归问题</span><br><span class=\"line\">      strategy.categoricalFeaturesInfo.foreach &#123; case (featureIndex, numCategories) =&gt;</span><br><span class=\"line\">        //如果特征只有1个取值，则当做连续特征看待，此处对其进行过滤</span><br><span class=\"line\">        if (numCategories &gt; 1) &#123;</span><br><span class=\"line\">          //numBins数量为特征可能的取值数量</span><br><span class=\"line\">          numBins(featureIndex) = numCategories </span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //设置每个分支节点对应的特征数量</span><br><span class=\"line\">    val _featureSubsetStrategy = featureSubsetStrategy match &#123;</span><br><span class=\"line\">      case &quot;auto&quot; =&gt;</span><br><span class=\"line\">        if (numTrees == 1) &#123; //如果是树，使用所有特征n</span><br><span class=\"line\">          &quot;all&quot;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          if (strategy.algo == Classification) &#123; //如果是用于分类的随机森林，使用sqrt(n)个特征</span><br><span class=\"line\">            &quot;sqrt&quot;</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">            &quot;onethird&quot;  //如果是用于回归的随机森林，使用n/3个特征</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      case _ =&gt; featureSubsetStrategy</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    val numFeaturesPerNode: Int = _featureSubsetStrategy match &#123;</span><br><span class=\"line\">      case &quot;all&quot; =&gt; numFeatures</span><br><span class=\"line\">      case &quot;sqrt&quot; =&gt; math.sqrt(numFeatures).ceil.toInt</span><br><span class=\"line\">      case &quot;log2&quot; =&gt; math.max(1, (math.log(numFeatures) / math.log(2)).ceil.toInt)</span><br><span class=\"line\">      case &quot;onethird&quot; =&gt; (numFeatures / 3.0).ceil.toInt</span><br><span class=\"line\">      case _ =&gt;</span><br><span class=\"line\">        Try(_featureSubsetStrategy.toInt).filter(_ &gt; 0).toOption match &#123;</span><br><span class=\"line\">          case Some(value) =&gt; math.min(value, numFeatures)</span><br><span class=\"line\">          case None =&gt;</span><br><span class=\"line\">            Try(_featureSubsetStrategy.toDouble).filter(_ &gt; 0).filter(_ &lt;= 1.0).toOption match &#123;</span><br><span class=\"line\">              case Some(value) =&gt; math.ceil(value * numFeatures).toInt</span><br><span class=\"line\">              case _ =&gt; throw new IllegalArgumentException(s&quot;Supported values:&quot; +</span><br><span class=\"line\">                s&quot; $&#123;RandomForestParams.supportedFeatureSubsetStrategies.mkString(&quot;, &quot;)&#125;,&quot; +</span><br><span class=\"line\">                s&quot; (0.0-1.0], [1-n].&quot;)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    new DecisionTreeMetadata(numFeatures, numExamples, numClasses, numBins.max,</span><br><span class=\"line\">      strategy.categoricalFeaturesInfo, unorderedFeatures.toSet, numBins,</span><br><span class=\"line\">      strategy.impurity, strategy.quantileCalculationStrategy, strategy.maxDepth,</span><br><span class=\"line\">      strategy.minInstancesPerNode, strategy.minInfoGain, numTrees, numFeaturesPerNode)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"DecisionTreeMetadata类\"><a href=\"#DecisionTreeMetadata类\" class=\"headerlink\" title=\"DecisionTreeMetadata类\"></a>DecisionTreeMetadata类</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private[spark] class DecisionTreeMetadata(</span><br><span class=\"line\">    val numFeatures: Int,</span><br><span class=\"line\">    val numExamples: Long,</span><br><span class=\"line\">    val numClasses: Int,</span><br><span class=\"line\">    val maxBins: Int,</span><br><span class=\"line\">    val featureArity: Map[Int, Int],</span><br><span class=\"line\">    val unorderedFeatures: Set[Int],</span><br><span class=\"line\">    val numBins: Array[Int],</span><br><span class=\"line\">    val impurity: Impurity,</span><br><span class=\"line\">    val quantileStrategy: QuantileStrategy,</span><br><span class=\"line\">    val maxDepth: Int,</span><br><span class=\"line\">    val minInstancesPerNode: Int,</span><br><span class=\"line\">    val minInfoGain: Double,</span><br><span class=\"line\">    val numTrees: Int,</span><br><span class=\"line\">    val numFeaturesPerNode: Int) extends Serializable &#123;</span><br><span class=\"line\">  //判断是否为无序特征</span><br><span class=\"line\">  def isUnordered(featureIndex: Int): Boolean = unorderedFeatures.contains(featureIndex)</span><br><span class=\"line\">  //判断是否用于分类的决策树（随机森林）</span><br><span class=\"line\">  def isClassification: Boolean = numClasses &gt;= 2</span><br><span class=\"line\">  //判断是否用于多分类的决策树（随机森林）</span><br><span class=\"line\">  def isMulticlass: Boolean = numClasses &gt; 2</span><br><span class=\"line\">  //判断是否拥有离散特征的多分类决策树（随机森林）</span><br><span class=\"line\">  def isMulticlassWithCategoricalFeatures: Boolean = isMulticlass &amp;&amp; (featureArity.size &gt; 0)</span><br><span class=\"line\">  //判断是否离散特征</span><br><span class=\"line\">  def isCategorical(featureIndex: Int): Boolean = featureArity.contains(featureIndex)</span><br><span class=\"line\"> //判断是否连续特征</span><br><span class=\"line\">  def isContinuous(featureIndex: Int): Boolean = !featureArity.contains(featureIndex)</span><br><span class=\"line\">  //判断划分的数量，对于无序特征, 划分数量为bin的数量；对于有序特征，为bin数量-1</span><br><span class=\"line\">  def numSplits(featureIndex: Int): Int = if (isUnordered(featureIndex)) &#123;</span><br><span class=\"line\">    numBins(featureIndex)</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    numBins(featureIndex) - 1</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  //对于连续特征，根据划分数量设置bin数量为划分数量加1</span><br><span class=\"line\">  def setNumSplits(featureIndex: Int, numSplits: Int) &#123;</span><br><span class=\"line\">    require(isContinuous(featureIndex),</span><br><span class=\"line\">      s&quot;Only number of bin for a continuous feature can be set.&quot;)</span><br><span class=\"line\">    numBins(featureIndex) = numSplits + 1</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  //判断是否需要对特征进行采样</span><br><span class=\"line\">  def subsamplingFeatures: Boolean = numFeatures != numFeaturesPerNode</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"findSplits\"><a href=\"#findSplits\" class=\"headerlink\" title=\"findSplits\"></a>findSplits</h4><p>通过使用采样的样本，寻找样本的划分splits和划分后的bins。</p>\n<p><strong>划分的思想：</strong>对连续特征和离散特征，分别采用不同处理方式。对于每个连续特征，numBins - 1个splits, 代表每个树的节点的所有可能的二值化分；对于每个离散特征，无序离散特征（用于多分类的维度较大的feature）基于特征的子集进行划分。有序类特征（用于回归、二分类、多分类的维度较小的feature)的每个取值对应一个bin.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected[tree] def findSplits(</span><br><span class=\"line\">      input: RDD[LabeledPoint],</span><br><span class=\"line\">      metadata: DecisionTreeMetadata,</span><br><span class=\"line\">      seed: Long): Array[Array[Split]] = &#123;</span><br><span class=\"line\">    logDebug(&quot;isMulticlass = &quot; + metadata.isMulticlass)</span><br><span class=\"line\">    val numFeatures = metadata.numFeatures //特征的数量</span><br><span class=\"line\">    // 得到所有连续特征索引</span><br><span class=\"line\">    val continuousFeatures = Range(0, numFeatures).filter(metadata.isContinuous)</span><br><span class=\"line\">    //当有连续特征的时候需要采样样本   </span><br><span class=\"line\">    val sampledInput = if (continuousFeatures.nonEmpty) &#123;</span><br><span class=\"line\">      // 计算近似分位数计算需要的样本数</span><br><span class=\"line\">      val requiredSamples = math.max(metadata.maxBins * metadata.maxBins, 10000)</span><br><span class=\"line\">      // 计算需要的样本占总样本比例</span><br><span class=\"line\">      val fraction = if (requiredSamples &lt; metadata.numExamples) &#123;</span><br><span class=\"line\">        requiredSamples.toDouble / metadata.numExamples</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        1.0</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      logDebug(&quot;fraction of data used for calculating quantiles = &quot; + fraction)</span><br><span class=\"line\">      input.sample(withReplacement = false, fraction, new XORShiftRandom(seed).nextInt())</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      input.sparkContext.emptyRDD[LabeledPoint]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //对每个连续特征和非有序类离散特征，通过排序的方式，寻找最佳的splits点</span><br><span class=\"line\">    findSplitsBySorting(sampledInput, metadata, continuousFeatures)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//对每个特征，通过排序的方式，寻找最佳的splits点</span><br><span class=\"line\">private def findSplitsBySorting(</span><br><span class=\"line\">     input: RDD[LabeledPoint],</span><br><span class=\"line\">     metadata: DecisionTreeMetadata,</span><br><span class=\"line\">     continuousFeatures: IndexedSeq[Int]): Array[Array[Split]] = &#123;</span><br><span class=\"line\">  </span><br><span class=\"line\">   //寻找连续特征的划分阈值</span><br><span class=\"line\">   val continuousSplits: scala.collection.Map[Int, Array[Split]] = &#123;</span><br><span class=\"line\">     //设置分区数量，如果连续特征的数量小于原始分区数，则进一步减少分区，防止无效的启动的task任务。</span><br><span class=\"line\">     val numPartitions = math.min(continuousFeatures.length, input.partitions.length)</span><br><span class=\"line\"></span><br><span class=\"line\">     input</span><br><span class=\"line\">       .flatMap(point =&gt; continuousFeatures.map(idx =&gt; (idx, point.features(idx))))</span><br><span class=\"line\">       .groupByKey(numPartitions)</span><br><span class=\"line\">       .map &#123; case (idx, samples) =&gt;</span><br><span class=\"line\">         val thresholds = findSplitsForContinuousFeature(samples, metadata, idx)</span><br><span class=\"line\">         val splits: Array[Split] = thresholds.map(thresh =&gt; new ContinuousSplit(idx, thresh))</span><br><span class=\"line\">         logDebug(s&quot;featureIndex = $idx, numSplits = $&#123;splits.length&#125;&quot;)</span><br><span class=\"line\">         (idx, splits)</span><br><span class=\"line\">       &#125;.collectAsMap()</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   //特征数量</span><br><span class=\"line\">   val numFeatures = metadata.numFeatures</span><br><span class=\"line\">   //汇总所有特征的split(不包括无序离散特征)</span><br><span class=\"line\">   val splits: Array[Array[Split]] = Array.tabulate(numFeatures) &#123;</span><br><span class=\"line\">     //如果是连续特征，返回该连续特征的split</span><br><span class=\"line\">     case i if metadata.isContinuous(i) =&gt;</span><br><span class=\"line\">       val split = continuousSplits(i)</span><br><span class=\"line\">       metadata.setNumSplits(i, split.length)</span><br><span class=\"line\">       split</span><br><span class=\"line\">     //如果是无序离散特征，则提取该特征的split， 具体是对于每个离散特征，其第k个split为其k对应二进制的所有位置为1的数值。</span><br><span class=\"line\">     case i if metadata.isCategorical(i) &amp;&amp; metadata.isUnordered(i) =&gt;</span><br><span class=\"line\">       // Unordered features</span><br><span class=\"line\">       // 2^(maxFeatureValue - 1) - 1 combinations</span><br><span class=\"line\">       //特征的取值数量</span><br><span class=\"line\">       val featureArity = metadata.featureArity(i)</span><br><span class=\"line\">       Array.tabulate[Split](metadata.numSplits(i)) &#123; splitIndex =&gt;</span><br><span class=\"line\">         val categories = extractMultiClassCategories(splitIndex + 1, featureArity)</span><br><span class=\"line\">         new CategoricalSplit(i, categories.toArray, featureArity)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     //对于有序离散特征，暂时不求解split, 在训练阶段求解</span><br><span class=\"line\">     case i if metadata.isCategorical(i) =&gt;</span><br><span class=\"line\">       // Ordered features</span><br><span class=\"line\">       //   Splits are constructed as needed during training.</span><br><span class=\"line\">       Array.empty[Split]</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   splits</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//将input这个数对应的二进制位置为1的位置加入到当前划分</span><br><span class=\"line\">private[tree] def extractMultiClassCategories(</span><br><span class=\"line\">      input: Int,</span><br><span class=\"line\">      maxFeatureValue: Int): List[Double] = &#123;</span><br><span class=\"line\">    var categories = List[Double]()</span><br><span class=\"line\">    var j = 0</span><br><span class=\"line\">    var bitShiftedInput = input</span><br><span class=\"line\">    while (j &lt; maxFeatureValue) &#123;</span><br><span class=\"line\">      if (bitShiftedInput % 2 != 0) &#123;</span><br><span class=\"line\">        // updating the list of categories.</span><br><span class=\"line\">        categories = j.toDouble :: categories</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      // Right shift by one</span><br><span class=\"line\">      bitShiftedInput = bitShiftedInput &gt;&gt; 1</span><br><span class=\"line\">      j += 1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    categories</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//对于连续特征，找到其对应的splits分割点</span><br><span class=\"line\">private[tree] def findSplitsForContinuousFeature(</span><br><span class=\"line\">      featureSamples: Iterable[Double], </span><br><span class=\"line\">      metadata: DecisionTreeMetadata, </span><br><span class=\"line\">      featureIndex: Int): Array[Double] = &#123;</span><br><span class=\"line\">    //确保有连续特征</span><br><span class=\"line\">    require(metadata.isContinuous(featureIndex),</span><br><span class=\"line\">      &quot;findSplitsForContinuousFeature can only be used to find splits for a continuous feature.&quot;)</span><br><span class=\"line\">    //寻找splits分割点</span><br><span class=\"line\">    val splits = if (featureSamples.isEmpty) &#123;</span><br><span class=\"line\">      Array.empty[Double]  //如果样本数为0， 返回空数组</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //得到metadata里的split数量</span><br><span class=\"line\">      val numSplits = metadata.numSplits(featureIndex) </span><br><span class=\"line\"></span><br><span class=\"line\">      //在采样得到的样本中，计算每个特征取值的计数、以及总样本数量</span><br><span class=\"line\">      val (valueCountMap, numSamples) = featureSamples.foldLeft((Map.empty[Double, Int], 0)) &#123;</span><br><span class=\"line\">        case ((m, cnt), x) =&gt;</span><br><span class=\"line\">          (m + ((x, m.getOrElse(x, 0) + 1)), cnt + 1)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      // 对于每个特征取值进行排序</span><br><span class=\"line\">      val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray</span><br><span class=\"line\">      //如果得到的possible splits数量小于metadata中该特征的的split数量，则直接以当前每个特征取值作为分割的阈值</span><br><span class=\"line\">      val possibleSplits = valueCounts.length - 1</span><br><span class=\"line\">      if (possibleSplits &lt;= numSplits) &#123; </span><br><span class=\"line\">        valueCounts.map(_._1).init</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        //否则，根据总样本数量，计算平均每个区间对应的特征取值数量，假设为n。然后，对于n, 2*n, 3*n ...的位置分别设置标记。设置2个游标分别指向valueCounts内部连续的两个特征取值，从前向后遍历，当后面游标到标记的距离大于前面的游标时，将前面游标的位置对应的特征取值设置为一个split点。</span><br><span class=\"line\">        //计算平均每个区间对应的特征取值数量</span><br><span class=\"line\">        val stride: Double = numSamples.toDouble / (numSplits + 1)</span><br><span class=\"line\">        logDebug(&quot;stride = &quot; + stride)</span><br><span class=\"line\">        //splitsBuilder用于存储每个分割阈值</span><br><span class=\"line\">        val splitsBuilder = mutable.ArrayBuilder.make[Double]</span><br><span class=\"line\">        //特征取值从小到大的位置索引</span><br><span class=\"line\">        var index = 1</span><br><span class=\"line\">        //当前访问的所有特征取值数量之和</span><br><span class=\"line\">        var currentCount = valueCounts(0)._2</span><br><span class=\"line\">        //下一次的标记位置      </span><br><span class=\"line\">        var targetCount = stride</span><br><span class=\"line\">        while (index &lt; valueCounts.length) &#123;</span><br><span class=\"line\">          val previousCount = currentCount</span><br><span class=\"line\">          currentCount += valueCounts(index)._2</span><br><span class=\"line\">          val previousGap = math.abs(previousCount - targetCount)</span><br><span class=\"line\">          val currentGap = math.abs(currentCount - targetCount)</span><br><span class=\"line\">          //使前面游标和后面游标的距离更小，且较小游标距离标记位置的距离最近</span><br><span class=\"line\">          if (previousGap &lt; currentGap) &#123;</span><br><span class=\"line\">            splitsBuilder += valueCounts(index - 1)._1</span><br><span class=\"line\">            targetCount += stride</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          index += 1</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        splitsBuilder.result()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    splits</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"TreePoint-convertToTreeRDD\"><a href=\"#TreePoint-convertToTreeRDD\" class=\"headerlink\" title=\"TreePoint.convertToTreeRDD\"></a>TreePoint.convertToTreeRDD</h4><p>调用TreePoint类的convertToTreeRDD方法，RDD[LabeledPoint]转化为RDD[TreePoint]。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def convertToTreeRDD(</span><br><span class=\"line\">     input: RDD[LabeledPoint],</span><br><span class=\"line\">     splits: Array[Array[Split]],</span><br><span class=\"line\">     metadata: DecisionTreeMetadata): RDD[TreePoint] = &#123;</span><br><span class=\"line\">   // 构建数组featureArity，存储每个特征对应的离散值个数，连续值对应的value为0</span><br><span class=\"line\">   val featureArity: Array[Int] = new Array[Int](metadata.numFeatures)</span><br><span class=\"line\">   var featureIndex = 0</span><br><span class=\"line\">   while (featureIndex &lt; metadata.numFeatures) &#123;</span><br><span class=\"line\">     featureArity(featureIndex) = metadata.featureArity.getOrElse(featureIndex, 0)</span><br><span class=\"line\">     featureIndex += 1</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   //获得所有连续特征的分裂阈值，如果是离散特征，则数组对应空</span><br><span class=\"line\">   val thresholds: Array[Array[Double]] = featureArity.zipWithIndex.map &#123; case (arity, idx) =&gt;</span><br><span class=\"line\">     if (arity == 0) &#123;</span><br><span class=\"line\">       splits(idx).map(_.asInstanceOf[ContinuousSplit].threshold)</span><br><span class=\"line\">     &#125; else &#123;</span><br><span class=\"line\">       Array.empty[Double]</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   //将样本的每个原始特征，转化为对应的bin特征值，用于训练</span><br><span class=\"line\">   input.map &#123; x =&gt;</span><br><span class=\"line\">     TreePoint.labeledPointToTreePoint(x, thresholds, featureArity)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//将单个样本的原始特征，转化为对应的bin特征值，用于训练</span><br><span class=\"line\">private def labeledPointToTreePoint(</span><br><span class=\"line\">    labeledPoint: LabeledPoint,</span><br><span class=\"line\">    thresholds: Array[Array[Double]],</span><br><span class=\"line\">    featureArity: Array[Int]): TreePoint = &#123;</span><br><span class=\"line\">  //特征数量</span><br><span class=\"line\">  val numFeatures = labeledPoint.features.size</span><br><span class=\"line\">  //为每个特征找到对应的bin特征值，存储在arr数组</span><br><span class=\"line\">  val arr = new Array[Int](numFeatures)</span><br><span class=\"line\">  var featureIndex = 0</span><br><span class=\"line\">  while (featureIndex &lt; numFeatures) &#123;</span><br><span class=\"line\">    //寻找数据点labeledPoint、当前特征featureIndex对应的bin特征值</span><br><span class=\"line\">    arr(featureIndex) =</span><br><span class=\"line\">      findBin(featureIndex, labeledPoint, featureArity(featureIndex), thresholds(featureIndex))</span><br><span class=\"line\">    featureIndex += 1</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  new TreePoint(labeledPoint.label, arr)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def findBin(</span><br><span class=\"line\">      featureIndex: Int,</span><br><span class=\"line\">      labeledPoint: LabeledPoint,</span><br><span class=\"line\">      featureArity: Int,</span><br><span class=\"line\">      thresholds: Array[Double]): Int = &#123;</span><br><span class=\"line\">    //获取当前labeledPoint的第featureIndex个原始特征值</span><br><span class=\"line\">    val featureValue = labeledPoint.features(featureIndex)</span><br><span class=\"line\">    </span><br><span class=\"line\">    if (featureArity == 0) &#123; </span><br><span class=\"line\">      //如果是连续特征，利用二分法得到当前特征值对应的离散区间下标</span><br><span class=\"line\">      val idx = java.util.Arrays.binarySearch(thresholds, featureValue)</span><br><span class=\"line\">      if (idx &gt;= 0) &#123;</span><br><span class=\"line\">        idx</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        -idx - 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //如果是离散值，则直接返回当前的特征值</span><br><span class=\"line\">      if (featureValue &lt; 0 || featureValue &gt;= featureArity) &#123;</span><br><span class=\"line\">        throw new IllegalArgumentException(</span><br><span class=\"line\">          s&quot;DecisionTree given invalid data:&quot; +</span><br><span class=\"line\">            s&quot; Feature $featureIndex is categorical with values in &#123;0,...,$&#123;featureArity - 1&#125;,&quot; +</span><br><span class=\"line\">            s&quot; but a data point gives it value $featureValue.\\n&quot; +</span><br><span class=\"line\">            &quot;  Bad data point: &quot; + labeledPoint.toString)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      featureValue.toInt</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//LabeledPoint类</span><br><span class=\"line\">case class LabeledPoint(@Since(&quot;2.0.0&quot;) label: Double, @Since(&quot;2.0.0&quot;) features: Vector) &#123;</span><br><span class=\"line\">  override def toString: String = &#123;</span><br><span class=\"line\">    s&quot;($label,$features)&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//TreePoint类</span><br><span class=\"line\">private[spark] class TreePoint(val label: Double, val binnedFeatures: Array[Int])</span><br><span class=\"line\">  extends Serializable &#123;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"BaggedPoint-convertToBaggedRDD\"><a href=\"#BaggedPoint-convertToBaggedRDD\" class=\"headerlink\" title=\"BaggedPoint.convertToBaggedRDD\"></a>BaggedPoint.convertToBaggedRDD</h4><p>RDD[Datum]数据集转换成RDD[BaggedPoint[Datum]的表示类型，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def convertToBaggedRDD[Datum] (</span><br><span class=\"line\">    input: RDD[Datum], //输入数据集</span><br><span class=\"line\">    subsamplingRate: Double, //采样率</span><br><span class=\"line\">    numSubsamples: Int, //采样次数</span><br><span class=\"line\">    withReplacement: Boolean, //是否有放回</span><br><span class=\"line\">    //随机数种子</span><br><span class=\"line\">    seed: Long = Utils.random.nextLong()): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class=\"line\">  if (withReplacement) &#123;//有放回采样，生成BaggedPoint结构表示</span><br><span class=\"line\">    convertToBaggedRDDSamplingWithReplacement(input, subsamplingRate, numSubsamples, seed)</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    //当采样比为1，并且采样次数为1时，不采样，只生成BaggedPoint结构表示</span><br><span class=\"line\">    if (numSubsamples == 1 &amp;&amp; subsamplingRate == 1.0) &#123;</span><br><span class=\"line\">      convertToBaggedRDDWithoutSampling(input)</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //无放回采样，生成BaggedPoint结构表示</span><br><span class=\"line\">      convertToBaggedRDDSamplingWithoutReplacement(input, subsamplingRate, numSubsamples, seed)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//有放回采样，数据转换为RDD[BaggedPoint[Datum]]</span><br><span class=\"line\">private def convertToBaggedRDDSamplingWithReplacement[Datum] (</span><br><span class=\"line\">    input: RDD[Datum],//输入数据集</span><br><span class=\"line\">    subsample: Double,//采样率</span><br><span class=\"line\">    numSubsamples: Int,//采样次数</span><br><span class=\"line\">    //随机数种子</span><br><span class=\"line\">    seed: Long): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class=\"line\">  input.mapPartitionsWithIndex &#123; (partitionIndex, instances) =&gt;</span><br><span class=\"line\">    //每个分区生成一个泊松采样器，通过采样率、随机种子、分区索引等初始化</span><br><span class=\"line\">    val poisson = new PoissonDistribution(subsample)</span><br><span class=\"line\">    poisson.reseedRandomGenerator(seed + partitionIndex + 1)</span><br><span class=\"line\">    //将每个实例变换成BaggedPoint结构表示</span><br><span class=\"line\">    instances.map &#123; instance =&gt;</span><br><span class=\"line\">      val subsampleWeights = new Array[Double](numSubsamples)</span><br><span class=\"line\">      var subsampleIndex = 0</span><br><span class=\"line\">      //依次对每次采样，生成权重（即该实例在每次无放回采样出现的次数）</span><br><span class=\"line\">      while (subsampleIndex &lt; numSubsamples) &#123;</span><br><span class=\"line\">        subsampleWeights(subsampleIndex) = poisson.sample()</span><br><span class=\"line\">        subsampleIndex += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      //生成BaggedPoint结构表示</span><br><span class=\"line\">      new BaggedPoint(instance, subsampleWeights) </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//BaggedPoint类，datum表示数据实例，subsampleWeights表示当前实例在每个采样中的权重。</span><br><span class=\"line\">如(datum, [1, 0, 4])表示有3次采样，数据实例在3次采样中出现的次数分别为1，0，4</span><br><span class=\"line\">private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])</span><br><span class=\"line\">  extends Serializable</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//原始数据（不采样）直接转换为BaggedPoint结构表示</span><br><span class=\"line\">private def convertToBaggedRDDWithoutSampling[Datum] (</span><br><span class=\"line\">    input: RDD[Datum]): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class=\"line\">  input.map(datum =&gt; new BaggedPoint(datum, Array(1.0)))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//无放回采样，数据转换为RDD[BaggedPoint[Datum]]</span><br><span class=\"line\">private def convertToBaggedRDDSamplingWithoutReplacement[Datum] (</span><br><span class=\"line\">    input: RDD[Datum],</span><br><span class=\"line\">    subsamplingRate: Double,</span><br><span class=\"line\">    numSubsamples: Int,</span><br><span class=\"line\">    seed: Long): RDD[BaggedPoint[Datum]] = &#123;</span><br><span class=\"line\">  input.mapPartitionsWithIndex &#123; (partitionIndex, instances) =&gt;</span><br><span class=\"line\">    //使用随机数种子，分区索引，构建随机数生成器</span><br><span class=\"line\">    val rng = new XORShiftRandom</span><br><span class=\"line\">    rng.setSeed(seed + partitionIndex + 1)</span><br><span class=\"line\">    //将每个实例变换成BaggedPoint结构表示</span><br><span class=\"line\">    instances.map &#123; instance =&gt;</span><br><span class=\"line\">      val subsampleWeights = new Array[Double](numSubsamples)</span><br><span class=\"line\">      var subsampleIndex = 0</span><br><span class=\"line\">      //对于每次采样，生成0-1之间的随机数，如果小于采样比，则对应权重为1，否则为0</span><br><span class=\"line\">      while (subsampleIndex &lt; numSubsamples) &#123;</span><br><span class=\"line\">        val x = rng.nextDouble()</span><br><span class=\"line\">        subsampleWeights(subsampleIndex) = &#123;</span><br><span class=\"line\">          if (x &lt; subsamplingRate) 1.0 else 0.0</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        subsampleIndex += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      //转换为BaggedPoint结构数据</span><br><span class=\"line\">      new BaggedPoint(instance, subsampleWeights)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"RandomForest-selectNodesToSplit\"><a href=\"#RandomForest-selectNodesToSplit\" class=\"headerlink\" title=\"RandomForest.selectNodesToSplit\"></a>RandomForest.selectNodesToSplit</h4><p>选择当前迭代待分裂的节点，以及确定每个节点使用的特征。每次选择都根据内存限制、每个节点占用的内存（如果每个节点使用的是采样后的特征），自适应地确定节点个数。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private[tree] def selectNodesToSplit(</span><br><span class=\"line\">      nodeStack: mutable.Stack[(Int, LearningNode)], //存储节点的栈结构</span><br><span class=\"line\">      maxMemoryUsage: Long, //最大占用内存限制</span><br><span class=\"line\">      metadata: DecisionTreeMetadata, //元数据</span><br><span class=\"line\">      //随机数</span><br><span class=\"line\">      rng: Random): </span><br><span class=\"line\">      //返回值包括：（1）每个树对应的待分裂节点数组， </span><br><span class=\"line\">      //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）</span><br><span class=\"line\">      (Map[Int, Array[LearningNode]], Map[Int, Map[Int, NodeIndexInfo]]) = &#123;</span><br><span class=\"line\">      //nodesForGroup(treeIndex) 存储第treeIndex个树对应的待分裂节点数组</span><br><span class=\"line\">      val mutableNodesForGroup = new mutable.HashMap[Int, mutable.ArrayBuffer[LearningNode]]()</span><br><span class=\"line\">      //每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）</span><br><span class=\"line\">      val mutableTreeToNodeToIndexInfo =</span><br><span class=\"line\">      new mutable.HashMap[Int, mutable.HashMap[Int, NodeIndexInfo]]()</span><br><span class=\"line\">      var memUsage: Long = 0L  //当前使用内存</span><br><span class=\"line\">      var numNodesInGroup = 0  //当前分组的节点数量</span><br><span class=\"line\">      // If maxMemoryInMB is set very small, we want to still try to split 1 node,</span><br><span class=\"line\">      // so we allow one iteration if memUsage == 0.</span><br><span class=\"line\">      //如果栈不空，并且（1）如果内存上限设置非常小，我们要去报至少能有1个节点用于分裂</span><br><span class=\"line\">      //（2）当前使用内存小于内存上限值，则进一步选择节点用于分裂</span><br><span class=\"line\">      while (nodeStack.nonEmpty &amp;&amp; (memUsage &lt; maxMemoryUsage || memUsage == 0)) &#123;</span><br><span class=\"line\">      val (treeIndex, node) = nodeStack.top //选择栈顶节点</span><br><span class=\"line\">      // Choose subset of features for node (if subsampling).</span><br><span class=\"line\">     </span><br><span class=\"line\">      val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) &#123;       //如果特征需要采样，则对所有特征进行无放回采样</span><br><span class=\"line\">        Some(SamplingUtils.reservoirSampleAndCount(Range(0,</span><br><span class=\"line\">          metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong())._1)</span><br><span class=\"line\">      &#125; else &#123;//如果特征不需要采样，则返回None</span><br><span class=\"line\">        None</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      //通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的内存</span><br><span class=\"line\">      val nodeMemUsage = RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L</span><br><span class=\"line\">      ////检查增加当前节点后，内存容量是是否超过限制</span><br><span class=\"line\">      if (memUsage + nodeMemUsage &lt;= maxMemoryUsage || memUsage == 0) &#123;</span><br><span class=\"line\">        //如果加入该节点后内存没有超过限制</span><br><span class=\"line\">        nodeStack.pop() //当前节点出栈</span><br><span class=\"line\">        //更新mutableNodesForGroup，将当前节点加入对应treeIndex的节点数组</span><br><span class=\"line\">        mutableNodesForGroup.getOrElseUpdate(treeIndex, new mutable.ArrayBuffer[LearningNode]()) +=</span><br><span class=\"line\">          node</span><br><span class=\"line\">        //更新mutableTreeToNodeToIndexInfo，将当前节点的具体信息，加入对应treeindex的节点map</span><br><span class=\"line\">        mutableTreeToNodeToIndexInfo</span><br><span class=\"line\">          .getOrElseUpdate(treeIndex, new mutable.HashMap[Int, NodeIndexInfo]())(node.id)</span><br><span class=\"line\">          = new NodeIndexInfo(numNodesInGroup, featureSubset)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      numNodesInGroup += 1 //当前分组的节点数量加一</span><br><span class=\"line\">      memUsage += nodeMemUsage //当前使用内存数量加一</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (memUsage &gt; maxMemoryUsage) &#123;</span><br><span class=\"line\">      // If maxMemoryUsage is 0, we should still allow splitting 1 node.</span><br><span class=\"line\">      logWarning(s&quot;Tree learning is using approximately $memUsage bytes per iteration, which&quot; +</span><br><span class=\"line\">        s&quot; exceeds requested limit maxMemoryUsage=$maxMemoryUsage. This allows splitting&quot; +</span><br><span class=\"line\">        s&quot; $numNodesInGroup nodes in this iteration.&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //转换可变map为不可变map类型</span><br><span class=\"line\">    val nodesForGroup: Map[Int, Array[LearningNode]] =</span><br><span class=\"line\">      mutableNodesForGroup.mapValues(_.toArray).toMap</span><br><span class=\"line\">    val treeToNodeToIndexInfo = mutableTreeToNodeToIndexInfo.mapValues(_.toMap).toMap</span><br><span class=\"line\">    //返回（1）每个树对应的待分裂节点数组， </span><br><span class=\"line\">    //(2)每个树对应的每个节点的详细信息（包括当前分组内节点编号、特征集合）</span><br><span class=\"line\">    (nodesForGroup, treeToNodeToIndexInfo)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//无放回采样</span><br><span class=\"line\">def reservoirSampleAndCount[T: ClassTag](</span><br><span class=\"line\">      input: Iterator[T], //input输入的迭代器</span><br><span class=\"line\">      k: Int, //采样的样本数</span><br><span class=\"line\">      seed: Long = Random.nextLong()) //随机数种子</span><br><span class=\"line\">    : (Array[T], Long) = &#123;</span><br><span class=\"line\">    val reservoir = new Array[T](k) //存储采样结果的数组</span><br><span class=\"line\">    // 放置迭代器的前k个元素到结果数组</span><br><span class=\"line\">    var i = 0</span><br><span class=\"line\">    while (i &lt; k &amp;&amp; input.hasNext) &#123;</span><br><span class=\"line\">      val item = input.next()</span><br><span class=\"line\">      reservoir(i) = item</span><br><span class=\"line\">      i += 1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    //如果输入元素个数小于k, 则这k个特征作为返回的结果</span><br><span class=\"line\">    if (i &lt; k) &#123;</span><br><span class=\"line\">      // If input size &lt; k, trim the array to return only an array of input size.</span><br><span class=\"line\">      val trimReservoir = new Array[T](i)</span><br><span class=\"line\">      System.arraycopy(reservoir, 0, trimReservoir, 0, i)</span><br><span class=\"line\">      (trimReservoir, i) //返回结果数组，以及原始数组的元素个数</span><br><span class=\"line\">    &#125; else &#123; </span><br><span class=\"line\">      //如果输入元素个数大于k, 继续采样过程，将后面元素以一定概率随机替换前面的某个元素</span><br><span class=\"line\">      var l = i.toLong</span><br><span class=\"line\">      val rand = new XORShiftRandom(seed)</span><br><span class=\"line\">      while (input.hasNext) &#123;</span><br><span class=\"line\">        val item = input.next()</span><br><span class=\"line\">        l += 1</span><br><span class=\"line\">        //当前结果数组有k个元素，l为当前元素的序号。k/l为当前元素替换结果数组中某个元素的概率。</span><br><span class=\"line\">        //在进行替换时，对结果数组的每个元素以相等概率发生替换</span><br><span class=\"line\">        //具体方式是产生一个0到l-1之间的随机整数replacementIndex，</span><br><span class=\"line\">        //如果小于k则对第replacementIndex这个元素进行替换</span><br><span class=\"line\">        val replacementIndex = (rand.nextDouble() * l).toLong</span><br><span class=\"line\">        if (replacementIndex &lt; k) &#123;</span><br><span class=\"line\">          reservoir(replacementIndex.toInt) = item</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      (reservoir, l) //返回结果数组，以及原始数组的元素个数</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//通过所有特征的对应的bin数量之和，以及同模型类别（分类还是回归），lable数量之间的关系确定当前节点需要使用的字节数</span><br><span class=\"line\">private def aggregateSizeForNode(</span><br><span class=\"line\">    metadata: DecisionTreeMetadata,</span><br><span class=\"line\">    featureSubset: Option[Array[Int]]): Long = &#123;</span><br><span class=\"line\">  //得到所有使用的特征的bin的数量之后</span><br><span class=\"line\">  val totalBins = if (featureSubset.nonEmpty) &#123;</span><br><span class=\"line\">    //如果使用采样特征，得到采样后的所有特征bin数量之和</span><br><span class=\"line\">    featureSubset.get.map(featureIndex =&gt; metadata.numBins(featureIndex).toLong).sum</span><br><span class=\"line\">  &#125; else &#123;//否则使用所有的特征的bin数量之和</span><br><span class=\"line\">    metadata.numBins.map(_.toLong).sum</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  if (metadata.isClassification) &#123;</span><br><span class=\"line\">    //如果是分类问题，则返回bin数量之和*类别个数</span><br><span class=\"line\">    metadata.numClasses * totalBins </span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    //否则返回bin数量之和*3</span><br><span class=\"line\">    3 * totalBins</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"RandomForest-findBestSplits\"><a href=\"#RandomForest-findBestSplits\" class=\"headerlink\" title=\"RandomForest.findBestSplits\"></a>RandomForest.findBestSplits</h4><p>给定selectNodesToSplit方法选择的一组节点，找到每个节点对应的最佳分类特征的分裂位置。<strong>求解的主要思想如下：</strong></p>\n<p><strong>基于节点的分组进行并行训练：</strong>对一组的节点同时进行每个bin的统计和计算，减少不必要的数据传输成本。这样每次迭代需要更多的计算和存储成本，但是可以大大减少迭代的次数</p>\n<p><strong>基于bin的最佳分割点计算：</strong>基于bin的计算来寻找最佳分割点，计算的思想不是依次对每个样本计算其对每个孩子节点的增益贡献，而是先将所有样本的每个特征映射到对应的bin，通过聚合每个bin的数据，进一步计算对应每个特征每个分割的增益。</p>\n<p><strong>对每个partition进行聚合：</strong>由于提取知道了每个特征对应的split个数，因此可以用一个数组存储所有的bin的聚合信息，通过使用RDD的聚合方法，大大减少通讯开销。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private[tree] def findBestSplits(</span><br><span class=\"line\">     input: RDD[BaggedPoint[TreePoint]], //训练数据</span><br><span class=\"line\">     metadata: DecisionTreeMetadata, //随机森林元数据信息</span><br><span class=\"line\">     topNodesForGroup: Map[Int, LearningNode], //存储当前节点分组对应的每个树的根节点</span><br><span class=\"line\">     nodesForGroup: Map[Int, Array[LearningNode]],//存储当前节点分组对应的每个树的节点数组</span><br><span class=\"line\">     treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]],//存储当前节点分组对应的每个树索引、节点索引、及详细信息</span><br><span class=\"line\">     splits: Array[Array[Split]], //存储每个特征的所有split信息</span><br><span class=\"line\">     //存储节点的栈结构，初始化时为各个树的根节点</span><br><span class=\"line\">     nodeStack: mutable.Stack[(Int, LearningNode)],</span><br><span class=\"line\">     timer: TimeTracker = new TimeTracker,       </span><br><span class=\"line\">     nodeIdCache: Option[NodeIdCache] = None): Unit = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   //存储当前分组的节点数量</span><br><span class=\"line\">   val numNodes = nodesForGroup.values.map(_.length).sum</span><br><span class=\"line\">   logDebug(&quot;numNodes = &quot; + numNodes)</span><br><span class=\"line\">   logDebug(&quot;numFeatures = &quot; + metadata.numFeatures)</span><br><span class=\"line\">   logDebug(&quot;numClasses = &quot; + metadata.numClasses)</span><br><span class=\"line\">   logDebug(&quot;isMulticlass = &quot; + metadata.isMulticlass)</span><br><span class=\"line\">   logDebug(&quot;isMulticlassWithCategoricalFeatures = &quot; +</span><br><span class=\"line\">     metadata.isMulticlassWithCategoricalFeatures)</span><br><span class=\"line\">   logDebug(&quot;using nodeIdCache = &quot; + nodeIdCache.nonEmpty.toString)</span><br><span class=\"line\"></span><br><span class=\"line\"> </span><br><span class=\"line\">   //对于一个特定的树的特定节点，通过baggedPoint数据点，更新DTStatsAggregator聚合信息（更新相关的特征及bin的聚合类信息）</span><br><span class=\"line\">   def nodeBinSeqOp(</span><br><span class=\"line\">       treeIndex: Int, //树的索引</span><br><span class=\"line\">       nodeInfo: NodeIndexInfo, //节点信息</span><br><span class=\"line\">       agg: Array[DTStatsAggregator], //聚合信息，(node, feature, bin)</span><br><span class=\"line\">       baggedPoint: BaggedPoint[TreePoint]): Unit = &#123;//数据点</span><br><span class=\"line\">     if (nodeInfo != null) &#123;//如果节点信息不为空，表示该节点在当前计算的节点集合中</span><br><span class=\"line\">       val aggNodeIndex = nodeInfo.nodeIndexInGroup //该节点在当前分组的编号</span><br><span class=\"line\">       val featuresForNode = nodeInfo.featureSubset //该节点对应的特征集合</span><br><span class=\"line\">       //该样本在该树上的采样次数，如果为n表示5个同样的数据点同时用于更新对应的聚合信息</span><br><span class=\"line\">       val instanceWeight = baggedPoint.subsampleWeights(treeIndex) </span><br><span class=\"line\">       if (metadata.unorderedFeatures.isEmpty) &#123;</span><br><span class=\"line\">         //如果不存在无序特征，根据有序特征进行更新</span><br><span class=\"line\">         orderedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, instanceWeight, featuresForNode)</span><br><span class=\"line\">       &#125; else &#123; //都是有序特征</span><br><span class=\"line\">         mixedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, splits,</span><br><span class=\"line\">           metadata.unorderedFeatures, instanceWeight, featuresForNode)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       agg(aggNodeIndex).updateParent(baggedPoint.datum.label, instanceWeight)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   //计算当前数据被划分到的树的节点，并更新在对应节点的聚合信息。对于每个特征的相关bin,更新其聚合信息。</span><br><span class=\"line\">   def binSeqOp(</span><br><span class=\"line\">       agg: Array[DTStatsAggregator],//agg数组存储聚合信息，数据结构为（node, feature, bin）</span><br><span class=\"line\">       baggedPoint: BaggedPoint[TreePoint]): Array[DTStatsAggregator] = &#123;</span><br><span class=\"line\">     treeToNodeToIndexInfo.foreach &#123; case (treeIndex, nodeIndexToInfo) =&gt;</span><br><span class=\"line\">       //得到要更新的节点编号</span><br><span class=\"line\">       val nodeIndex = </span><br><span class=\"line\">         topNodesForGroup(treeIndex).predictImpl(baggedPoint.datum.binnedFeatures, splits)</span><br><span class=\"line\">       //对上步得到的节点，根据样本点更新其对应的bin的聚合信息</span><br><span class=\"line\">       nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     agg</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   /**</span><br><span class=\"line\">    * Do the same thing as binSeqOp, but with nodeIdCache.</span><br><span class=\"line\">    */</span><br><span class=\"line\">   def binSeqOpWithNodeIdCache(</span><br><span class=\"line\">       agg: Array[DTStatsAggregator],</span><br><span class=\"line\">       dataPoint: (BaggedPoint[TreePoint], Array[Int])): Array[DTStatsAggregator] = &#123;</span><br><span class=\"line\">     treeToNodeToIndexInfo.foreach &#123; case (treeIndex, nodeIndexToInfo) =&gt;</span><br><span class=\"line\">       val baggedPoint = dataPoint._1</span><br><span class=\"line\">       val nodeIdCache = dataPoint._2</span><br><span class=\"line\">       val nodeIndex = nodeIdCache(treeIndex)</span><br><span class=\"line\">       nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">     agg</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   </span><br><span class=\"line\">   //从treeToNodeToIndexInfo中获取每个节点对应的特征集合。key为节点在本组节点的编号，value为对应特征集合</span><br><span class=\"line\">   def getNodeToFeatures(</span><br><span class=\"line\">       treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]]): Option[Map[Int, Array[Int]]] = &#123;</span><br><span class=\"line\">     if (!metadata.subsamplingFeatures) &#123; //如果定义为不进行特征采样</span><br><span class=\"line\">       None</span><br><span class=\"line\">     &#125; else &#123;</span><br><span class=\"line\">       //定义为特征采样，从treeToNodeToIndexInfo中获取对应的节点编号和特征集合。</span><br><span class=\"line\">       val mutableNodeToFeatures = new mutable.HashMap[Int, Array[Int]]()</span><br><span class=\"line\">       treeToNodeToIndexInfo.values.foreach &#123; nodeIdToNodeInfo =&gt;</span><br><span class=\"line\">         nodeIdToNodeInfo.values.foreach &#123; nodeIndexInfo =&gt;</span><br><span class=\"line\">           assert(nodeIndexInfo.featureSubset.isDefined)</span><br><span class=\"line\">           mutableNodeToFeatures(nodeIndexInfo.nodeIndexInGroup) = nodeIndexInfo.featureSubset.get</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       Some(mutableNodeToFeatures.toMap)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   </span><br><span class=\"line\">   //用于训练的节点数组</span><br><span class=\"line\">   val nodes = new Array[LearningNode](numNodes)</span><br><span class=\"line\">   //根据nodesForGroup，在nodes中存储本轮迭代的节点，存储到nodes中</span><br><span class=\"line\">   nodesForGroup.foreach &#123; case (treeIndex, nodesForTree) =&gt;</span><br><span class=\"line\">     nodesForTree.foreach &#123; node =&gt;</span><br><span class=\"line\">       nodes(treeToNodeToIndexInfo(treeIndex)(node.id).nodeIndexInGroup) = node</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   //对于所有的节点，计算最佳特征及分割点</span><br><span class=\"line\">   timer.start(&quot;chooseSplits&quot;)</span><br><span class=\"line\">   //对于每个分区，迭代所有的样本，计算每个节点的聚合信息，</span><br><span class=\"line\">   //产出(nodeIndex, nodeAggregateStats)数据结构，</span><br><span class=\"line\">   //通过reduceByKey操作，一个节点的所有信息会被shuffle到同一个分区，通过合并信息，</span><br><span class=\"line\">   //计算每个节点的最佳分割，最后只有最佳的分割用于进一步构建决策树。</span><br><span class=\"line\">   val nodeToFeatures = getNodeToFeatures(treeToNodeToIndexInfo)//</span><br><span class=\"line\">   val nodeToFeaturesBc = input.sparkContext.broadcast(nodeToFeatures)</span><br><span class=\"line\"></span><br><span class=\"line\">   val partitionAggregates: RDD[(Int, DTStatsAggregator)] = if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">     input.zip(nodeIdCache.get.nodeIdsForInstances).mapPartitions &#123; points =&gt;</span><br><span class=\"line\">       // Construct a nodeStatsAggregators array to hold node aggregate stats,</span><br><span class=\"line\">       // each node will have a nodeStatsAggregator</span><br><span class=\"line\">       val nodeStatsAggregators = Array.tabulate(numNodes) &#123; nodeIndex =&gt;</span><br><span class=\"line\">         val featuresForNode = nodeToFeaturesBc.value.map &#123; nodeToFeatures =&gt;</span><br><span class=\"line\">           nodeToFeatures(nodeIndex)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         new DTStatsAggregator(metadata, featuresForNode)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       // iterator all instances in current partition and update aggregate stats</span><br><span class=\"line\">       points.foreach(binSeqOpWithNodeIdCache(nodeStatsAggregators, _))</span><br><span class=\"line\">       // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,</span><br><span class=\"line\">       // which can be combined with other partition using `reduceByKey`</span><br><span class=\"line\">       nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125; else &#123;</span><br><span class=\"line\">     input.mapPartitions &#123; points =&gt;</span><br><span class=\"line\">       // 在每个分区内，构建一个nodeStatsAggregators数组，其中每个元素对应一个node的DTStatsAggregator，该DTStatsAggregator包括了决策树元数据信息、以及该node对应的特征集合</span><br><span class=\"line\">       val nodeStatsAggregators = Array.tabulate(numNodes) &#123; nodeIndex =&gt;</span><br><span class=\"line\">         val featuresForNode = nodeToFeaturesBc.value.flatMap &#123; nodeToFeatures =&gt;</span><br><span class=\"line\">           Some(nodeToFeatures(nodeIndex))</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         new DTStatsAggregator(metadata, featuresForNode)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       //对当前分区，迭代所有样本，更新nodeStatsAggregators，即每个node对应的DTStatsAggregator</span><br><span class=\"line\">       points.foreach(binSeqOp(nodeStatsAggregators, _))</span><br><span class=\"line\">       //转化成(nodeIndex, nodeAggregateStats)格式，用于后续通过reduceByKey对多个分区的结果进行聚合。</span><br><span class=\"line\">       nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   //reduceByKey聚合多个partition的统计特征</span><br><span class=\"line\">   val nodeToBestSplits = partitionAggregates.reduceByKey((a, b) =&gt; a.merge(b)).map &#123;</span><br><span class=\"line\">     case (nodeIndex, aggStats) =&gt;</span><br><span class=\"line\">       //得到节点对应的特征集合</span><br><span class=\"line\">       val featuresForNode = nodeToFeaturesBc.value.flatMap &#123; nodeToFeatures =&gt;</span><br><span class=\"line\">         Some(nodeToFeatures(nodeIndex))</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">       // 找到最佳分裂特征和分裂位置，并返回度量的统计特征</span><br><span class=\"line\">       val (split: Split, stats: ImpurityStats) =</span><br><span class=\"line\">         binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))</span><br><span class=\"line\">       (nodeIndex, (split, stats))</span><br><span class=\"line\">   &#125;.collectAsMap()</span><br><span class=\"line\"></span><br><span class=\"line\">   timer.stop(&quot;chooseSplits&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">   val nodeIdUpdaters = if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">     Array.fill[mutable.Map[Int, NodeIndexUpdater]](</span><br><span class=\"line\">       metadata.numTrees)(mutable.Map[Int, NodeIndexUpdater]())</span><br><span class=\"line\">   &#125; else &#123;</span><br><span class=\"line\">     null</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   // Iterate over all nodes in this group.</span><br><span class=\"line\">   //对于本组所有节点，更新节点本身信息，如果孩子节点是课分裂的叶子节点，则将其加入栈中</span><br><span class=\"line\">   nodesForGroup.foreach &#123; case (treeIndex, nodesForTree) =&gt;</span><br><span class=\"line\">     nodesForTree.foreach &#123; node =&gt;</span><br><span class=\"line\">       val nodeIndex = node.id //节点id</span><br><span class=\"line\">       val nodeInfo = treeToNodeToIndexInfo(treeIndex)(nodeIndex) //节点信息，包括节点在当前分组编号，节点特征等</span><br><span class=\"line\">       val aggNodeIndex = nodeInfo.nodeIndexInGroup //节点在当前分组编号</span><br><span class=\"line\">       //节点对应的最佳分裂，及最佳分裂对应的不纯度度量相关统计信息</span><br><span class=\"line\">       val (split: Split, stats: ImpurityStats) =</span><br><span class=\"line\">         nodeToBestSplits(aggNodeIndex) </span><br><span class=\"line\">       logDebug(&quot;best split = &quot; + split)</span><br><span class=\"line\"></span><br><span class=\"line\">       //如果信息增益小于0，或者层次达到上限，则将当前节点设置为叶子节点</span><br><span class=\"line\">       val isLeaf =</span><br><span class=\"line\">         (stats.gain &lt;= 0) || (LearningNode.indexToLevel(nodeIndex) == metadata.maxDepth)</span><br><span class=\"line\">       node.isLeaf = isLeaf</span><br><span class=\"line\">       node.stats = stats</span><br><span class=\"line\">       logDebug(&quot;Node = &quot; + node)</span><br><span class=\"line\">       </span><br><span class=\"line\">       //当前节点非叶子节点，创建子节点</span><br><span class=\"line\">       if (!isLeaf) &#123;</span><br><span class=\"line\">         node.split = Some(split) //设置节点split参数</span><br><span class=\"line\">         //子节点层数是否达到最大值</span><br><span class=\"line\">         val childIsLeaf = (LearningNode.indexToLevel(nodeIndex) + 1) == metadata.maxDepth</span><br><span class=\"line\">         //左孩子节点层数达到最大值，或者不纯度度量等于0，则左孩子节点为叶子节点</span><br><span class=\"line\">         val leftChildIsLeaf = childIsLeaf || (stats.leftImpurity == 0.0)</span><br><span class=\"line\">         //右孩子节点层数达到最大值，或者不纯度度量等于0，则右孩子节点为叶子节点          </span><br><span class=\"line\">         val rightChildIsLeaf = childIsLeaf || (stats.rightImpurity == 0.0)</span><br><span class=\"line\">         //创建左孩子节点，getEmptyImpurityStats(stats.leftImpurityCalculator)为左孩子的不纯度度量，只有impurity、impurityCalculator两个属性</span><br><span class=\"line\">         node.leftChild = Some(LearningNode(LearningNode.leftChildIndex(nodeIndex),</span><br><span class=\"line\">           leftChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.leftImpurityCalculator)))</span><br><span class=\"line\">         //创建右孩子节点</span><br><span class=\"line\">         node.rightChild = Some(LearningNode(LearningNode.rightChildIndex(nodeIndex),</span><br><span class=\"line\">           rightChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.rightImpurityCalculator)))</span><br><span class=\"line\"></span><br><span class=\"line\">         if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">           val nodeIndexUpdater = NodeIndexUpdater(</span><br><span class=\"line\">             split = split,</span><br><span class=\"line\">             nodeIndex = nodeIndex)</span><br><span class=\"line\">           nodeIdUpdaters(treeIndex).put(nodeIndex, nodeIndexUpdater)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         // enqueue left child and right child if they are not leaves</span><br><span class=\"line\">         //如果左孩子节点不是叶子节点，则将左孩子节点入栈</span><br><span class=\"line\">         if (!leftChildIsLeaf) &#123;</span><br><span class=\"line\">           nodeStack.push((treeIndex, node.leftChild.get))</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         if (!rightChildIsLeaf) &#123;</span><br><span class=\"line\">           //如果右孩子节点不是叶子节点，则将右孩子节点入栈</span><br><span class=\"line\">           nodeStack.push((treeIndex, node.rightChild.get))</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         logDebug(&quot;leftChildIndex = &quot; + node.leftChild.get.id +</span><br><span class=\"line\">           &quot;, impurity = &quot; + stats.leftImpurity)</span><br><span class=\"line\">         logDebug(&quot;rightChildIndex = &quot; + node.rightChild.get.id +</span><br><span class=\"line\">           &quot;, impurity = &quot; + stats.rightImpurity)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   if (nodeIdCache.nonEmpty) &#123;</span><br><span class=\"line\">     // Update the cache if needed.</span><br><span class=\"line\">     nodeIdCache.get.updateNodeIndices(input, nodeIdUpdaters, splits)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//得到当前数据点对应的node index输出,模仿对数据的预测过程，从根节点开始向下传播，</span><br><span class=\"line\">//直到一个叶子节点或者未进行分裂的节点终止，返回终止节点对应的索引。</span><br><span class=\"line\">def predictImpl(binnedFeatures: Array[Int], splits: Array[Array[Split]]): Int = &#123;</span><br><span class=\"line\">  if (this.isLeaf || this.split.isEmpty) &#123;</span><br><span class=\"line\">    this.id //如果当前节点是叶子节点或者未分裂的节点，返回当前节点索引</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    val split = this.split.get //当前节点的split</span><br><span class=\"line\">    val featureIndex = split.featureIndex //当前节点split对应的特征索引</span><br><span class=\"line\">    //根据数据点在featureIndex特征上的取值，以及featureIndex特征对应的分裂，判断当前数据点是否应该向左传递。</span><br><span class=\"line\">    val splitLeft = split.shouldGoLeft(binnedFeatures(featureIndex), splits(featureIndex)) </span><br><span class=\"line\">    if (this.leftChild.isEmpty) &#123; //如果左孩子为空</span><br><span class=\"line\">      // Not yet split. Return next layer of nodes to train</span><br><span class=\"line\">      if (splitLeft) &#123; //当前节点应该向左传递，得到左孩子节点索引值</span><br><span class=\"line\">        LearningNode.leftChildIndex(this.id)</span><br><span class=\"line\">      &#125; else &#123; //当前节点应该向右传递，得到右孩子节点索引值</span><br><span class=\"line\">        LearningNode.rightChildIndex(this.id)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123; //如果左孩子不为空，</span><br><span class=\"line\">      if (splitLeft) &#123; //当前节点应该向左传递，从左节点开始，递归计算最终节点的索引</span><br><span class=\"line\">        this.leftChild.get.predictImpl(binnedFeatures, splits)</span><br><span class=\"line\">      &#125; else &#123; //当前节点应该向右传递，从右节点开始，递归计算最终节点的索引</span><br><span class=\"line\">        this.rightChild.get.predictImpl(binnedFeatures, splits)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//对于排序类特征，根据数据点、权重，更新每个特征的每个bin信息        </span><br><span class=\"line\">private def orderedBinSeqOp(</span><br><span class=\"line\">      agg: DTStatsAggregator, //聚合信息，(feature, bin)</span><br><span class=\"line\">      treePoint: TreePoint,</span><br><span class=\"line\">      instanceWeight: Double,</span><br><span class=\"line\">      featuresForNode: Option[Array[Int]]): Unit = &#123;</span><br><span class=\"line\">    val label = treePoint.label</span><br><span class=\"line\"></span><br><span class=\"line\">    // 如果是采样特征</span><br><span class=\"line\">    if (featuresForNode.nonEmpty) &#123;</span><br><span class=\"line\">      // 使用采样的特征，对于每个特征的每个bin，进行更新</span><br><span class=\"line\">      var featureIndexIdx = 0</span><br><span class=\"line\">      while (featureIndexIdx &lt; featuresForNode.get.length) &#123;</span><br><span class=\"line\">        val binIndex = treePoint.binnedFeatures(featuresForNode.get.apply(featureIndexIdx))</span><br><span class=\"line\">        agg.update(featureIndexIdx, binIndex, label, instanceWeight)</span><br><span class=\"line\">        featureIndexIdx += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      // 如果是非采样特征，使用所有特征，对每个特征的每个bin，进行更新</span><br><span class=\"line\">      val numFeatures = agg.metadata.numFeatures</span><br><span class=\"line\">      var featureIndex = 0</span><br><span class=\"line\">      while (featureIndex &lt; numFeatures) &#123;</span><br><span class=\"line\">        val binIndex = treePoint.binnedFeatures(featureIndex)</span><br><span class=\"line\">        agg.update(featureIndex, binIndex, label, instanceWeight)</span><br><span class=\"line\">        featureIndex += 1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//相对于orderedBinSeqOp函数，mixedBinSeqOp函数在同时包括排序和非排序特征情况下，更新聚合信息.</span><br><span class=\"line\">//对于有序特征，对每个特征更新一个bin</span><br><span class=\"line\">//对于无序特征，类别的子集对应的bin需要消息，每个子集的靠左bin或者靠右bin需要更新</span><br><span class=\"line\">private def mixedBinSeqOp(</span><br><span class=\"line\">      agg: DTStatsAggregator, //聚合信息，(feature, bin)</span><br><span class=\"line\">      treePoint: TreePoint,</span><br><span class=\"line\">      splits: Array[Array[Split]],</span><br><span class=\"line\">      unorderedFeatures: Set[Int],</span><br><span class=\"line\">      instanceWeight: Double,</span><br><span class=\"line\">      featuresForNode: Option[Array[Int]]): Unit = &#123;</span><br><span class=\"line\">    val numFeaturesPerNode = if (featuresForNode.nonEmpty) &#123;</span><br><span class=\"line\">      // 如果特征需要采样，使用采样特征</span><br><span class=\"line\">      featuresForNode.get.length</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      // 否则使用所有特征</span><br><span class=\"line\">      agg.metadata.numFeatures</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 迭代每个特征，更新该节点对应的bin聚合信息.</span><br><span class=\"line\">    var featureIndexIdx = 0</span><br><span class=\"line\">    while (featureIndexIdx &lt; numFeaturesPerNode) &#123;</span><br><span class=\"line\">      //得到特征对应的原始索引值</span><br><span class=\"line\">      val featureIndex = if (featuresForNode.nonEmpty) &#123;</span><br><span class=\"line\">        featuresForNode.get.apply(featureIndexIdx)</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        featureIndexIdx</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (unorderedFeatures.contains(featureIndex)) &#123;</span><br><span class=\"line\">        //如果当前特征是无序特征</span><br><span class=\"line\">        val featureValue = treePoint.binnedFeatures(featureIndex) //得到bin features</span><br><span class=\"line\">        //得到当前特征偏移量</span><br><span class=\"line\">        val leftNodeFeatureOffset = agg.getFeatureOffset(featureIndexIdx)</span><br><span class=\"line\">        // Update the left or right bin for each split.</span><br><span class=\"line\">        //得到当前特征的split数量</span><br><span class=\"line\">        val numSplits = agg.metadata.numSplits(featureIndex)</span><br><span class=\"line\">        //得到当前特征分裂信息</span><br><span class=\"line\">        val featureSplits = splits(featureIndex)</span><br><span class=\"line\">        var splitIndex = 0</span><br><span class=\"line\">        while (splitIndex &lt; numSplits) &#123;</span><br><span class=\"line\">          //根据当前特征值，判断是否应该向左传递，如果向左传递，则将节点对当前特征的当前区间聚合信息进行更新</span><br><span class=\"line\">          if (featureSplits(splitIndex).shouldGoLeft(featureValue, featureSplits)) &#123;</span><br><span class=\"line\">            agg.featureUpdate(leftNodeFeatureOffset, splitIndex, treePoint.label, instanceWeight)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          splitIndex += 1</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        // 如果是有序特征，则直接更新对应特征的对应bin信息</span><br><span class=\"line\">        val binIndex = treePoint.binnedFeatures(featureIndex)</span><br><span class=\"line\">        agg.update(featureIndexIdx, binIndex, treePoint.label, instanceWeight)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      featureIndexIdx += 1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//寻找最佳分裂特征和分裂位置</span><br><span class=\"line\">private[tree] def binsToBestSplit(</span><br><span class=\"line\">      binAggregates: DTStatsAggregator, //所有feature的bin的统计信息</span><br><span class=\"line\">      splits: Array[Array[Split]],//所有feature的所有split</span><br><span class=\"line\">      featuresForNode: Option[Array[Int]],//node对应的feature子集</span><br><span class=\"line\">      //当前node</span><br><span class=\"line\">      node: LearningNode): (Split, ImpurityStats) = &#123; //返回值为最佳分裂，及对应的不纯度相关度量</span><br><span class=\"line\"></span><br><span class=\"line\">    // Calculate InformationGain and ImpurityStats if current node is top node</span><br><span class=\"line\">    // 当前节点对应的树的层次</span><br><span class=\"line\">    val level = LearningNode.indexToLevel(node.id)</span><br><span class=\"line\">    // 如果是根节点，不纯度度量为0</span><br><span class=\"line\">    var gainAndImpurityStats: ImpurityStats = if (level == 0) &#123;</span><br><span class=\"line\">      null</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //否则为当前节点对应的相关度量stats</span><br><span class=\"line\">      node.stats</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //获得合法的特征分裂</span><br><span class=\"line\">    val validFeatureSplits =</span><br><span class=\"line\">      Range(0, binAggregates.metadata.numFeaturesPerNode).view.map &#123; </span><br><span class=\"line\">      //得到原始特征对应的feature index</span><br><span class=\"line\">      featureIndexIdx =&gt;</span><br><span class=\"line\">        featuresForNode.map(features =&gt; (featureIndexIdx, features(featureIndexIdx)))</span><br><span class=\"line\">          .getOrElse((featureIndexIdx, featureIndexIdx))</span><br><span class=\"line\">      &#125;.withFilter &#123; case (_, featureIndex) =&gt; //过滤对应split数量为0的特征</span><br><span class=\"line\">        binAggregates.metadata.numSplits(featureIndex) != 0</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    //对每个(feature,split), 计算增益，并选择增益最大的(feature,split)</span><br><span class=\"line\">    val (bestSplit, bestSplitStats) =</span><br><span class=\"line\">      validFeatureSplits.map &#123; case (featureIndexIdx, featureIndex) =&gt;</span><br><span class=\"line\">        //得到索引为featureIndex的特征对应的split数量</span><br><span class=\"line\">        val numSplits = binAggregates.metadata.numSplits(featureIndex)</span><br><span class=\"line\">        if (binAggregates.metadata.isContinuous(featureIndex)) &#123;</span><br><span class=\"line\">          //如果是连续特征</span><br><span class=\"line\">          //计算每个bin的累积统计信息（包括第一个bin到当前bin之间的所有bin对应的统计信息）</span><br><span class=\"line\">          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class=\"line\">          var splitIndex = 0</span><br><span class=\"line\">          while (splitIndex &lt; numSplits) &#123;</span><br><span class=\"line\">            binAggregates.mergeForFeature(nodeFeatureOffset, splitIndex + 1, splitIndex)</span><br><span class=\"line\">            splitIndex += 1</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          //找到最好的split</span><br><span class=\"line\">          val (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class=\"line\">            Range(0, numSplits).map &#123; case splitIdx =&gt;</span><br><span class=\"line\">              //得到当前split左孩子对应的统计信息</span><br><span class=\"line\">              val leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, splitIdx)</span><br><span class=\"line\">              //得到当前split右孩子对应的统计信息， 为得到右孩子对应的统计信息，需要所有的统计信息减去左孩子的统计信息</span><br><span class=\"line\">              val rightChildStats =</span><br><span class=\"line\">                binAggregates.getImpurityCalculator(nodeFeatureOffset, numSplits)</span><br><span class=\"line\">              //所有的统计信息减去左孩子的统计信息</span><br><span class=\"line\">              rightChildStats.subtract(leftChildStats)</span><br><span class=\"line\">              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,</span><br><span class=\"line\">                leftChildStats, rightChildStats, binAggregates.metadata)</span><br><span class=\"line\">              (splitIdx, gainAndImpurityStats)//分裂索引，不纯度度量信息</span><br><span class=\"line\">            &#125;.maxBy(_._2.gain)//取信息增益最大的分裂</span><br><span class=\"line\">          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)</span><br><span class=\"line\">        &#125; else if (binAggregates.metadata.isUnordered(featureIndex)) &#123;</span><br><span class=\"line\">          //无序离散特征</span><br><span class=\"line\">          val leftChildOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class=\"line\">          val (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class=\"line\">            Range(0, numSplits).map &#123; splitIndex =&gt;</span><br><span class=\"line\">              //得到左孩子聚合信息</span><br><span class=\"line\">              val leftChildStats = binAggregates.getImpurityCalculator(leftChildOffset, splitIndex)</span><br><span class=\"line\">              //得到右孩子聚合信息</span><br><span class=\"line\">              val rightChildStats = binAggregates.getParentImpurityCalculator()</span><br><span class=\"line\">                .subtract(leftChildStats)</span><br><span class=\"line\">              //计算不纯度度量相关统计信息</span><br><span class=\"line\">              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,</span><br><span class=\"line\">                leftChildStats, rightChildStats, binAggregates.metadata)</span><br><span class=\"line\">              (splitIndex, gainAndImpurityStats) //分裂索引，不纯度度量信息</span><br><span class=\"line\">            &#125;.maxBy(_._2.gain)//取信息增益最大的分裂</span><br><span class=\"line\">          (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          // 对于排序离散特征</span><br><span class=\"line\">          //得到聚合信息的其实地址</span><br><span class=\"line\">          val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class=\"line\">          //得到类别数量</span><br><span class=\"line\">          val numCategories = binAggregates.metadata.numBins(featureIndex)</span><br><span class=\"line\"></span><br><span class=\"line\">          //每个bin是一个特征值，根据质心对这些特征值排序，共K个特征值，对应生成K-1个划分</span><br><span class=\"line\">          val centroidForCategories = Range(0, numCategories).map &#123; case featureValue =&gt;</span><br><span class=\"line\">            //得到不纯度度量的统计信息</span><br><span class=\"line\">            val categoryStats =</span><br><span class=\"line\">              binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)</span><br><span class=\"line\">            val centroid = if (categoryStats.count != 0) &#123;//如果对应样本数量不为0，</span><br><span class=\"line\">              if (binAggregates.metadata.isMulticlass) &#123;</span><br><span class=\"line\">                //如果是多分类决策树，则将对应多标签的不纯度度量作为质心</span><br><span class=\"line\">                categoryStats.calculate()</span><br><span class=\"line\">              &#125; else if (binAggregates.metadata.isClassification) &#123;</span><br><span class=\"line\">                //如果是二分类问题，则将对应的正样本数量作为质心</span><br><span class=\"line\">                categoryStats.stats(1)</span><br><span class=\"line\">              &#125; else &#123;</span><br><span class=\"line\">                //如果是回归问题，则将对应的预测值作为质心</span><br><span class=\"line\">                categoryStats.predict</span><br><span class=\"line\">              &#125;</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">              Double.MaxValue //如果对应样本数量为0，则质心为Double.MaxValue</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            (featureValue, centroid) //返回每个特征值对应的样本质心</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">          logDebug(&quot;Centroids for categorical variable: &quot; + centroidForCategories.mkString(&quot;,&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">          // 根据质心，将特征对应的bin排序（即对应的离散特征值排序）</span><br><span class=\"line\">          val categoriesSortedByCentroid = centroidForCategories.toList.sortBy(_._2)</span><br><span class=\"line\"></span><br><span class=\"line\">          logDebug(&quot;Sorted centroids for categorical variable = &quot; +</span><br><span class=\"line\">            categoriesSortedByCentroid.mkString(&quot;,&quot;))</span><br><span class=\"line\"></span><br><span class=\"line\">          // 从左到右，依次计算每个category对应的从第一个category到当前categofy的统计信息聚合结果</span><br><span class=\"line\">          var splitIndex = 0</span><br><span class=\"line\">          while (splitIndex &lt; numSplits) &#123;</span><br><span class=\"line\">            val currentCategory = categoriesSortedByCentroid(splitIndex)._1</span><br><span class=\"line\">            val nextCategory = categoriesSortedByCentroid(splitIndex + 1)._1</span><br><span class=\"line\">            binAggregates.mergeForFeature(nodeFeatureOffset, nextCategory, currentCategory)</span><br><span class=\"line\">            splitIndex += 1</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          </span><br><span class=\"line\">          //所有特征值的聚合结果对应的category索引</span><br><span class=\"line\">          val lastCategory = categoriesSortedByCentroid.last._1</span><br><span class=\"line\">          //找到最佳的分裂</span><br><span class=\"line\">          val (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class=\"line\">            Range(0, numSplits).map &#123; splitIndex =&gt;</span><br><span class=\"line\">              //得到当前索引的特征值</span><br><span class=\"line\">              val featureValue = categoriesSortedByCentroid(splitIndex)._1</span><br><span class=\"line\">              //得到左孩子对应的聚合信息</span><br><span class=\"line\">              val leftChildStats =</span><br><span class=\"line\">                binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)</span><br><span class=\"line\">              //得到右孩子对应的聚合信息</span><br><span class=\"line\">              val rightChildStats =</span><br><span class=\"line\">                binAggregates.getImpurityCalculator(nodeFeatureOffset, lastCategory)</span><br><span class=\"line\">              rightChildStats.subtract(leftChildStats)</span><br><span class=\"line\">              //得到不纯度度量的相关统计信息</span><br><span class=\"line\">              gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,</span><br><span class=\"line\">                leftChildStats, rightChildStats, binAggregates.metadata)</span><br><span class=\"line\">              (splitIndex, gainAndImpurityStats)</span><br><span class=\"line\">            &#125;.maxBy(_._2.gain)//根据信息增益进行排序，得到信息增益最大的split索引及增益</span><br><span class=\"line\">          </span><br><span class=\"line\">          //得到最佳分裂边界</span><br><span class=\"line\">          val categoriesForSplit =</span><br><span class=\"line\">            categoriesSortedByCentroid.map(_._1.toDouble).slice(0, bestFeatureSplitIndex + 1)</span><br><span class=\"line\">          //得到最佳分裂，包括特征索引、划分边界、类别数量等</span><br><span class=\"line\">          val bestFeatureSplit =</span><br><span class=\"line\">            new CategoricalSplit(featureIndex, categoriesForSplit.toArray, numCategories)</span><br><span class=\"line\">           //返回最佳分裂，及对应的增益统计信息</span><br><span class=\"line\">          (bestFeatureSplit, bestFeatureGainStats)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;.maxBy(_._2.gain)//针对所有特征，按照信息增益进行排序，取增益最大的特征</span><br><span class=\"line\"></span><br><span class=\"line\">    (bestSplit, bestSplitStats)//返回最佳分裂，及对应的增益统计信息</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">根据分裂对应的左孩子聚合信息，右孩子聚合信息，计算当前节点不纯度度量的相关统计信息</span><br><span class=\"line\">private def calculateImpurityStats(</span><br><span class=\"line\">      stats: ImpurityStats,</span><br><span class=\"line\">      leftImpurityCalculator: ImpurityCalculator,</span><br><span class=\"line\">      rightImpurityCalculator: ImpurityCalculator,</span><br><span class=\"line\">      metadata: DecisionTreeMetadata): ImpurityStats = &#123;</span><br><span class=\"line\">    //得到父节点的聚合信息</span><br><span class=\"line\">    val parentImpurityCalculator: ImpurityCalculator = if (stats == null) &#123;</span><br><span class=\"line\">      leftImpurityCalculator.copy.add(rightImpurityCalculator)</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      stats.impurityCalculator</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //得到父节点不纯度度量</span><br><span class=\"line\">    val impurity: Double = if (stats == null) &#123;</span><br><span class=\"line\">      parentImpurityCalculator.calculate()</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      stats.impurity</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">   </span><br><span class=\"line\">    val leftCount = leftImpurityCalculator.count //根据当前分裂得到的左孩子对应样本数量</span><br><span class=\"line\">    val rightCount = rightImpurityCalculator.count //根据当前分裂得到的右孩子对应样本数量</span><br><span class=\"line\"></span><br><span class=\"line\">    val totalCount = leftCount + rightCount  //当前分裂对应的总样本数量</span><br><span class=\"line\"></span><br><span class=\"line\">    // If left child or right child doesn&apos;t satisfy minimum instances per node,</span><br><span class=\"line\">    // then this split is invalid, return invalid information gain stats.</span><br><span class=\"line\">    //如果左孩子或者右孩子样本数量小于下限值，返回不合法的不纯度度量信息</span><br><span class=\"line\">    if ((leftCount &lt; metadata.minInstancesPerNode) ||</span><br><span class=\"line\">      (rightCount &lt; metadata.minInstancesPerNode)) &#123;</span><br><span class=\"line\">      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //左孩子对应的不纯度度量</span><br><span class=\"line\">    val leftImpurity = leftImpurityCalculator.calculate() // Note: This equals 0 if count = 0</span><br><span class=\"line\">    //右孩子对应的不纯度度量</span><br><span class=\"line\">    val rightImpurity = rightImpurityCalculator.calculate()</span><br><span class=\"line\">    //左孩子权重</span><br><span class=\"line\">    val leftWeight = leftCount / totalCount.toDouble</span><br><span class=\"line\">    //右孩子权重</span><br><span class=\"line\">    val rightWeight = rightCount / totalCount.toDouble</span><br><span class=\"line\">    //信息增益</span><br><span class=\"line\">    val gain = impurity - leftWeight * leftImpurity - rightWeight * rightImpurity</span><br><span class=\"line\">    //信息增益小于下限值，则返回不合法的不纯度度量信息</span><br><span class=\"line\">      if (gain &lt; metadata.minInfoGain) &#123;</span><br><span class=\"line\">      return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    //返回不纯度度量信息</span><br><span class=\"line\">    new ImpurityStats(gain, impurity, parentImpurityCalculator,</span><br><span class=\"line\">      leftImpurityCalculator, rightImpurityCalculator)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"模型预测\"><a href=\"#模型预测\" class=\"headerlink\" title=\"模型预测\"></a>模型预测</h2><p>通过模型训练生成决策树（随机森林）模型RandomForestModel，随机森林模型继承了树的组合模型TreeEnsembleModel，进一步通过predictBySumming函数，对传进的样本点进行预测。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//对样本点features进行预测</span><br><span class=\"line\">private def predictBySumming(features: Vector): Double = &#123;</span><br><span class=\"line\">  //对每棵决策树进行预测，然后自后结果为每个决策树结果的加权求和</span><br><span class=\"line\">  val treePredictions = trees.map(_.predict(features))</span><br><span class=\"line\">  blas.ddot(numTrees, treePredictions, 1, treeWeights, 1)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//DecisionTreeModel.predict方法</span><br><span class=\"line\">def predict(features: Vector): Double = &#123;</span><br><span class=\"line\">  //根据头部节点预测lable</span><br><span class=\"line\">  topNode.predict(features)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//Node. predict方法</span><br><span class=\"line\">def predict(features: Vector): Double = &#123;</span><br><span class=\"line\">  if (isLeaf) &#123;</span><br><span class=\"line\">    predict.predict //如果是叶子节点，直接输出</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    if (split.get.featureType == Continuous) &#123; </span><br><span class=\"line\">      //如果是连续特征，根据分裂阈值，决定走左孩子节点还是右孩子节点</span><br><span class=\"line\">      if (features(split.get.feature) &lt;= split.get.threshold) &#123;</span><br><span class=\"line\">        leftNode.get.predict(features)</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        rightNode.get.predict(features)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      //如果是离散特征，根据特征是否被当前节点对应的特征集合包含，决定走左孩子节点还是右孩子节点</span><br><span class=\"line\">      if (split.get.categories.contains(features(split.get.feature))) &#123;</span><br><span class=\"line\">        leftNode.get.predict(features)</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        rightNode.get.predict(features)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>【1】<a href=\"http://spark.apache.org/mllib/\" target=\"_blank\" rel=\"noopener\">http://spark.apache.org/mllib/</a><br>【2】<a href=\"http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html</a></p>\n"}],"PostAsset":[{"_id":"source/_posts/mf/data_model.png","slug":"data_model.png","post":"cjdikguda0006ga01h7pwpqbo","modified":0,"renderable":0},{"_id":"source/_posts/qualified_model/probability.png","slug":"probability.png","post":"cjdikgudd000aga01zx4i7zz0","modified":0,"renderable":0},{"_id":"source/_posts/dnn_for_youtube_recommandation/evaluation_of_diffrent_configure.png","slug":"evaluation_of_diffrent_configure.png","post":"cjdikgud80005ga01idhc83co","modified":0,"renderable":0},{"_id":"source/_posts/dnn_for_youtube_recommandation/example_age_efficacy.png","slug":"example_age_efficacy.png","post":"cjdikgud80005ga01idhc83co","modified":0,"renderable":0},{"_id":"source/_posts/dnn_for_youtube_recommandation/model_performance_with_feature_depth.png","slug":"model_performance_with_feature_depth.png","post":"cjdikgud80005ga01idhc83co","modified":0,"renderable":0},{"_id":"source/_posts/dnn_for_youtube_recommandation/recommand_matching.png","slug":"recommand_matching.png","post":"cjdikgud80005ga01idhc83co","modified":0,"renderable":0},{"_id":"source/_posts/dnn_for_youtube_recommandation/recommand_ranking.png","slug":"recommand_ranking.png","post":"cjdikgud80005ga01idhc83co","modified":0,"renderable":0},{"_id":"source/_posts/dnn_for_youtube_recommandation/recommand_system.png","slug":"recommand_system.png","post":"cjdikgud80005ga01idhc83co","modified":0,"renderable":0},{"_id":"source/_posts/dnn_for_youtube_recommandation/trainning_sequence_behavior_and_lable.png","slug":"trainning_sequence_behavior_and_lable.png","post":"cjdikgud80005ga01idhc83co","modified":0,"renderable":0},{"_id":"source/_posts/decision_tree/decision_tree_example.png","slug":"decision_tree_example.png","post":"cjdikguhr001sga01rt9gx0j9","modified":0,"renderable":0},{"_id":"source/_posts/user_profile/function_of_user_profile.png","slug":"function_of_user_profile.png","post":"cjdikguhn001qga01k24kjnr4","modified":0,"renderable":0},{"_id":"source/_posts/user_profile/life_recycle.png","slug":"life_recycle.png","post":"cjdikguhn001qga01k24kjnr4","modified":0,"renderable":0},{"_id":"source/_posts/user_profile/life_recycle_conversion.png","slug":"life_recycle_conversion.png","post":"cjdikguhn001qga01k24kjnr4","modified":0,"renderable":0},{"_id":"source/_posts/user_profile/user_interest_profile.png","slug":"user_interest_profile.png","post":"cjdikguhn001qga01k24kjnr4","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cjdikgucx0000ga01xo5et13r","category_id":"cjdikgud40002ga01b4m7c44h","_id":"cjdikgude000bga01fcj8obmp"},{"post_id":"cjdikgud20001ga01iusnbywz","category_id":"cjdikgudb0007ga01nnetrsc3","_id":"cjdikgudh000fga01bh31y8qr"},{"post_id":"cjdikgud70004ga010cpm9k6g","category_id":"cjdikgudb0007ga01nnetrsc3","_id":"cjdikgudj000jga01c9129pqh"},{"post_id":"cjdikgud80005ga01idhc83co","category_id":"cjdikgudb0007ga01nnetrsc3","_id":"cjdikgudk000nga014vt301i4"},{"post_id":"cjdikguda0006ga01h7pwpqbo","category_id":"cjdikgudj000iga01cvx2sj2f","_id":"cjdikgudl000sga01wi58g3jv"},{"post_id":"cjdikgudc0009ga01iqpxes74","category_id":"cjdikgudj000iga01cvx2sj2f","_id":"cjdikgudm000vga012at5qs94"},{"post_id":"cjdikgudd000aga01zx4i7zz0","category_id":"cjdikgudl000rga01wlpvb9az","_id":"cjdikgudm000xga01duabh39i"},{"post_id":"cjdikguhn001qga01k24kjnr4","category_id":"cjdikgudl000rga01wlpvb9az","_id":"cjdikguhx001vga01ldelh1ux"},{"post_id":"cjdikguhr001sga01rt9gx0j9","category_id":"cjdikgudj000iga01cvx2sj2f","_id":"cjdikguhx001wga01vwpqkoj6"}],"PostTag":[{"post_id":"cjdikgucx0000ga01xo5et13r","tag_id":"cjdikgud60003ga01m01rszcb","_id":"cjdikgudi000hga01ic9k7cuc"},{"post_id":"cjdikgucx0000ga01xo5et13r","tag_id":"cjdikgudb0008ga01kzdc0jtm","_id":"cjdikgudj000kga01vt9x5dpd"},{"post_id":"cjdikgucx0000ga01xo5et13r","tag_id":"cjdikgudf000dga01ylz72fyr","_id":"cjdikgudj000mga01ya9xt37c"},{"post_id":"cjdikgud20001ga01iusnbywz","tag_id":"cjdikgudi000gga01ghfpxto0","_id":"cjdikgudl000qga01udvu3w2r"},{"post_id":"cjdikgud20001ga01iusnbywz","tag_id":"cjdikgudj000lga01c08efbb1","_id":"cjdikgudl000tga01sogc1f55"},{"post_id":"cjdikgud70004ga010cpm9k6g","tag_id":"cjdikgudi000gga01ghfpxto0","_id":"cjdikgudn0010ga0187pupdon"},{"post_id":"cjdikgud70004ga010cpm9k6g","tag_id":"cjdikgudl000uga01pcqlu0z3","_id":"cjdikgudn0011ga01nh7x8r0s"},{"post_id":"cjdikgud70004ga010cpm9k6g","tag_id":"cjdikgudm000wga01st3jvrj3","_id":"cjdikgudo0013ga011xhxw10v"},{"post_id":"cjdikgud70004ga010cpm9k6g","tag_id":"cjdikgudn000yga01r894tt64","_id":"cjdikgudo0014ga01tq1kqdxn"},{"post_id":"cjdikgud80005ga01idhc83co","tag_id":"cjdikgudn000zga01457hs3xd","_id":"cjdikgudo0017ga01mt9j98mj"},{"post_id":"cjdikgud80005ga01idhc83co","tag_id":"cjdikgudn0012ga01zscna2x2","_id":"cjdikgudp0018ga01cw7y4ozz"},{"post_id":"cjdikgud80005ga01idhc83co","tag_id":"cjdikgudo0015ga01vpgiaitl","_id":"cjdikgudp001aga01r6h3grge"},{"post_id":"cjdikguda0006ga01h7pwpqbo","tag_id":"cjdikgudm000wga01st3jvrj3","_id":"cjdikgudr001ega01muf3kraz"},{"post_id":"cjdikguda0006ga01h7pwpqbo","tag_id":"cjdikgudn000yga01r894tt64","_id":"cjdikgudr001fga0126278f35"},{"post_id":"cjdikguda0006ga01h7pwpqbo","tag_id":"cjdikgudi000gga01ghfpxto0","_id":"cjdikgudr001hga01751ugqha"},{"post_id":"cjdikguda0006ga01h7pwpqbo","tag_id":"cjdikgudl000uga01pcqlu0z3","_id":"cjdikgudr001iga01bnm3258v"},{"post_id":"cjdikgudc0009ga01iqpxes74","tag_id":"cjdikgudr001dga01o658vzeg","_id":"cjdikguds001lga01yojaabs4"},{"post_id":"cjdikgudc0009ga01iqpxes74","tag_id":"cjdikgudr001gga016ysuf6p1","_id":"cjdikguds001mga01787ehtak"},{"post_id":"cjdikgudc0009ga01iqpxes74","tag_id":"cjdikguds001jga01v1vz23ph","_id":"cjdikguds001nga01q1oaywg7"},{"post_id":"cjdikgudd000aga01zx4i7zz0","tag_id":"cjdikguds001kga01t7zewofw","_id":"cjdikgudt001oga01vmrdi16f"},{"post_id":"cjdikguhn001qga01k24kjnr4","tag_id":"cjdikguhu001tga019bt2ykhf","_id":"cjdikgui10023ga01ifol9qb8"},{"post_id":"cjdikguhn001qga01k24kjnr4","tag_id":"cjdikguhx001xga01sdee7479","_id":"cjdikgui20024ga0197wk7ab5"},{"post_id":"cjdikguhn001qga01k24kjnr4","tag_id":"cjdikguhy001yga013aqyaycu","_id":"cjdikgui20026ga01bfiiy8h8"},{"post_id":"cjdikguhn001qga01k24kjnr4","tag_id":"cjdikguhz001zga01cijzxi0n","_id":"cjdikgui20027ga01faq27gtw"},{"post_id":"cjdikguhn001qga01k24kjnr4","tag_id":"cjdikgui00020ga01752y1oq8","_id":"cjdikgui30028ga01pnlm7pq9"},{"post_id":"cjdikguhn001qga01k24kjnr4","tag_id":"cjdikgui00021ga0186eu6wgb","_id":"cjdikgui30029ga01jbmevgd5"},{"post_id":"cjdikguhr001sga01rt9gx0j9","tag_id":"cjdikgui10022ga01h4nsd3yi","_id":"cjdikgui3002aga01hejab6n6"},{"post_id":"cjdikguhr001sga01rt9gx0j9","tag_id":"cjdikgui20025ga01s93cynmn","_id":"cjdikgui3002bga01x0l24sm6"}],"Tag":[{"name":"lapack","_id":"cjdikgud60003ga01m01rszcb"},{"name":"clapack","_id":"cjdikgudb0008ga01kzdc0jtm"},{"name":"线性代数工具包","_id":"cjdikgudf000dga01ylz72fyr"},{"name":"推荐算法","_id":"cjdikgudi000gga01ghfpxto0"},{"name":"基于性别的推荐","_id":"cjdikgudj000lga01c08efbb1"},{"name":"协同过滤","_id":"cjdikgudl000uga01pcqlu0z3"},{"name":"矩阵分解","_id":"cjdikgudm000wga01st3jvrj3"},{"name":"隐语义模型","_id":"cjdikgudn000yga01r894tt64"},{"name":"个性化推荐","_id":"cjdikgudn000zga01457hs3xd"},{"name":"深度学习","_id":"cjdikgudn0012ga01zscna2x2"},{"name":"DNN","_id":"cjdikgudo0015ga01vpgiaitl"},{"name":"lbfgs","_id":"cjdikgudr001dga01o658vzeg"},{"name":"拟牛顿算法","_id":"cjdikgudr001gga016ysuf6p1"},{"name":"非线性优化","_id":"cjdikguds001jga01v1vz23ph"},{"name":"优质挖掘","_id":"cjdikguds001kga01t7zewofw"},{"name":"用户画像","_id":"cjdikguhu001tga019bt2ykhf"},{"name":"兴趣挖掘","_id":"cjdikguhx001xga01sdee7479"},{"name":"年龄性别挖掘","_id":"cjdikguhy001yga013aqyaycu"},{"name":"常住地挖掘","_id":"cjdikguhz001zga01cijzxi0n"},{"name":"生命周期画像","_id":"cjdikgui00020ga01752y1oq8"},{"name":"流失预测模型","_id":"cjdikgui00021ga0186eu6wgb"},{"name":"决策树","_id":"cjdikgui10022ga01h4nsd3yi"},{"name":"spark mlilib源码","_id":"cjdikgui20025ga01s93cynmn"}]}}